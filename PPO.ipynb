{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proximal Policy Optimization (PPO) Clipped Objective in Pytorch\n",
    "\n",
    "I am no expert on the Natural Policy or TROP. But the idea of Natural Policy and TROP are to control the step size. Natural policy try to find a consistent step size using qudratic approximation, and then TRPO add an extra layer to make sure the parameter update does not change the policy beyond a KL divergence threshold.\n",
    "\n",
    "And the PPO is a simplified implementation that can achieve similiar performance as TRPO without the involved calculation of the Hessian matrix approximation or KL divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from IPython import display\n",
    "from IPython.display import HTML\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.distributions import Normal, Categorical\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "FloatTensor = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor if use_cuda else torch.LongTensor\n",
    "ByteTensor = torch.cuda.ByteTensor if use_cuda else torch.ByteTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_torch_model(model, filename):\n",
    "  if not os.path.exists(os.path.dirname(filename)):\n",
    "    os.makedirs(os.path.dirname(filename))\n",
    "  torch.save(model.state_dict(), filename)\n",
    "\n",
    "def load_torch_model(model, filename):\n",
    "  model.load_state_dict(torch.load(filename))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNet_discrete(nn.Module):\n",
    "  def __init__(self, input_size, output_size):\n",
    "    super(PolicyNet_continuous,self).__init__()\n",
    "    self.l1_linear = nn.Linear(input_size, 64)\n",
    "    self.l2_linear = nn.Linear(64, 32)\n",
    "    self.l3_linear = nn.Linear(32, output_size)\n",
    "    nn.init.kaiming_normal_(self.l1_linear.weight)\n",
    "    nn.init.kaiming_normal_(self.l2_linear.weight)\n",
    "    self.l3_linear.weight.data.zero_()\n",
    "    \n",
    "  def forward(self,x):\n",
    "    out = F.relu(self.l1_linear(x))\n",
    "    out = F.relu(self.l2_linear(out))\n",
    "    out = F.softmax(F.sigmoid(self.l3_linear(out)))\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNet_continuous(nn.Module):\n",
    "  def __init__(self, input_size, output_size):\n",
    "    super(PolicyNet_continuous,self).__init__()\n",
    "    self.l1_linear = nn.Linear(input_size, 64)\n",
    "    self.l2_linear = nn.Linear(64, 32)\n",
    "    self.l3_linear = nn.Linear(32, output_size)\n",
    "    nn.init.kaiming_normal_(self.l1_linear.weight)\n",
    "    nn.init.kaiming_normal_(self.l2_linear.weight)\n",
    "    self.l3_linear.weight.data.zero_()\n",
    "    \n",
    "  def forward(self,x):\n",
    "    out = F.relu(self.l1_linear(x))\n",
    "    out = F.relu(self.l2_linear(out))\n",
    "    out = F.tanh(self.l3_linear(out))\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueNet(nn.Module):\n",
    "  def __init__(self, input_size):\n",
    "    super(ValueNet,self).__init__()\n",
    "    self.l1_linear = nn.Linear(input_size, 64)\n",
    "    self.l2_linear = nn.Linear(64, 32)\n",
    "    self.l3_linear = nn.Linear(32, 1)\n",
    "    nn.init.kaiming_normal_(self.l1_linear.weight)\n",
    "    nn.init.kaiming_normal_(self.l2_linear.weight)\n",
    "    self.l3_linear.weight.data.zero_()\n",
    "\n",
    "  def forward(self, x):\n",
    "    out = F.relu(self.l1_linear(x))\n",
    "    out = F.relu(self.l2_linear(out))\n",
    "    out = self.l3_linear(out)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPO():\n",
    "  def __init__(self, env, steps_in_state = 1):\n",
    "    self.is_training = True\n",
    "    self.state_value_range = [{'max':None, 'min':None}] * env.observation_space.shape[0]\n",
    "    self.steps_in_state = steps_in_state\n",
    "    self.actor = PolicyNet_continuous(env.observation_space.shape[0] * steps_in_state, env.action_space.shape[0])\n",
    "    self.actor_prime = PolicyNet_continuous(env.observation_space.shape[0] * steps_in_state, env.action_space.shape[0])\n",
    "    self.critic = ValueNet(env.observation_space.shape[0] * steps_in_state)\n",
    "    if use_cuda:\n",
    "      self.actor.cuda()\n",
    "      self.actor_prime.cuda()\n",
    "      self.critic.cuda()\n",
    "    # copy the weight in actor, make sure actor and actor_prime start with same weights\n",
    "    self.actor_prime.load_state_dict(self.actor.state_dict())\n",
    "    self.env = env\n",
    "    self.range_scale = (env.action_space.high[0] - env.action_space.low[0]) / 2.0\n",
    "    self._gamma = 0.96\n",
    "    self._epsilon = 0.2\n",
    "\n",
    "  def pick_action(self, state):\n",
    "    action = self.actor(state)\n",
    "    # add noise\n",
    "    if self.is_training:\n",
    "      action_dist = Normal(action, 0.5)\n",
    "      action = action_dist.sample()\n",
    "    else:\n",
    "      action = action.item()\n",
    "    return np.clip(action, -1.0, 1.0)\n",
    "  \n",
    "  def update_ppo_clip(self, batch):\n",
    "    (states, actions, rewards, next_states, ended) = zip(*batch)\n",
    "    states_tensor = torch.stack(states)\n",
    "    actions_tensor = FloatTensor(actions).view(-1,1)\n",
    "    rewards_tensor = FloatTensor(rewards).view(-1,1)\n",
    "    next_states_tensor = torch.stack(next_states)\n",
    "    ended_tensor = FloatTensor(ended).view(-1,1)\n",
    "    \n",
    "    critic_loss = rewards_tensor + self._gamma * (1 - ended_tensor) * self.critic(next_states_tensor) - self.critic(states_tensor)\n",
    "    advantage = []\n",
    "    for delta in critic_loss.view(-1).tolist()[::-1]:\n",
    "      adv = delta\n",
    "      if len(advantage) > 0:\n",
    "        adv += self._gamma * advantage[0]\n",
    "      advantage.insert(0, adv)\n",
    "    advantage_tensor = FloatTensor(advantage).view(-1,1)\n",
    "                             \n",
    "    # hardcode standard deviation for the action probability to 0.5\n",
    "    action_dist_prime = Normal(self.actor_prime(states_tensor), FloatTensor([0.5]*len(batch)))\n",
    "    action_dist_old = Normal(self.actor(states_tensor), FloatTensor([0.5]*len(batch)))\n",
    "    \n",
    "    action_prob_ratio = torch.exp(action_dist_prime.log_prob(actions_tensor) - action_dist_old.log_prob(actions_tensor))\n",
    "    actor_loss = torch.min(action_prob_ratio * advantage_tensor, \\\n",
    "                           torch.clamp(action_prob_ratio, 1 - self._epsilon, 1 + self._epsilon) * advantage_tensor)\n",
    "    \n",
    "    actor_loss = -actor_loss.mean()\n",
    "    self.actor_prime_optimizer.zero_grad()\n",
    "    actor_loss.backward(retain_graph=True)\n",
    "    self.actor_prime_optimizer.step()\n",
    "\n",
    "    # mean square of critic_loss\n",
    "    critic_loss = critic_loss * critic_loss\n",
    "    critic_loss = critic_loss.mean()\n",
    "    self.critic_coptimizer.zero_grad()\n",
    "    critic_loss.backward(retain_graph=True)\n",
    "    self.critic_coptimizer.step()\n",
    "    \n",
    "  def train(self, env, episode_limit=1000, batch_size=64, copy_on_episode=10, lr=1e-3, lr_actor=None, lr_critic=None, checkpoint=100):\n",
    "    lr_actor = lr if lr_actor == None else lr_actor\n",
    "    lr_critic = lr if lr_critic == None else lr_critic\n",
    "    \n",
    "    self.actor_prime_optimizer = torch.optim.Adam(self.actor_prime.parameters(), lr=lr_actor, weight_decay=1e-3)\n",
    "    self.critic_coptimizer = torch.optim.Adam(self.critic.parameters(), lr=lr_critic, weight_decay=1e-3)\n",
    "    \n",
    "    best_score = -99999\n",
    "    running_score = None\n",
    "    self.iteration = 0\n",
    "    for episode_count in range(episode_limit):\n",
    "      s0 = env.reset()\n",
    "      seq = [s0] * self.steps_in_state\n",
    "      state = FloatTensor(seq).view(-1)\n",
    "      episode_ended = False\n",
    "      score = 0\n",
    "      episode = []\n",
    "      while not episode_ended:\n",
    "        action =  self.pick_action(state)\n",
    "        (s1, reward, episode_ended, info) = env.step([action * self.range_scale])\n",
    "        seq = seq[1:]\n",
    "        seq.append(s1)\n",
    "        next_state = FloatTensor(seq).view(-1)\n",
    "        if episode_ended:\n",
    "          ended = 1\n",
    "        else:\n",
    "          ended = 0\n",
    "        episode.append((state, action, reward, next_state, ended))\n",
    "        s0 = s1\n",
    "        state = next_state\n",
    "        score += reward\n",
    "        if len(episode) == batch_size or episode_ended:\n",
    "          if (episode_ended):\n",
    "            episode.pop()\n",
    "          if len(episode) > 0:\n",
    "            self.update_ppo_clip(episode)\n",
    "          episode = []\n",
    "\n",
    "      if running_score == None:\n",
    "        running_score = score\n",
    "      else:\n",
    "        running_score = running_score * 0.9 + score * 0.1\n",
    "      \n",
    "      if (episode_count + 1) % checkpoint == 0 and running_score != None:\n",
    "        if running_score > best_score:\n",
    "          best_score = running_score\n",
    "          save_torch_model(self.actor,'model/ppo_actor_best.pth')          \n",
    "        save_torch_model(self.actor,'model/ppo_actor_iter_%d.pth' %(episode_count+1))\n",
    "        print('%d: running_score:%.2f, ' %(episode_count+1, running_score))\n",
    "        \n",
    "      if  (episode_count + 1) % copy_on_episode == 0:\n",
    "        self.actor.load_state_dict(self.actor_prime.state_dict())\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('Pendulum-v0')\n",
    "agent = PPO(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: running_score:-883.88, \n"
     ]
    }
   ],
   "source": [
    "agent.is_training = True\n",
    "agent.train(env, episode_limit=1, batch_size=1, copy_on_episode=1, lr_actor=1e-4, lr_critic=1e-3, checkpoint=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200: running_score:-1215.53, \n",
      "400: running_score:-1080.34, \n",
      "600: running_score:-1174.50, \n",
      "800: running_score:-1147.52, \n",
      "1000: running_score:-1030.34, \n",
      "1200: running_score:-914.68, \n",
      "1400: running_score:-862.33, \n",
      "1600: running_score:-870.42, \n",
      "1800: running_score:-815.15, \n",
      "2000: running_score:-729.43, \n",
      "2200: running_score:-659.83, \n",
      "2400: running_score:-626.04, \n",
      "2600: running_score:-595.97, \n",
      "2800: running_score:-443.90, \n",
      "3000: running_score:-295.77, \n"
     ]
    }
   ],
   "source": [
    "agent.is_training = True\n",
    "agent.train(env, episode_limit=3000, batch_size=64, copy_on_episode=5, lr_actor=1e-4, lr_critic=1e-3, checkpoint=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent.is_training = True\n",
    "# agent.train(env, episode_limit=500, batch_size=64, copy_on_episode=5, lr_actor=1e-4, lr_critic=1e-4, checkpoint=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-132.5008541134088\n"
     ]
    }
   ],
   "source": [
    "# use the final mean as solution and run a sample episode\n",
    "agent.is_training = False\n",
    "load_torch_model(agent.actor,'model/ppo_actor_best.pth')\n",
    "state = env.reset()\n",
    "frames = []\n",
    "frames.append(env.render(mode='rgb_array'))\n",
    "ended = False\n",
    "score = 0\n",
    "while not ended:\n",
    "  action = agent.pick_action(FloatTensor([state]).view(-1))\n",
    "  (state, reward, ended, info) = env.step([action*2])\n",
    "  score += reward\n",
    "  frames.append(env.render(mode='rgb_array'))\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "def animate(frames):\n",
    "  fig, ax = plt.subplots()\n",
    "  ax.grid('off')\n",
    "  ax.axis('off')\n",
    "  ims = []\n",
    "  for i in range(len(frames)):\n",
    "      im = plt.imshow(frames[i], animated=True)\n",
    "      ims.append([im])\n",
    "  ani = animation.ArtistAnimation(fig, ims, interval=20, blit=True, repeat_delay=1000)\n",
    "  return ani\n",
    "\n",
    "ani = animate(frames)\n",
    "ani.save('pendulum_ppo.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video width=\"400\" controls>\n",
       "  <source src=\"pendulum_ppo.mp4\" type=\"video/mp4\">\n",
       "</video>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<video width=\"400\" controls>\n",
    "  <source src=\"pendulum_ppo.mp4\" type=\"video/mp4\">\n",
    "</video>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch0.4]",
   "language": "python",
   "name": "conda-env-pytorch0.4-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
