{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pytorch_cartpole_sqn.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/julianw/learn_ml/blob/master/pytorch_cartpole_sqn.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "cmkoV0dq3iNM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Shallow neural network for Q function approximation\n",
        "The following 3 cells are intended to be use in google colab environment https://colab.research.google.com/ which allow free gpu usage"
      ]
    },
    {
      "metadata": {
        "id": "xjwZ9UC62aIH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#!kill -9 -1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aHMLQp0r2pEQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#!pip3 install torch==0.4.0 torchvision==0.2.1 > /dev/null"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "J0HQw2DA2yWy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#!pip3 install gym > /dev/null"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_5lyv58_3LTY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "import random\n",
        "import os\n",
        "import gym\n",
        "from google.colab import files"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zr7zdZkz3bwI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "use_cuda = torch.cuda.is_available()\n",
        "FloatTensor = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor\n",
        "LongTensor = torch.cuda.LongTensor if use_cuda else torch.LongTensor\n",
        "ByteTensor = torch.cuda.ByteTensor if use_cuda else torch.ByteTensor"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VnYNFOzWA9AE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def download(filename):\n",
        "  files.download(filename)\n",
        "\n",
        "def save_torch_model(model, filename):\n",
        "  if not os.path.exists(os.path.dirname(filename)):\n",
        "    os.makedirs(os.path.dirname(filename))\n",
        "  torch.save(model.state_dict(), filename)\n",
        "\n",
        "def load_torch_model(model, filename):\n",
        "  model.load_state_dict(torch.load(filename))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "W1R7hd-LsjOW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Replay memory \n",
        "Save every step as the agent performs an action in an environment (in this case is the cartpole-v0)"
      ]
    },
    {
      "metadata": {
        "id": "HcqnzVI33h8B",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class ReplayMemory():\n",
        "  def __init__(self, memory_size = 1000):\n",
        "    self.transitions = []\n",
        "    self.memory_size = memory_size\n",
        "    self.loc_pointer = 0\n",
        "  \n",
        "  def clear(self):\n",
        "    self.transitions = []\n",
        "    self.loc_pointer = 0\n",
        "  \n",
        "  def add(self, step_tuple):\n",
        "    # expect a tuple of transition contain:\n",
        "    # state, action, reward, next_state, ended\n",
        "    if len(self.transitions) <= self.loc_pointer:\n",
        "      self.transitions.append(None)\n",
        "    self.transitions[self.loc_pointer] = step_tuple\n",
        "    self.loc_pointer += 1\n",
        "    if self.loc_pointer >= self.memory_size:\n",
        "      self.loc_pointer %= self.memory_size\n",
        "  \n",
        "  def get_sample(self, batch_size):\n",
        "    return random.sample(self.transitions, batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "V3b5YCdCxPGf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Define the nerual network\n",
        "Keep in mind this is NN is used for Q value approximation. The output value / range is depends on the environment. Don't blindly add a activation function at the output layer"
      ]
    },
    {
      "metadata": {
        "id": "LtJLNd1g3jLN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class NN(nn.Module):\n",
        "  def __init__(self, input_size, output_size):\n",
        "    super(NN,self).__init__()\n",
        "    self.l1_linear = nn.Linear(input_size, 128, bias=False)\n",
        "    nn.init.kaiming_normal_(self.l1_linear.weight)\n",
        "    self.l2_linear = nn.Linear(128, output_size, bias=False)\n",
        "    nn.init.kaiming_normal_(self.l2_linear.weight)\n",
        "    \n",
        "  def forward(self,x):\n",
        "    out = F.relu(self.l1_linear(x))\n",
        "    out = self.l2_linear(out)\n",
        "    return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Q78kvbB-3lJe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class SQN():\n",
        "  _epsilon = 0.2\n",
        "  _gamma = 0.9\n",
        "  replay_memory = ReplayMemory()\n",
        "  \n",
        "  def __init__(self, env, state_size = None, output_size = None):\n",
        "    self.env = env\n",
        "    self.output_size = output_size\n",
        "    self.state_size = state_size\n",
        "    self.Q = NN(state_size * 2, output_size)\n",
        "    if use_cuda:\n",
        "      self.Q.cuda()\n",
        "      \n",
        "  def epsilon(self):\n",
        "    return self._epsilon\n",
        "      \n",
        "  def predict(self, state):\n",
        "    s = Variable(FloatTensor([state]))\n",
        "    action_value = self.Q(s)\n",
        "    return action_value.data.tolist()[0]\n",
        "\n",
        "  def pick_action(self, state):\n",
        "    action = None\n",
        "    if random.random() < self.epsilon():\n",
        "      action =  random.randint(0, self.output_size -1)\n",
        "    else:\n",
        "      action_value = self.predict(state)\n",
        "      action = action_value.index(max(action_value))\n",
        "    return action\n",
        "  \n",
        "  def update_Q(self, batch):\n",
        "    # Q learning, Q(s,a) = Q(s,a) + alpha * [reward + gamma * max(Q(s')) - Q(s,a)]\n",
        "    # Target of the Q function is the one step bellman equation \"reward + gamma * max(Q(s'))\"\n",
        "    # so error is [taget - current estimation] = [reward + gamma * max(Q(s')) - Q(s,a)]\n",
        "    \n",
        "    (state, action, reward, next_state, ended) = tuple(zip(*batch))\n",
        "\n",
        "    var_state = Variable(FloatTensor(state))\n",
        "    var_action = Variable(LongTensor(action))\n",
        "    var_ended = Variable(FloatTensor(ended))\n",
        "    var_reward = Variable(FloatTensor(reward))\n",
        "    var_next_state = Variable(FloatTensor(next_state))\n",
        "    \n",
        "    # current estimation, take the Q value of the action performed\n",
        "    state_action_values = self.Q(var_state).gather(1, var_action.view(-1,1))\n",
        "    \n",
        "    # target. If an episode ended at this step, only reward is used as there are no next state\n",
        "    target_values = Variable(var_reward + (1 - var_ended) * self._gamma * self.Q(var_next_state).max(1)[0])\n",
        "    \n",
        "    loss = F.mse_loss(state_action_values, target_values.view(-1,1))\n",
        "    self.optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    self.optimizer.step()\n",
        "\n",
        "  def train(self, env, episode, iter_per_episode = 100, batch_size = 32, lr=1e-3):\n",
        "    self.optimizer = torch.optim.Adam(self.Q.parameters(), lr=lr, weight_decay=1e-3)\n",
        "    best_score = 0\n",
        "    for i in range(episode):\n",
        "      s0 = env.reset()\n",
        "      state = np.append(s0,s0)\n",
        "      episode_ended = False\n",
        "      step = 0\n",
        "      while not episode_ended:\n",
        "        action =  self.pick_action(state)\n",
        "        (s1, reward, episode_ended, info) = env.step(action)\n",
        "        next_state = np.append(s0,s1)\n",
        "        step += 1\n",
        "        if episode_ended:\n",
        "          ended = 1\n",
        "          if step < 200:\n",
        "            # pole tipped over\n",
        "            reward = 0.0\n",
        "          else:\n",
        "            # environment terminate as max step reached\n",
        "            reward = 1.0\n",
        "        else:\n",
        "          ended = 0\n",
        "        self.replay_memory.add((state,action,reward,next_state,ended))\n",
        "        s0 = s1\n",
        "        state = next_state\n",
        "        \n",
        "      if (i + 1) % 100 == 0:\n",
        "        if step > best_score:\n",
        "          save_torch_model(self.Q, 'model/cartpole_sqn_best.pth')\n",
        "          best_score = step\n",
        "        save_torch_model(self.Q,'model/cartpole_sqn_iter_%d.pth' %(i+1))\n",
        "        # longer the better, that mean the agent can keep the pole up for longer period\n",
        "        print(i+1,': pole tip over at step:', step)\n",
        "        \n",
        "      if len(self.replay_memory.transitions) > batch_size:\n",
        "        for j in range(iter_per_episode):\n",
        "          batch = self.replay_memory.get_sample(batch_size)\n",
        "          self.update_Q(batch)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3HahQ5ij3m92",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d5134616-1c2b-498d-bc45-340954d28371"
      },
      "cell_type": "code",
      "source": [
        "env = gym.make('CartPole-v0')\n",
        "agent = SQN(env,4,2)"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "lyrwdNIc3phN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "68714967-a66c-481f-f45b-e4036a809568"
      },
      "cell_type": "code",
      "source": [
        "# run 600 epidsodes, each episode train the Q network for 50 times using 64 batches\n",
        "agent.train(env, episode=600, iter_per_episode=50, batch_size=64, lr=5e-4)"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100 : periods the agent can keep the pole up: 14\n",
            "200 : periods the agent can keep the pole up: 45\n",
            "300 : periods the agent can keep the pole up: 41\n",
            "400 : periods the agent can keep the pole up: 164\n",
            "500 : periods the agent can keep the pole up: 200\n",
            "600 : periods the agent can keep the pole up: 155\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "95NRP-Go1Hr-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kF4XiBKg3RKa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}