{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A simple implementation of TD0 and TD_lambda with OpenAI Gym FrozenLakeNotSlippery-v0\n",
    "\n",
    "This implement the basic Q value(state-action value) table lookup using SARSA update and Q learning to walk a maze.\n",
    "\n",
    "SARSA update:\n",
    "$Q(state_t,action_t) \\leftarrow Q(state_t,action_t) + \\alpha[reward + \\gamma * Q(state_{t+1},action_{t+1}) - Q(state_t,action_t)]$\n",
    "\n",
    "Q learning:\n",
    "$Q(state_t,action_t) \\leftarrow Q(state_t,action_t) + \\alpha[reward + \\gamma * max(Q(state_{t+1},action)) - Q(state_t,action_t)]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym, random\n",
    "import time\n",
    "from gym.envs.registration import register\n",
    "register(\n",
    "    id='FrozenLakeNotSlippery4x4-v0',\n",
    "    entry_point='gym.envs.toy_text:FrozenLakeEnv',\n",
    "    kwargs={'map_name' : '4x4', 'is_slippery': False},\n",
    ")\n",
    "register(\n",
    "    id='FrozenLakeNotSlippery8x8-v0',\n",
    "    entry_point='gym.envs.toy_text:FrozenLakeEnv',\n",
    "    kwargs={'map_name' : '8x8', 'is_slippery': False},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FrozenLake is a grid world with some randomness, as the agent and slip and rumble into a different cell it intended to walk to.\n",
    "The FrozenLakeNotSlippery-v0 environment is same as FrozenLake except that it remove the randomness.  \n",
    "S - is where the agent start  \n",
    "F - is safe cell  \n",
    "H - is hole, when agent hit a hole, episode end with 0 reward  \n",
    "G - is goal state, 1.0 reward  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('FrozenLakeNotSlippery4x4-v0')\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TD0 with Q value table look up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TD0(object):\n",
    "  \n",
    "  def __init__(self):\n",
    "    self.Q = {}\n",
    "    self.iteration = 0\n",
    "    self._gamma = 0.5\n",
    "    self._alpha = 0.02\n",
    "    self._epsilon = 0.4\n",
    "\n",
    "    # init Q, FrozenLake environment contain a grid with 64 cells, \n",
    "    # and 4 actions to choose from. setting all (state, action) pair to 0\n",
    "    for i in range(64):\n",
    "      self.Q[i] = [0.0] * 4\n",
    "\n",
    "  def alpha(self):\n",
    "    decay = 1 / ( 1 + self.iteration / 5000)\n",
    "    return self._alpha * decay\n",
    "  \n",
    "  def epsilon(self):\n",
    "    decay = 1 / ( 1 + self.iteration / 5000)\n",
    "    return self._epsilon * decay\n",
    "\n",
    "  def update_sarsa(self, s0, a0, r, s1, a1):\n",
    "    # on policy SARAS update\n",
    "    # using bellman equation, q value target is r + discount * Q(next_state, next_action)\n",
    "    # delta = target - current state action estimation\n",
    "    if a1 != None:\n",
    "      delta = r + self._gamma * self.Q[s1][a1] - self.Q[s0][a0]\n",
    "    else:\n",
    "      delta = r - self.Q[s0][a0]\n",
    "    self.Q[s0][a0] += self.alpha() * delta\n",
    "    \n",
    "  def pick_action(self, state):\n",
    "    if random.random() < self.epsilon():\n",
    "      return random.randrange(0,4,1)\n",
    "    else:\n",
    "      return self.Q[state].index(max(self.Q[state]))\n",
    "    \n",
    "  def show_policy(self):\n",
    "    # greedy policy\n",
    "    action_name_map = ['left','down','right','up','none']\n",
    "    for i in range(4):\n",
    "      policy = \"\"\n",
    "      for j in range(4):\n",
    "        state = i*4+j\n",
    "        action_index = self.Q[state].index(max(self.Q[state]))\n",
    "        if self.Q[state] == [0.0]*4:\n",
    "          action_index = 4\n",
    "        policy += \"%2d: %-10s\" %(state, action_name_map[action_index])\n",
    "      print(policy)\n",
    "    \n",
    "  def train(self, env, episode, use_step_cost = False):\n",
    "    for i in range(episode):\n",
    "      self.iteration += 1\n",
    "      s0 = env.reset()\n",
    "      a0 = self.pick_action(s0)\n",
    "      episode_ended = False\n",
    "      while not episode_ended:\n",
    "        (s1, reward, episode_ended, info) = env.step(a0)\n",
    "        # set use_step_cost = True, the agent should learn the shortest path\n",
    "        if use_step_cost and reward <= 0.0:\n",
    "            reward = -0.1\n",
    "            if episode_ended:\n",
    "              # walk into hole\n",
    "              reward = -1.0\n",
    "        if not episode_ended:\n",
    "          a1 = self.pick_action(s1)\n",
    "        else:\n",
    "          a1 = None\n",
    "        self.update_sarsa(s0,a0,reward,s1,a1)\n",
    "        s0 = s1\n",
    "        a0 = a1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLakeNotSlippery4x4-v0')\n",
    "agent = TD0()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0: none       1: none       2: none       3: none      \n",
      " 4: none       5: none       6: none       7: none      \n",
      " 8: none       9: none      10: none      11: none      \n",
      "12: none      13: none      14: right     15: none      \n",
      "==================================================\n",
      " 0: down       1: left       2: left       3: none      \n",
      " 4: down       5: none       6: up         7: none      \n",
      " 8: right      9: down      10: down      11: none      \n",
      "12: none      13: right     14: right     15: none      \n",
      "==================================================\n",
      " 0: down       1: left       2: left       3: none      \n",
      " 4: down       5: none       6: up         7: none      \n",
      " 8: right      9: down      10: down      11: none      \n",
      "12: none      13: right     14: right     15: none      \n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "  agent.train(env, 5000)\n",
    "  agent.show_policy()\n",
    "  print (\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remark: The Agent may not learn the shortest path as the cost / reward for taking a move is 0. So walking a longer route is having the same reward as walking a shorter path. To make the agent learn the shortest path, set reward to a small negative value (e.g. -0.1) and a bigger cost falling into a hole can make the agent prefer shorter path."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TD Lambda with Q value table lookup using SARAS update and Q update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TD_lambda(object):\n",
    "  \n",
    "  def __init__(self, grid_w, grid_h):\n",
    "    # init Q, FrozenLake environment contain a grid with 16 cells, \n",
    "    # and 4 actions to choose from \n",
    "    self.Q = {}\n",
    "    self.iteration = 0\n",
    "    self._gamma = 0.9\n",
    "    self._lambda = 0.5\n",
    "    self._alpha = 0.02\n",
    "    self._epsilon = 0.4\n",
    "\n",
    "    self.grid_w = grid_w\n",
    "    self.grid_h = grid_h\n",
    "    for i in range(grid_w * grid_h):\n",
    "      self.Q[i] = [0.0] * 4\n",
    "      \n",
    "  def alpha(self):\n",
    "    # maybe a bit over kill to wrap alpha in a function,\n",
    "    # but this make it easy if want to do alpha decay \n",
    "    decay = 1 / ( 1 + self.iteration / 5000)\n",
    "    return self._alpha * decay\n",
    "  \n",
    "  def epsilon(self):\n",
    "    # same as alpha, wrap in a function make it easier to do epsilon decay if needed\n",
    "    decay = 1 / ( 1 + self.iteration / 5000)\n",
    "    return self._epsilon * decay\n",
    "\n",
    "  def update_eligibility(self):\n",
    "    for key in self.eligibility:\n",
    "      self.eligibility[key] *= self._gamma * self._lambda\n",
    "\n",
    "  def update_sarsa(self, s0, a0, r, s1, a1):\n",
    "    if a1 != None:\n",
    "      delta = r + self._gamma * self.Q[s1][a1] - self.Q[s0][a0]\n",
    "    else:\n",
    "      delta = r - self.Q[s0][a0]\n",
    "    for (s,a) in self.eligibility:\n",
    "      self.Q[s][a] += self.alpha() * delta * self.eligibility[s,a]\n",
    "\n",
    "  def update_Q(self, s0, a0, r, s1, a1):\n",
    "    if a1 != None:\n",
    "      delta = r + self._gamma * max(self.Q[s1]) - self.Q[s0][a0]\n",
    "    else:\n",
    "      delta = r - self.Q[s0][a0]\n",
    "    for (s,a) in self.eligibility:\n",
    "      self.Q[s][a] += self.alpha() * delta * self.eligibility[s,a]\n",
    "\n",
    "  def pick_action(self, state):\n",
    "    if random.random() < self.epsilon():\n",
    "      return random.randrange(0,4,1)\n",
    "    else:\n",
    "      return self.Q[state].index(max(self.Q[state]))\n",
    "\n",
    "  def show_policy(self):\n",
    "    # greedy policy\n",
    "    action_name_map = ['left','down','right','up','none']\n",
    "    for i in range(self.grid_h):\n",
    "      policy = \"\"\n",
    "      for j in range(self.grid_w):\n",
    "        state = i * self.grid_w + j\n",
    "        action_index = self.Q[state].index(max(self.Q[state]))\n",
    "        if self.Q[state] == [0]*4:\n",
    "          action_index = 4\n",
    "        policy += \"%2d: %-10s\" %(state, action_name_map[action_index])\n",
    "      print(policy)\n",
    "\n",
    "  def train(self, env, episode, update='sarsa', use_step_cost = False):\n",
    "    for i in range(episode):\n",
    "      self.iteration += 1\n",
    "      s0 = env.reset()\n",
    "      a0 = self.pick_action(s0)\n",
    "      self.eligibility = {}\n",
    "      episode_ended = False\n",
    "      while not episode_ended:\n",
    "        (s1, reward, episode_ended, info) = env.step(a0)\n",
    "        # agent should learn the shortest path with step cost (with enough iteration)\n",
    "        # step cost helps to converge faster in grid world too.\n",
    "        if use_step_cost and reward <= 0.0:\n",
    "            reward = -0.1\n",
    "            if episode_ended:\n",
    "              reward = -1\n",
    "\n",
    "        if (s0,a0) in self.eligibility:\n",
    "          self.eligibility[s0,a0] += 1\n",
    "        else:\n",
    "          self.eligibility[s0,a0] = 1\n",
    "        if not episode_ended:\n",
    "          a1 = self.pick_action(s1)\n",
    "        else:\n",
    "          a1 = None\n",
    "        if update == 'sarsa':\n",
    "          self.update_sarsa(s0,a0,reward,s1,a1)\n",
    "        elif update == 'Q':\n",
    "          self.update_Q(s0,a0,reward,s1,a1)\n",
    "        self.update_eligibility()\n",
    "        s0 = s1\n",
    "        a0 = a1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLakeNotSlippery4x4-v0')\n",
    "agent_lambda = TD_lambda(4,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0: none       1: none       2: none       3: none      \n",
      " 4: none       5: none       6: none       7: none      \n",
      " 8: none       9: none      10: none      11: none      \n",
      "12: none      13: none      14: none      15: none      \n",
      "==================================================\n",
      " 0: down       1: left       2: left       3: none      \n",
      " 4: down       5: none       6: none       7: none      \n",
      " 8: right      9: down      10: left      11: none      \n",
      "12: none      13: right     14: right     15: none      \n",
      "==================================================\n",
      " 0: down       1: left       2: left       3: none      \n",
      " 4: down       5: none       6: none       7: none      \n",
      " 8: right      9: down      10: left      11: none      \n",
      "12: none      13: right     14: right     15: none      \n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "  # set update to sarsa to use on policy SARSA update\n",
    "  agent_lambda.train(env, 5000, update='sarsa')\n",
    "  agent_lambda.show_policy()\n",
    "  print (\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLakeNotSlippery4x4-v0')\n",
    "agent_lambda = TD_lambda(4,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0: none       1: none       2: none       3: none      \n",
      " 4: none       5: none       6: none       7: none      \n",
      " 8: none       9: none      10: none      11: none      \n",
      "12: none      13: none      14: none      15: none      \n",
      "==================================================\n",
      " 0: down       1: left       2: left       3: none      \n",
      " 4: down       5: none       6: up         7: none      \n",
      " 8: right      9: down      10: down      11: none      \n",
      "12: none      13: right     14: right     15: none      \n",
      "==================================================\n",
      " 0: down       1: left       2: left       3: left      \n",
      " 4: down       5: none       6: up         7: none      \n",
      " 8: right      9: right     10: down      11: none      \n",
      "12: none      13: right     14: right     15: none      \n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "  # set update to sarsa to use Q update \n",
    "  agent_lambda.train(env, 5000, update='Q')\n",
    "  agent_lambda.show_policy()\n",
    "  print (\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('FrozenLakeNotSlippery8x8-v0')\n",
    "env.render()\n",
    "agent_lambda = TD_lambda(8,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0: down       1: left       2: left       3: none      \n",
      " 4: down       5: none       6: down       7: none      \n",
      " 8: right      9: down      10: left      11: none      \n",
      "12: none      13: right     14: right     15: none      \n",
      "==============================================================================================================\n"
     ]
    }
   ],
   "source": [
    "for i in range(8):\n",
    "  agent_lambda.train(env, 20000, update='Q', use_step_cost=True)\n",
    "agent_lambda.show_policy()\n",
    "print (\"=\" * 110)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import random\n",
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "FloatTensor = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor if use_cuda else torch.LongTensor\n",
    "ByteTensor = torch.cuda.ByteTensor if use_cuda else torch.ByteTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN(nn.Module):\n",
    "  def __init__(self, in_size, out_size):\n",
    "    super(NN, self).__init__()\n",
    "    self.in_size = in_size\n",
    "    self.out_size = out_size\n",
    "    self.l1_linear = nn.Linear(in_size, 4, bias=False)\n",
    "#     self.l2_linear = nn.Linear(4,out_size, bias=False)\n",
    "  \n",
    "  def forward(self, x):\n",
    "    out = self.l1_linear(x)\n",
    "#     out = F.sigmoid(self.l1_linear(x))\n",
    "#     out = F.sigmoid(self.l2_linear(out))\n",
    "    return out\n",
    "\n",
    "class TransitionModel():\n",
    "  state = {}\n",
    "  total_count = 0\n",
    "  def __init__(self):\n",
    "    pass\n",
    "  \n",
    "  def clear(self):\n",
    "    self.state = {}\n",
    "    self.total_count = 0\n",
    "  \n",
    "  def add_transition(self, s0, a0, r, s1):\n",
    "    if (s0,a0) not in self.state:\n",
    "      self.state[s0,a0] = {\n",
    "        't':{},\n",
    "        'c':0\n",
    "      }\n",
    "    transition = self.state[s0,a0]['t']\n",
    "    if (r, s1) not in transition:\n",
    "      transition[r, s1] = 0\n",
    "    transition[r, s1] += 1\n",
    "    self.state[s0, a0]['c'] += 1\n",
    "    self.total_count += 1\n",
    "\n",
    "  def get_transition(self, s0, a0):\n",
    "    if (s0,a0) not in self.state:\n",
    "      return None\n",
    "    threshold = random.random()\n",
    "    count = 0.0\n",
    "    transition = self.state[s0,a0]['t']\n",
    "    total_count = self.state[s0,a0]['c']\n",
    "    for key in transition:\n",
    "      count += transition[key] / total_count * 1.0\n",
    "      if count > threshold:\n",
    "        return key\n",
    "\n",
    "  def get_random_transition(self, sample_count=1):\n",
    "    transitions = []\n",
    "    start_states = list(self.state.keys())\n",
    "    start_states_thresholds = []\n",
    "    tmp = 0.0\n",
    "    for i in range(len(start_states)):\n",
    "      tmp += self.state[start_states[i]]['c'] * 1.0 / self.total_count\n",
    "      start_states_thresholds.append(tmp)\n",
    "\n",
    "    for i in range(sample_count):\n",
    "      threshold = random.random()\n",
    "      idx = 0\n",
    "      for j in range(len(start_states)):\n",
    "        if start_states_thresholds[j] > threshold:\n",
    "          idx = j\n",
    "          break\n",
    "      (s0, a0) = start_states[idx]\n",
    "      (r, s1) = self.get_transition(s0, a0)\n",
    "      transitions.append((s0,a0,r,s1))\n",
    "    return transitions\n",
    "\n",
    "class TransitionHistory():\n",
    "  transitions = []\n",
    "  def __init__(self):\n",
    "    pass\n",
    "  \n",
    "  def clear(self):\n",
    "    self.transitions = []\n",
    "  \n",
    "  def add_transition(self, s0, a0, r, s1):\n",
    "    self.transitions.append((s0,a0,r,s1))\n",
    "  \n",
    "  def get_sample_transition(self, batch_size):\n",
    "    return random.sample(self.transitions, batch_size)\n",
    "\n",
    "class q_approx():\n",
    "  _gamma = 0.8\n",
    "  _lambda = 0.5\n",
    "  _epsilon = 0.2\n",
    "  transition_history = TransitionHistory()\n",
    "\n",
    "  def __init__(self, size):\n",
    "    self.Q = NN(size, 4)\n",
    "    if use_cuda:\n",
    "      self.Q.cuda()\n",
    "    self.size = size\n",
    "    self.optimizer = torch.optim.Adagrad(self.Q.parameters(),weight_decay=1e-5)\n",
    "#     self.optimizer = torch.optim.SGD(self.Q.parameters(),lr=2.0)\n",
    "    self.loss_fn = nn.MSELoss()\n",
    "\n",
    "  def index_to_onehot(self, index, total=16):\n",
    "    onehot = [0] * total\n",
    "    onehot[index] = 1\n",
    "    return onehot\n",
    "  \n",
    "  def epsilon(self):\n",
    "    return self._epsilon\n",
    "\n",
    "  def pick_action(self, state):\n",
    "    if random.random() < self.epsilon():\n",
    "      return random.randrange(0,4,1)\n",
    "    else:\n",
    "      self.predict(state)\n",
    "      onehot = self.index_to_onehot(state)\n",
    "      s = Variable(FloatTensor([onehot]))\n",
    "      action_value = self.Q(s).data.tolist()\n",
    "      return action_value.index(max(action_value))\n",
    "\n",
    "  def predict(self, state):\n",
    "    onehot = self.index_to_onehot(state)\n",
    "    s = Variable(FloatTensor([onehot]))\n",
    "    action_value = self.Q(s).data.tolist()[0]\n",
    "    return action_value\n",
    "\n",
    "  def update_Q(self, batch):\n",
    "#     print(batch)\n",
    "    (state, action, reward, next_state) = tuple(zip(*batch))\n",
    "    non_final_mask = torch.ByteTensor(tuple(map(lambda s:s is not None, next_state)))\n",
    "    non_final_next_states = Variable(FloatTensor([self.index_to_onehot(s) for s in next_state if s is not None]), volatile=True)\n",
    "\n",
    "    state_batch = Variable(FloatTensor([self.index_to_onehot(s) for s in state]))\n",
    "    action_batch = Variable(LongTensor([a for a in action]))\n",
    "    reward_batch = Variable(FloatTensor(reward))\n",
    "#     print('state_batch:',state_batch)\n",
    "#     print('self.Q(state_batch):',self.Q(state_batch))\n",
    "#     print('action_batch:',action_batch)\n",
    "    state_action_values = self.Q(state_batch).gather(1, action_batch.view(-1,1))\n",
    "    \n",
    "    next_state_values = Variable(torch.zeros(len(batch)).type(FloatTensor))\n",
    "    next_state_values[non_final_mask] = self.Q(non_final_next_states).max(1)[0]\n",
    "    \n",
    "    expected_state_action_values = (next_state_values * self._gamma) + reward_batch\n",
    "    expected_state_action_values = Variable(expected_state_action_values.view(-1,1).data)\n",
    "    \n",
    "#     print('state_action_values',state_action_values)\n",
    "#     print('expected_state_action_values',expected_state_action_values)\n",
    "    loss = self.loss_fn(state_action_values, expected_state_action_values)\n",
    "#     print('loss', loss.data[0])\n",
    "    self.optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    \n",
    "    # gradient clipping\n",
    "    for param in self.Q.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "\n",
    "    self.optimizer.step()\n",
    "\n",
    "\n",
    "  def show_policy(self):\n",
    "    # greedy policy\n",
    "# SFFF\n",
    "# FHFH\n",
    "# FFFH\n",
    "# HFFG\n",
    "    action_name_map = ['left','down','right','up','none']\n",
    "    for i in range(4):\n",
    "      policy = \"\"\n",
    "      for j in range(4):\n",
    "        if (i * 4 + j) in [5,7,11,12,15]:\n",
    "          policy += \"%2d: %-10s\" %(state, 'None')\n",
    "          continue\n",
    "        state = i*4+j\n",
    "        action_value = self.predict(state)\n",
    "        action_index = action_value.index(max(action_value))\n",
    "        policy += \"%2d: %-10s\" %(state, action_name_map[action_index])\n",
    "      print(policy)\n",
    "\n",
    "  def train(self, env, episode, epoch, batch_size = 32):\n",
    "    for i in range(episode):\n",
    "      s0 = env.reset()\n",
    "      a0 = self.pick_action(s0)\n",
    "      episode_ended = False\n",
    "      while not episode_ended:\n",
    "        (s1, reward, episode_ended, info) = env.step(a0)\n",
    "        if reward > 0.0:\n",
    "          print('reward:', reward)\n",
    "          \n",
    "        if not episode_ended:\n",
    "          if reward <= 0.0:\n",
    "            reward = -0.1\n",
    "          a1 = self.pick_action(s1)\n",
    "        else:\n",
    "          if reward <= 0.0:\n",
    "            reward = -1.0\n",
    "          a1 = None\n",
    "          s1 = None\n",
    "        self.transition_history.add_transition(s0,a0,reward,s1)\n",
    "        s0 = s1\n",
    "        a0 = a1\n",
    "      self.update_Q(self.transition_history.transitions)\n",
    "      self.transition_history.clear()\n",
    "#     iteration = epoch * len(self.transition_history.transitions) // batch_size\n",
    "#     for i in range(iteration):\n",
    "#       batch = self.transition_history.get_sample_transition(batch_size)\n",
    "#       self.update_Q(batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLakeNotSlippery4x4-v0')\n",
    "agent = q_approx(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0: right      1: right      2: down       3: right     \n",
      " 4: down       4: None       6: down       6: None      \n",
      " 8: right      9: right     10: down      10: None      \n",
      "10: None      13: right     14: right     14: None      \n",
      "reward: 1.0\n",
      "reward: 1.0\n",
      "reward: 1.0\n",
      "reward: 1.0\n",
      "reward: 1.0\n",
      "reward: 1.0\n",
      "reward: 1.0\n",
      "reward: 1.0\n",
      "i: 9\n",
      " 0: right      1: right      2: down       3: right     \n",
      " 4: down       4: None       6: down       6: None      \n",
      " 8: right      9: down      10: down      10: None      \n",
      "10: None      13: right     14: right     14: None      \n",
      "reward: 1.0\n",
      "i: 19\n",
      " 0: right      1: right      2: down       3: right     \n",
      " 4: down       4: None       6: down       6: None      \n",
      " 8: right      9: down      10: down      10: None      \n",
      "10: None      13: right     14: right     14: None      \n",
      "reward: 1.0\n",
      "reward: 1.0\n",
      "reward: 1.0\n",
      "reward: 1.0\n",
      "reward: 1.0\n",
      "reward: 1.0\n",
      "i: 29\n",
      " 0: right      1: right      2: down       3: right     \n",
      " 4: down       4: None       6: down       6: None      \n",
      " 8: right      9: down      10: down      10: None      \n",
      "10: None      13: right     14: right     14: None      \n",
      "reward: 1.0\n",
      "reward: 1.0\n",
      "reward: 1.0\n",
      "reward: 1.0\n",
      "i: 39\n",
      " 0: right      1: right      2: down       3: right     \n",
      " 4: down       4: None       6: down       6: None      \n",
      " 8: right      9: down      10: down      10: None      \n",
      "10: None      13: right     14: right     14: None      \n",
      "reward: 1.0\n",
      "reward: 1.0\n",
      "reward: 1.0\n",
      "reward: 1.0\n",
      "reward: 1.0\n",
      "reward: 1.0\n",
      "reward: 1.0\n",
      "i: 49\n",
      " 0: right      1: right      2: down       3: right     \n",
      " 4: down       4: None       6: down       6: None      \n",
      " 8: right      9: right     10: down      10: None      \n",
      "10: None      13: right     14: right     14: None      \n",
      "reward: 1.0\n",
      "i: 59\n",
      " 0: right      1: right      2: down       3: right     \n",
      " 4: down       4: None       6: down       6: None      \n",
      " 8: right      9: right     10: down      10: None      \n",
      "10: None      13: right     14: right     14: None      \n",
      "reward: 1.0\n",
      "i: 69\n",
      " 0: right      1: right      2: down       3: right     \n",
      " 4: down       4: None       6: down       6: None      \n",
      " 8: right      9: right     10: down      10: None      \n",
      "10: None      13: right     14: right     14: None      \n",
      "reward: 1.0\n",
      "reward: 1.0\n",
      "reward: 1.0\n",
      "reward: 1.0\n",
      "i: 79\n",
      " 0: right      1: right      2: down       3: right     \n",
      " 4: down       4: None       6: down       6: None      \n",
      " 8: right      9: right     10: down      10: None      \n",
      "10: None      13: right     14: right     14: None      \n",
      "reward: 1.0\n",
      "reward: 1.0\n",
      "reward: 1.0\n",
      "reward: 1.0\n",
      "reward: 1.0\n",
      "reward: 1.0\n",
      "reward: 1.0\n",
      "i: 89\n",
      " 0: right      1: right      2: down       3: right     \n",
      " 4: down       4: None       6: down       6: None      \n",
      " 8: right      9: right     10: down      10: None      \n",
      "10: None      13: right     14: right     14: None      \n",
      "reward: 1.0\n",
      "reward: 1.0\n",
      "reward: 1.0\n",
      "reward: 1.0\n",
      "i: 99\n",
      " 0: right      1: right      2: down       3: right     \n",
      " 4: down       4: None       6: down       6: None      \n",
      " 8: right      9: down      10: down      10: None      \n",
      "10: None      13: right     14: right     14: None      \n"
     ]
    }
   ],
   "source": [
    "# print(agent.Q.state_dict())\n",
    "# print('='*50)\n",
    "agent.show_policy()\n",
    "for i in range(100):\n",
    "  agent.train(env,5000,1)\n",
    "  if (i + 1) % 10 == 0:\n",
    "    print('i:',i)\n",
    "    agent.show_policy()\n",
    "#     print(agent.Q.state_dict())\n",
    "#     print('='*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state: 0 right [-0.3841942250728607, -0.3387400805950165, -0.31577813625335693, -0.38410425186157227]\n",
      "state: 1 right [-0.384112685918808, -0.9999314546585083, -0.23986807465553284, -0.31474339962005615]\n",
      "state: 2 down [-0.31462711095809937, -0.15629145503044128, -0.16590292751789093, -0.2260712832212448]\n",
      "state: 3 right [-0.2275475561618805, -0.2446267306804657, -0.0822184681892395, -0.08603277802467346]\n",
      "state: 4 down [-0.33876967430114746, -0.2653079926967621, -0.9999646544456482, -0.38393115997314453]\n",
      "state: 6 down [-0.7284090518951416, -0.06479938328266144, -0.2104904055595398, -0.11793595552444458]\n",
      "state: 8 right [-0.2653837502002716, -0.999298632144928, -0.18375496566295624, -0.3385081887245178]\n",
      "state: 9 right [-0.265155553817749, -0.09389536827802658, -0.09310631453990936, -0.969906210899353]\n",
      "state: 10 down [-0.1804787516593933, 0.005862134508788586, -0.39025938510894775, -0.12580232322216034]\n",
      "state: 13 right [-0.9697795510292053, -0.08061707019805908, 0.005955095868557692, -0.1414671391248703]\n",
      "state: 14 right [-0.08467772603034973, 0.0013781692832708359, 0.12343517690896988, -0.015672078356146812]\n"
     ]
    }
   ],
   "source": [
    "action_name_map = ['left','down','right','up','none']\n",
    "for i in range(4):\n",
    "  policy = \"\"\n",
    "  for j in range(4):\n",
    "    if (i * 4 + j) in [5,7,11,12,15]:\n",
    "      continue\n",
    "    state = i*4+j\n",
    "    action_value = agent.predict(state)\n",
    "    action_index = action_value.index(max(action_value))\n",
    "    print('state:', state, action_name_map[action_index], action_value)\n",
    "\n",
    "#     policy += \"%2d: %-10s\" %(state, action_name_map[action_index])\n",
    "#   print(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch36]",
   "language": "python",
   "name": "conda-env-pytorch36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
