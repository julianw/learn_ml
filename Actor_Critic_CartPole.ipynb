{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actor Critic Cartpole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from IPython import display\n",
    "from IPython.display import HTML\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.distributions import Normal, Categorical\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "FloatTensor = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor if use_cuda else torch.LongTensor\n",
    "ByteTensor = torch.cuda.ByteTensor if use_cuda else torch.ByteTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_torch_model(model, filename):\n",
    "  if not os.path.exists(os.path.dirname(filename)):\n",
    "    os.makedirs(os.path.dirname(filename))\n",
    "  torch.save(model.state_dict(), filename)\n",
    "\n",
    "def load_torch_model(model, filename):\n",
    "  model.load_state_dict(torch.load(filename))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_grad(source, target):\n",
    "  grads = []\n",
    "  for param in source.parameters():\n",
    "    grads.append(param.grad.clone())\n",
    "  grads.reverse()\n",
    "  for param in target.parameters():\n",
    "    param.grad = grads.pop()\n",
    "\n",
    "def zero_grad(model):\n",
    "  for param in model.parameters():\n",
    "    if type(param.grad) != type(None):\n",
    "      param.grad.data.zero_()\n",
    "      \n",
    "def update_target(target_net, eval_net, tau):\n",
    "  fast = eval_net.state_dict()\n",
    "  slow = target_net.state_dict()\n",
    "  for t in slow:\n",
    "    slow[t] = slow[t] * (1. - tau) + fast[t] * tau\n",
    "\n",
    "  target_net.load_state_dict(slow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNet_discret(nn.Module):\n",
    "  def __init__(self, input_size, output_size):\n",
    "    super(PolicyNet_discret,self).__init__()\n",
    "    self.l1_linear = nn.Linear(input_size, 512)\n",
    "    self.l2_linear = nn.Linear(512, 256)\n",
    "    self.l3_linear = nn.Linear(256, output_size)\n",
    "    nn.init.kaiming_normal_(self.l1_linear.weight)\n",
    "    nn.init.kaiming_normal_(self.l2_linear.weight)\n",
    "    self.l3_linear.weight.data.zero_()\n",
    "    \n",
    "  def forward(self,x):\n",
    "    out = F.relu(self.l1_linear(x))\n",
    "    out = F.relu(self.l2_linear(out))\n",
    "    out = F.softmax(self.l3_linear(out),dim=0)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueNet(nn.Module):\n",
    "  def __init__(self, input_size):\n",
    "    super(ValueNet,self).__init__()\n",
    "    self.l1_linear = nn.Linear(input_size, 512)\n",
    "    self.l2_linear = nn.Linear(512,256)\n",
    "    self.l3_linear = nn.Linear(256, 1)\n",
    "    nn.init.kaiming_normal_(self.l1_linear.weight)\n",
    "    nn.init.kaiming_normal_(self.l2_linear.weight)\n",
    "    self.l3_linear.weight.data.zero_()\n",
    "\n",
    "  def forward(self, x):\n",
    "    out = F.relu(self.l1_linear(x))\n",
    "    out = F.relu(self.l2_linear(out))\n",
    "    out = self.l3_linear(out)\n",
    "    return out    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic():\n",
    "  def __init__(self, env, steps_in_state = 2):\n",
    "    self.steps_in_state = steps_in_state\n",
    "    self.policy = PolicyNet_discret(env.observation_space.shape[0] * steps_in_state,env.action_space.n)\n",
    "    self.value = ValueNet(env.observation_space.shape[0] * steps_in_state)\n",
    "    if use_cuda:\n",
    "      self.policy.cuda()\n",
    "      self.value.cuda()\n",
    "    self.env = env\n",
    "    self._gamma = 0.96\n",
    "    \n",
    "  def pick_action(self, state):\n",
    "    probs = self.policy(state)\n",
    "    action_dist = Categorical(probs)\n",
    "    action = action_dist.sample()\n",
    "    action = action.item()\n",
    "    return (action, action_dist.log_prob(FloatTensor([action])))\n",
    "  \n",
    "  def update_actor_critic(self, episode):\n",
    "    (states, actions, rewards, next_states, log_probs, ended) = zip(*episode)\n",
    "    \n",
    "    rewards = FloatTensor(rewards)\n",
    "    ended = FloatTensor(ended)\n",
    "    state_value = self.value(torch.stack(states))\n",
    "    next_state_value = self.value(torch.stack(next_states))\n",
    "    target_value = rewards + (1 - ended) * self._gamma * next_state_value\n",
    "    \n",
    "    delta = target_value - state_value\n",
    "\n",
    "    value_loss = F.mse_loss(state_value, target_value)\n",
    "    \n",
    "    policy_loss = []\n",
    "    for log_prob, d in zip(log_probs, delta):\n",
    "      policy_loss.append(-log_prob * d)\n",
    "    policy_loss = torch.stack(policy_loss).sum()\n",
    "    \n",
    "    self.value_optimizer.zero_grad()\n",
    "    value_loss.backward(retain_graph=True)\n",
    "    self.value_optimizer.step()\n",
    "    \n",
    "    self.policy_optimizer.zero_grad()\n",
    "    policy_loss.backward()\n",
    "    self.policy_optimizer.step()\n",
    "\n",
    "  def train(self, env, episode, lr=1e-3, target_copylr=1e-3, lr_policy=None, lr_value=None, checkpoint=100):\n",
    "    lr_policy = lr if lr_policy == None else lr_policy\n",
    "    lr_value = lr if lr_value == None else lr_value\n",
    "    self.policy_optimizer = torch.optim.Adam(self.policy.parameters(), lr=lr_policy, weight_decay=1e-3)\n",
    "    self.value_optimizer = torch.optim.Adam(self.value.parameters(), lr=lr_value, weight_decay=1e-3)\n",
    "    best_score = -99999\n",
    "    running_score = None\n",
    "    for i in range(episode):\n",
    "      s0 = env.reset()\n",
    "      seq = [s0] * self.steps_in_state\n",
    "      state = FloatTensor(seq).view(-1)\n",
    "      episode = []\n",
    "      episode_ended = False\n",
    "      score = 0\n",
    "      while not episode_ended:\n",
    "        (action, log_prob) =  self.pick_action(state)\n",
    "        (s1, reward, episode_ended, info) = env.step(action)\n",
    "        seq = seq[1:]\n",
    "        seq.append(s1)\n",
    "        next_state = FloatTensor(seq).view(-1)\n",
    "        if episode_ended:\n",
    "          ended = 1\n",
    "        else:\n",
    "          ended = 0\n",
    "        episode.append((state, action, reward, next_state, log_prob, ended))\n",
    "\n",
    "        s0 = s1\n",
    "        state = next_state\n",
    "        score += reward\n",
    "        \n",
    "      if running_score == None:\n",
    "        running_score = score\n",
    "      else:\n",
    "        running_score = running_score * 0.9 + score * 0.1\n",
    "        \n",
    "      if (i + 1) % checkpoint == 0:\n",
    "        is_best = False\n",
    "        if running_score > best_score:\n",
    "          is_best = True\n",
    "          save_torch_model(self.policy, 'model/actor_critic_cartpole_policy_best.pth')\n",
    "          best_score = running_score\n",
    "        save_torch_model(self.policy,'model/actor_critic_cartpole_policy_iter_%d.pth' %(i+1))\n",
    "        print('%d: running_score:%.2f, is_best:%s' %(i+1, running_score, is_best))\n",
    "\n",
    "      self.update_actor_critic(episode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "agent = ActorCritic(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100: running_score:24.29, is_best:True\n",
      "200: running_score:28.67, is_best:True\n",
      "300: running_score:50.39, is_best:True\n",
      "400: running_score:131.17, is_best:True\n",
      "500: running_score:147.21, is_best:True\n",
      "600: running_score:198.62, is_best:True\n",
      "700: running_score:155.24, is_best:False\n",
      "800: running_score:180.74, is_best:False\n",
      "900: running_score:194.96, is_best:False\n",
      "1000: running_score:192.11, is_best:False\n"
     ]
    }
   ],
   "source": [
    "agent.train(env, 1000, lr=1e-4, checkpoint=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch0.4]",
   "language": "python",
   "name": "conda-env-pytorch0.4-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
