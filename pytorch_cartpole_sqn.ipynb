{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cmkoV0dq3iNM"
   },
   "source": [
    "## Q learning using neural network as function approximation solving cartpole\n",
    "\n",
    "Basic Q learning with replay memory to train a Q network to estimate action value\n",
    "\n",
    "library:  \n",
    "pytorch 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_5lyv58_3LTY"
   },
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "from IPython.display import HTML\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import gym\n",
    "#from google.colab import files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zr7zdZkz3bwI"
   },
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "FloatTensor = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor if use_cuda else torch.LongTensor\n",
    "ByteTensor = torch.cuda.ByteTensor if use_cuda else torch.ByteTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VnYNFOzWA9AE"
   },
   "outputs": [],
   "source": [
    "def save_torch_model(model, filename):\n",
    "  if not os.path.exists(os.path.dirname(filename)):\n",
    "    os.makedirs(os.path.dirname(filename))\n",
    "  torch.save(model.state_dict(), filename)\n",
    "\n",
    "def load_torch_model(model, filename):\n",
    "  model.load_state_dict(torch.load(filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W1R7hd-LsjOW"
   },
   "source": [
    "## Replay memory \n",
    "Save every step as the agent performs an action in an environment (in this case is the cartpole-v0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HcqnzVI33h8B"
   },
   "outputs": [],
   "source": [
    "class ReplayMemory():\n",
    "  def __init__(self, memory_size = 1000):\n",
    "    self.transitions = []\n",
    "    self.memory_size = memory_size\n",
    "    self.loc_pointer = 0\n",
    "  \n",
    "  def clear(self):\n",
    "    self.transitions = []\n",
    "    self.loc_pointer = 0\n",
    "  \n",
    "  def add(self, step_tuple):\n",
    "    # expect a tuple of transition contain:\n",
    "    # state, action, reward, next_state, ended\n",
    "    if len(self.transitions) <= self.loc_pointer:\n",
    "      self.transitions.append(None)\n",
    "    self.transitions[self.loc_pointer] = step_tuple\n",
    "    self.loc_pointer += 1\n",
    "    if self.loc_pointer >= self.memory_size:\n",
    "      self.loc_pointer %= self.memory_size\n",
    "  \n",
    "  def get_sample(self, batch_size):\n",
    "    return random.sample(self.transitions, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LtJLNd1g3jLN"
   },
   "outputs": [],
   "source": [
    "# define the a model for Q value approximation\n",
    "class NN(nn.Module):\n",
    "  def __init__(self, input_size, output_size):\n",
    "    super(NN,self).__init__()\n",
    "    self.l1_linear = nn.Linear(input_size, 128, bias=False)\n",
    "    nn.init.kaiming_normal_(self.l1_linear.weight)\n",
    "    self.l2_linear = nn.Linear(128, output_size, bias=False)\n",
    "    nn.init.kaiming_normal_(self.l2_linear.weight)\n",
    "    \n",
    "  def forward(self,x):\n",
    "    out = F.relu(self.l1_linear(x))\n",
    "    out = self.l2_linear(out)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q78kvbB-3lJe"
   },
   "outputs": [],
   "source": [
    "# \"Shallow\" Q network agent\n",
    "class SQN():\n",
    "  _epsilon = 0.3\n",
    "  _gamma = 0.95\n",
    "  replay_memory = ReplayMemory()\n",
    "  \n",
    "  def __init__(self, env, state_size = None, output_size = None):\n",
    "    self.is_training = True\n",
    "    self.iteration = 0\n",
    "    self.env = env\n",
    "    self.output_size = output_size\n",
    "    self.state_size = state_size\n",
    "    self.Q = NN(state_size * 2, output_size)\n",
    "    if use_cuda:\n",
    "      self.Q.cuda()\n",
    "      \n",
    "  def epsilon(self):\n",
    "    decay = 1 / (1 + self.iteration)\n",
    "    return self._epsilon * decay\n",
    "      \n",
    "  def predict(self, state):\n",
    "    s = Variable(FloatTensor([state]))\n",
    "    action_value = self.Q(s)\n",
    "    return action_value.data.tolist()[0]\n",
    "\n",
    "  def pick_action(self, state):\n",
    "    if self.is_training and random.random() < self.epsilon():\n",
    "      action =  random.randint(0, self.output_size -1)\n",
    "    else:\n",
    "      action_value = self.predict(state)\n",
    "      action = action_value.index(max(action_value))\n",
    "    return action\n",
    "  \n",
    "  def update_Q(self, batch):\n",
    "    # Q learning, Q(s,a) = Q(s,a) + alpha * [reward + gamma * max(Q(s')) - Q(s,a)]\n",
    "    # Target of the Q function is the one step bellman equation \"reward + gamma * max(Q(s'))\"\n",
    "    # so error is [taget - current estimation] = [reward + gamma * max(Q(s')) - Q(s,a)]\n",
    "    \n",
    "    (state, action, reward, next_state, ended) = tuple(zip(*batch))\n",
    "\n",
    "    var_state = Variable(FloatTensor(state))\n",
    "    var_action = Variable(LongTensor(action))\n",
    "    var_ended = Variable(FloatTensor(ended))\n",
    "    var_reward = Variable(FloatTensor(reward))\n",
    "    var_next_state = Variable(FloatTensor(next_state))\n",
    "    \n",
    "    # current estimation, take the Q value of the action performed\n",
    "    state_action_values = self.Q(var_state).gather(1, var_action.view(-1,1))\n",
    "    \n",
    "    # target. If an episode ended at this step, only reward is used as there are no next state\n",
    "    target_values = Variable(var_reward + (1 - var_ended) * self._gamma * self.Q(var_next_state).max(1)[0])\n",
    "    \n",
    "    loss = F.mse_loss(state_action_values, target_values.view(-1,1))\n",
    "    self.optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    self.optimizer.step()\n",
    "\n",
    "  def train(self, env, episode, iter_per_episode = 100, batch_size = 32, lr=1e-3, checkpoint = 50):\n",
    "    self.optimizer = torch.optim.Adam(self.Q.parameters(), lr=lr, weight_decay=1e-3)\n",
    "    running_score = None\n",
    "    best_score = -99999\n",
    "    for i in range(episode):\n",
    "      s0 = env.reset()\n",
    "      # state contain 2 time step, as single state does not contain enough information where the pole is moving or moving how fast\n",
    "      state = np.append(s0,s0)\n",
    "      episode_ended = False\n",
    "      score = 0\n",
    "      while not episode_ended:\n",
    "        action =  self.pick_action(state)\n",
    "        (s1, reward, episode_ended, info) = env.step(action)\n",
    "        next_state = np.append(s0,s1)\n",
    "        score += 1\n",
    "        if episode_ended:\n",
    "          ended = 1\n",
    "        else:\n",
    "          ended = 0\n",
    "        self.replay_memory.add((state,action,reward,next_state,ended))\n",
    "        s0 = s1\n",
    "        state = next_state\n",
    "        \n",
    "      if running_score == None:\n",
    "        running_score = score\n",
    "      else:\n",
    "        running_score = running_score * 0.9 + score * 0.1\n",
    "        \n",
    "        \n",
    "      if (i + 1) % checkpoint == 0:\n",
    "        if running_score > best_score and running_score > 100:\n",
    "          best_score = running_score\n",
    "          save_torch_model(self.Q,'model/cartpole_sqn_best.pth')\n",
    "        save_torch_model(self.Q,'model/cartpole_sqn_iter_%d.pth' %(i+1))\n",
    "        # longer the better, that mean the agent can keep the pole up for longer period\n",
    "        print(i+1,': running_score:', running_score)\n",
    "        \n",
    "      if len(self.replay_memory.transitions) > batch_size:\n",
    "        for j in range(iter_per_episode):\n",
    "          batch = self.replay_memory.get_sample(batch_size)\n",
    "          self.update_Q(batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "3HahQ5ij3m92",
    "outputId": "d5134616-1c2b-498d-bc45-340954d28371"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "agent = SQN(env,4,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "lyrwdNIc3phN",
    "outputId": "68714967-a66c-481f-f45b-e4036a809568"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 : running_score: 23.011921307368173\n",
      "200 : running_score: 17.840581295144926\n",
      "300 : running_score: 61.219751902765246\n",
      "400 : running_score: 16.199829456527162\n",
      "500 : running_score: 107.18701508126131\n",
      "600 : running_score: 73.62057823902238\n",
      "700 : running_score: 109.22056504160464\n",
      "800 : running_score: 183.54841456120877\n",
      "900 : running_score: 142.95538271472319\n",
      "1000 : running_score: 198.05190059263006\n"
     ]
    }
   ],
   "source": [
    "# run 600 epidsodes, each episode train the Q network for 50 times using 64 batches\n",
    "agent.train(env, episode=1000, iter_per_episode=10, batch_size=64, lr=1e-3, checkpoint=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "95NRP-Go1Hr-"
   },
   "outputs": [],
   "source": [
    "# sample run with the trained model and record the frames\n",
    "frames = []\n",
    "agent.is_training = False\n",
    "s0 = env.reset()\n",
    "frames.append(env.render(mode='rgb_array'))\n",
    "state = np.append(s0,s0)\n",
    "episode_ended = False\n",
    "score = 0\n",
    "while not episode_ended:\n",
    "  action =  agent.pick_action(state)\n",
    "  (s1, reward, episode_ended, info) = env.step(action)\n",
    "  frames.append(env.render(mode='rgb_array'))\n",
    "  next_state = np.append(s0,s1)\n",
    "  state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "def animate(frames):\n",
    "  fig, ax = plt.subplots()\n",
    "  ax.grid('off')\n",
    "  ax.axis('off')\n",
    "  ims = []\n",
    "  for i in range(len(frames)):\n",
    "      im = plt.imshow(frames[i], animated=True)\n",
    "      ims.append([im])\n",
    "  ani = animation.ArtistAnimation(fig, ims, interval=20, blit=True, repeat_delay=1000)\n",
    "  return ani\n",
    "\n",
    "ani = animate(frames)\n",
    "ani.save('cartpole_sqn.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video width=\"400\" controls>\n",
       "  <source src=\"cartpole_sqn.mp4\" type=\"video/mp4\">\n",
       "</video>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<video width=\"400\" controls>\n",
    "  <source src=\"cartpole_sqn.mp4\" type=\"video/mp4\">\n",
    "</video>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "pytorch_cartpole_sqn.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python [conda env:pytorch0.4]",
   "language": "python",
   "name": "conda-env-pytorch0.4-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
