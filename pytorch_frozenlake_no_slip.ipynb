{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym, random\n",
    "import time\n",
    "from gym.envs.registration import register\n",
    "register(\n",
    "    id='FrozenLakeNotSlippery4x4-v0',\n",
    "    entry_point='gym.envs.toy_text:FrozenLakeEnv',\n",
    "    kwargs={'map_name' : '4x4', 'is_slippery': False},\n",
    ")\n",
    "register(\n",
    "    id='FrozenLakeNotSlippery8x8-v0',\n",
    "    entry_point='gym.envs.toy_text:FrozenLakeEnv',\n",
    "    kwargs={'map_name' : '8x8', 'is_slippery': False},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import random\n",
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "FloatTensor = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor if use_cuda else torch.LongTensor\n",
    "ByteTensor = torch.cuda.ByteTensor if use_cuda else torch.ByteTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SNN(nn.Module):    # Shallow neural network\n",
    "  def __init__(self, input_size, output_size):\n",
    "    super(SNN, self).__init__()\n",
    "    self.l1_linear = nn.Linear(input_size, output_size, bias = False)\n",
    "\n",
    "  def forward(self,x):\n",
    "    out = F.sigmoid(self.l1_linear(x))\n",
    "    return out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransitionHistory():\n",
    "  def __init__(self, max_count = 1000):\n",
    "    self.transitions = []\n",
    "    self.max_count = max_count\n",
    "    self.loc_pointer = 0\n",
    "  \n",
    "  def clear(self):\n",
    "    self.transitions = []\n",
    "    self.loc_pointer = 0\n",
    "  \n",
    "  def add_transition(self, s0, a0, r, s1):\n",
    "    if len(self.transitions) <= self.loc_pointer:\n",
    "      self.transitions.append(None)\n",
    "    self.transitions[self.loc_pointer] = (s0, a0, r, s1)\n",
    "    self.loc_pointer += 1\n",
    "    if self.loc_pointer >= self.max_count:\n",
    "      self.loc_pointer %= self.max_count\n",
    "  \n",
    "  def get_sample_transition(self, batch_size):\n",
    "    return random.sample(self.transitions, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SQN(): # Shallow q network\n",
    "  _gamma = 0.8\n",
    "  _lambda = 0.5\n",
    "  _epsilon = 0.2\n",
    "  transition_history = TransitionHistory()\n",
    "\n",
    "  def __init__(self, size):\n",
    "    self.Q = SNN(size, 4)\n",
    "    if use_cuda:\n",
    "      self.Q.cuda()\n",
    "    self.size = size\n",
    "    self.optimizer = torch.optim.Adagrad(self.Q.parameters(),weight_decay=1e-5)\n",
    "#     self.optimizer = torch.optim.SGD(self.Q.parameters(),lr=2.0)\n",
    "    self.loss_fn = nn.MSELoss()\n",
    "\n",
    "  def index_to_onehot(self, index, total=16):\n",
    "    onehot = [0] * total\n",
    "    onehot[index] = 1\n",
    "    return onehot\n",
    "  \n",
    "  def epsilon(self):\n",
    "    return self._epsilon\n",
    "\n",
    "  def pick_action(self, state):\n",
    "    if random.random() < self.epsilon():\n",
    "      return random.randrange(0,4,1)\n",
    "    else:\n",
    "      self.predict(state)\n",
    "      onehot = self.index_to_onehot(state)\n",
    "      s = Variable(FloatTensor([onehot]))\n",
    "      action_value = self.Q(s).data.tolist()\n",
    "      return action_value.index(max(action_value))\n",
    "\n",
    "  def predict(self, state):\n",
    "    onehot = self.index_to_onehot(state)\n",
    "    s = Variable(FloatTensor([onehot]))\n",
    "    action_value = self.Q(s).data.tolist()[0]\n",
    "    return action_value\n",
    "\n",
    "  def update_Q(self, batch):\n",
    "#     print(batch)\n",
    "    (state, action, reward, next_state) = tuple(zip(*batch))\n",
    "    non_final_mask = torch.ByteTensor(tuple(map(lambda s:s is not None, next_state)))\n",
    "    non_final_next_states = Variable(FloatTensor([self.index_to_onehot(s) for s in next_state if s is not None]), volatile=True)\n",
    "\n",
    "    state_batch = Variable(FloatTensor([self.index_to_onehot(s) for s in state]))\n",
    "    action_batch = Variable(LongTensor([a for a in action]))\n",
    "    reward_batch = Variable(FloatTensor(reward))\n",
    "#     print('state_batch:',state_batch)\n",
    "#     print('self.Q(state_batch):',self.Q(state_batch))\n",
    "#     print('action_batch:',action_batch)\n",
    "    state_action_values = self.Q(state_batch).gather(1, action_batch.view(-1,1))\n",
    "    \n",
    "    next_state_values = Variable(torch.zeros(len(batch)).type(FloatTensor))\n",
    "    next_state_values[non_final_mask] = self.Q(non_final_next_states).max(1)[0]\n",
    "    \n",
    "    expected_state_action_values = (next_state_values * self._gamma) + reward_batch\n",
    "    expected_state_action_values = Variable(expected_state_action_values.view(-1,1).data)\n",
    "    \n",
    "#     print('state_action_values',state_action_values)\n",
    "#     print('expected_state_action_values',expected_state_action_values)\n",
    "    loss = self.loss_fn(state_action_values, expected_state_action_values)\n",
    "#     print('loss', loss.data[0])\n",
    "    self.optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    \n",
    "    # gradient clipping\n",
    "    for param in self.Q.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "\n",
    "    self.optimizer.step()\n",
    "\n",
    "\n",
    "  def show_policy(self):\n",
    "    # greedy policy\n",
    "# SFFF\n",
    "# FHFH\n",
    "# FFFH\n",
    "# HFFG\n",
    "    action_name_map = ['left','down','right','up','none']\n",
    "    for i in range(4):\n",
    "      policy = \"\"\n",
    "      for j in range(4):\n",
    "        if (i * 4 + j) in [5,7,11,12,15]:\n",
    "          policy += \"%2d: %-10s\" %(state, 'None')\n",
    "          continue\n",
    "        state = i*4+j\n",
    "        action_value = self.predict(state)\n",
    "        action_index = action_value.index(max(action_value))\n",
    "        policy += \"%2d: %-10s\" %(state, action_name_map[action_index])\n",
    "      print(policy)\n",
    "\n",
    "  def train(self, env, episode, batch_per_episode = 20, batch_size = 32):\n",
    "    for i in range(episode):\n",
    "      s0 = env.reset()\n",
    "      a0 = self.pick_action(s0)\n",
    "      episode_ended = False\n",
    "      while not episode_ended:\n",
    "        (s1, reward, episode_ended, info) = env.step(a0)\n",
    "        if reward > 0.0:\n",
    "          print('reward:', reward)\n",
    "          \n",
    "        if not episode_ended:\n",
    "          if reward <= 0.0:\n",
    "            reward = -0.1\n",
    "          a1 = self.pick_action(s1)\n",
    "        else:\n",
    "          if reward <= 0.0:\n",
    "            reward = -1.0\n",
    "          a1 = None\n",
    "          s1 = None\n",
    "        self.transition_history.add_transition(s0,a0,reward,s1)\n",
    "        s0 = s1\n",
    "        a0 = a1\n",
    "      for i in range(batch_per_episode):\n",
    "        batch = self.transition_history.get_sample_transition(batch_size)\n",
    "        self.update_Q(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLakeNotSlippery4x4-v0')\n",
    "agent = SQN(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0: right      1: right      2: down       3: right     \n",
      " 4: down       4: None       6: up         6: None      \n",
      " 8: right      9: right     10: down      10: None      \n",
      "10: None      13: right     14: right     14: None      \n",
      "i: 9\n",
      " 0: right      1: right      2: down       3: right     \n",
      " 4: down       4: None       6: down       6: None      \n",
      " 8: right      9: right     10: down      10: None      \n",
      "10: None      13: right     14: right     14: None      \n",
      "i: 19\n",
      " 0: right      1: right      2: down       3: right     \n",
      " 4: down       4: None       6: up         6: None      \n",
      " 8: right      9: right     10: down      10: None      \n",
      "10: None      13: right     14: right     14: None      \n",
      "i: 29\n",
      " 0: right      1: right      2: right      3: right     \n",
      " 4: down       4: None       6: up         6: None      \n",
      " 8: right      9: right     10: down      10: None      \n",
      "10: None      13: right     14: right     14: None      \n",
      "i: 39\n",
      " 0: right      1: right      2: right      3: right     \n",
      " 4: down       4: None       6: up         6: None      \n",
      " 8: right      9: right     10: down      10: None      \n",
      "10: None      13: right     14: right     14: None      \n",
      "i: 49\n",
      " 0: right      1: right      2: right      3: right     \n",
      " 4: down       4: None       6: up         6: None      \n",
      " 8: right      9: right     10: down      10: None      \n",
      "10: None      13: right     14: right     14: None      \n",
      "reward: 1.0\n",
      "i: 59\n",
      " 0: right      1: right      2: right      3: right     \n",
      " 4: down       4: None       6: down       6: None      \n",
      " 8: right      9: right     10: down      10: None      \n",
      "10: None      13: right     14: right     14: None      \n",
      "i: 69\n",
      " 0: right      1: right      2: right      3: right     \n",
      " 4: down       4: None       6: down       6: None      \n",
      " 8: right      9: right     10: down      10: None      \n",
      "10: None      13: right     14: right     14: None      \n",
      "i: 79\n",
      " 0: right      1: right      2: right      3: right     \n",
      " 4: down       4: None       6: down       6: None      \n",
      " 8: right      9: right     10: down      10: None      \n",
      "10: None      13: right     14: right     14: None      \n",
      "i: 89\n",
      " 0: right      1: right      2: right      3: up        \n",
      " 4: down       4: None       6: down       6: None      \n",
      " 8: right      9: right     10: down      10: None      \n",
      "10: None      13: right     14: right     14: None      \n",
      "i: 99\n",
      " 0: right      1: right      2: down       3: right     \n",
      " 4: down       4: None       6: down       6: None      \n",
      " 8: right      9: right     10: down      10: None      \n",
      "10: None      13: right     14: right     14: None      \n"
     ]
    }
   ],
   "source": [
    "# print(agent.Q.state_dict())\n",
    "# print('='*50)\n",
    "agent.show_policy()\n",
    "for i in range(100):\n",
    "  agent.train(env,500,20)\n",
    "  if (i + 1) % 10 == 0:\n",
    "    print('i:',i)\n",
    "    agent.show_policy()\n",
    "#     print(agent.Q.state_dict())\n",
    "#     print('='*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch36]",
   "language": "python",
   "name": "conda-env-pytorch36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
