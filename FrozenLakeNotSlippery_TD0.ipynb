{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A simple implementation of TD0 and TD_lambda on OpenAI Gym FrozenLakeNotSlippery-v0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym, random\n",
    "import time\n",
    "from gym.envs.registration import register\n",
    "register(\n",
    "    id='FrozenLakeNotSlippery-v0',\n",
    "    entry_point='gym.envs.toy_text:FrozenLakeEnv',\n",
    "    kwargs={'map_name' : '4x4', 'is_slippery': False},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TD0(object):\n",
    "  Q = {}\n",
    "  gamma = 0.5\n",
    "  _alpha = 0.01\n",
    "  _epsilon = 0.2\n",
    "  \n",
    "  def __init__(self):\n",
    "    # init Q, FrozenLake environment contain a grid with 16 cells, \n",
    "    # and 4 actions to choose from \n",
    "    for i in xrange(16):\n",
    "      self.Q[i] = [0.0] * 4\n",
    "\n",
    "  def alpha(self):\n",
    "    return self._alpha\n",
    "  \n",
    "  def epsilon(self):\n",
    "    return self._epsilon\n",
    "\n",
    "  def update_sarsa(self, s0, a0, r, s1, a1):\n",
    "    if a1 != None:\n",
    "      delta = r + self.gamma * self.Q[s1][a1] - self.Q[s0][a0]\n",
    "    else:\n",
    "      delta = r - self.Q[s0][a0]\n",
    "    self.Q[s0][a0] += self.alpha() * delta\n",
    "    \n",
    "  def pick_action(self, state):\n",
    "    if random.random() < self.epsilon():\n",
    "      return random.randrange(0,4,1)\n",
    "    else:\n",
    "      return self.Q[state].index(max(self.Q[state]))\n",
    "    \n",
    "  def show_policy(self):\n",
    "    action_name_map = ['left','down','right','up','none']\n",
    "    for i in range(4):\n",
    "      policy = \"\"\n",
    "      for j in range(4):\n",
    "        state = i*4+j\n",
    "        action_index = self.Q[state].index(max(self.Q[state]))\n",
    "        if self.Q[state] == [0.0]*4:\n",
    "          action_index = 4\n",
    "        policy += \"%2d: %-10s\" %(state, action_name_map[action_index])\n",
    "      print policy\n",
    "    \n",
    "  def train(self, env, episode):\n",
    "    for i in xrange(episode):\n",
    "      s0 = env.reset()\n",
    "      a0 = self.pick_action(s0)\n",
    "      episode_ended = False\n",
    "      while not episode_ended:\n",
    "        (s1, reward, episode_ended, info) = env.step(a0)\n",
    "        # uncomment the following 4 lines the agent should learn the shortest path\n",
    "#         if reward <= 0.0:\n",
    "#           reward = -0.1\n",
    "#           if episode_ended:\n",
    "#             reward = -0.3\n",
    "        if not episode_ended:\n",
    "          a1 = self.pick_action(s1)\n",
    "        else:\n",
    "          a1 = None\n",
    "        self.update_sarsa(s0,a0,reward,s1,a1)\n",
    "        s0 = s1\n",
    "        a0 = a1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLakeNotSlippery-v0')\n",
    "agent = TD0()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0: down       1: left       2: left       3: none      \n",
      " 4: down       5: none       6: up         7: none      \n",
      " 8: right      9: down      10: down      11: none      \n",
      "12: none      13: right     14: right     15: none      \n",
      "==================================================\n",
      " 0: down       1: left       2: left       3: left      \n",
      " 4: down       5: none       6: up         7: none      \n",
      " 8: right      9: down      10: down      11: none      \n",
      "12: none      13: right     14: right     15: none      \n",
      "==================================================\n",
      " 0: down       1: left       2: left       3: left      \n",
      " 4: down       5: none       6: down       7: none      \n",
      " 8: right      9: down      10: down      11: none      \n",
      "12: none      13: right     14: right     15: none      \n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "  agent.train(env, 5000)\n",
    "  agent.show_policy()\n",
    "  print \"=\" * 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remark: The Agent may not learn the shortest path as the cost / reward for taking a move is 0. So walking a longer route is having the same reward as walking a shorter path. To make the agent learn the shortest path, set reward to a small negative value (e.g. -0.1) and a bigger cost falling into a hole can make the agent prefer shorter path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TD_lambda(object):\n",
    "  Q = {}\n",
    "  _gamma = 0.5\n",
    "  _lambda = 1.0\n",
    "  _alpha = 0.01\n",
    "  _epsilon = 0.2\n",
    "  \n",
    "  def __init__(self):\n",
    "    # init Q, FrozenLake environment contain a grid with 16 cells, \n",
    "    # and 4 actions to choose from \n",
    "    for i in xrange(16):\n",
    "      self.Q[i] = [0.0] * 4\n",
    "\n",
    "  def alpha(self):\n",
    "    # maybe a bit over kill to wrap alpha in a function,\n",
    "    # but this make it easy if want to do alpha decay \n",
    "    return self._alpha\n",
    "  \n",
    "  def epsilon(self):\n",
    "    # same as alpha, wrap in a function make it easier to do epsilon decay if needed\n",
    "    return self._epsilon\n",
    "\n",
    "  def update_eligibility(self):\n",
    "    for key in self.eligibility:\n",
    "      self.eligibility *= self._gamma * self._lambda\n",
    "\n",
    "  def update_sarsa(self, s0, a0, r, s1, a1):\n",
    "    if a1 != None:\n",
    "      delta = r + self._gamma * self.Q[s1][a1] - self.Q[s0][a0]\n",
    "    else:\n",
    "      delta = r - self.Q[s0][a0]\n",
    "    for (s,a) in self.eligibility:\n",
    "      self.Q[s][a] += self.alpha() * delta * self.eligibility[s,a]\n",
    "    \n",
    "  def pick_action(self, state):\n",
    "    if random.random() < self.epsilon():\n",
    "      return random.randrange(0,4,1)\n",
    "    else:\n",
    "      return self.Q[state].index(max(self.Q[state]))\n",
    "    \n",
    "  def show_policy(self):\n",
    "    action_name_map = ['left','down','right','up','none']\n",
    "    for i in range(4):\n",
    "      policy = \"\"\n",
    "      for j in range(4):\n",
    "        state = i*4+j\n",
    "        action_index = self.Q[state].index(max(self.Q[state]))\n",
    "        if self.Q[state] == [0.0]*4:\n",
    "          action_index = 4\n",
    "        policy += \"%2d: %-10s\" %(state, action_name_map[action_index])\n",
    "      print policy\n",
    "    \n",
    "  def train(self, env, episode):\n",
    "    for i in xrange(episode):\n",
    "      s0 = env.reset()\n",
    "      a0 = self.pick_action(s0)\n",
    "      self.eligibility = {}\n",
    "      episode_ended = False\n",
    "      while not episode_ended:\n",
    "        (s1, reward, episode_ended, info) = env.step(a0)\n",
    "        if (s0,a0) in self.eligibility:\n",
    "          self.eligibility[s0,a0] += 1\n",
    "        else:\n",
    "          self.eligibility[s0,a0] = 1\n",
    "        if not episode_ended:\n",
    "          a1 = self.pick_action(s1)\n",
    "        else:\n",
    "          a1 = None\n",
    "        self.update_sarsa(s0,a0,reward,s1,a1)\n",
    "        s0 = s1\n",
    "        a0 = a1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLakeNotSlippery-v0')\n",
    "agent_lambda = TD_lambda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0: none       1: none       2: none       3: none      \n",
      " 4: none       5: none       6: none       7: none      \n",
      " 8: none       9: none      10: none      11: none      \n",
      "12: none      13: none      14: none      15: none      \n",
      "==================================================\n",
      " 0: right      1: right      2: down       3: down      \n",
      " 4: down       5: none       6: down       7: none      \n",
      " 8: right      9: right     10: down      11: none      \n",
      "12: none      13: up        14: right     15: none      \n",
      "==================================================\n",
      " 0: right      1: right      2: down       3: down      \n",
      " 4: down       5: none       6: down       7: none      \n",
      " 8: right      9: right     10: down      11: none      \n",
      "12: none      13: up        14: right     15: none      \n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "  agent_lambda.train(env, 3000)\n",
    "  agent_lambda.show_policy()\n",
    "  print \"=\" * 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Somehow TD_lambda tend to converge with the shortest path. The reason maybe the longer route the agent walk, the more likely it fall into a hole before hitting goal state. So the shorter path would hit goal state more often and recevive more reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
