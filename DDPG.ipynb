{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DDPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from IPython import display\n",
    "from IPython.display import HTML\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.distributions import Normal, Categorical\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "FloatTensor = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor if use_cuda else torch.LongTensor\n",
    "ByteTensor = torch.cuda.ByteTensor if use_cuda else torch.ByteTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_torch_model(model, filename):\n",
    "  if not os.path.exists(os.path.dirname(filename)):\n",
    "    os.makedirs(os.path.dirname(filename))\n",
    "  torch.save(model.state_dict(), filename)\n",
    "\n",
    "def load_torch_model(model, filename):\n",
    "  model.load_state_dict(torch.load(filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_grad(source, target):\n",
    "  grads = []\n",
    "  for param in source.parameters():\n",
    "    grads.append(param.grad.clone())\n",
    "  grads.reverse()\n",
    "  for param in target.parameters():\n",
    "    param.grad = grads.pop()\n",
    "\n",
    "def zero_grad(model):\n",
    "  for param in model.parameters():\n",
    "    if type(param.grad) != type(None):\n",
    "      param.grad.data.zero_()\n",
    "      \n",
    "def update_target(target_net, eval_net, tau):\n",
    "  fast = eval_net.state_dict()\n",
    "  slow = target_net.state_dict()\n",
    "  for t in slow:\n",
    "    slow[t] = slow[t] * (1. - tau) + fast[t] * tau\n",
    "\n",
    "  target_net.load_state_dict(slow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory():\n",
    "  def __init__(self, memory_size = 100000):\n",
    "    self.transitions = []\n",
    "    self.memory_size = memory_size\n",
    "    self.loc_pointer = 0\n",
    "  \n",
    "  def clear(self):\n",
    "    self.transitions = []\n",
    "    self.loc_pointer = 0\n",
    "  \n",
    "  def add(self, step_tuple):\n",
    "    # expect a tuple of transition contain:\n",
    "    # state, action, reward, next_state, ended\n",
    "    if len(self.transitions) <= self.loc_pointer:\n",
    "      self.transitions.append(None)\n",
    "    self.transitions[self.loc_pointer] = step_tuple\n",
    "    self.loc_pointer += 1\n",
    "    if self.loc_pointer >= self.memory_size:\n",
    "      self.loc_pointer %= self.memory_size\n",
    "  \n",
    "  def get_sample(self, batch_size):\n",
    "    return random.sample(self.transitions, batch_size)\n",
    "  \n",
    "  def usage(self):\n",
    "    return len(self.transitions) / self.memory_size\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNet_continuous(nn.Module):\n",
    "  def __init__(self, input_size, output_size):\n",
    "    super(PolicyNet_continuous,self).__init__()\n",
    "    self.l1_linear = nn.Linear(input_size, 64)\n",
    "    self.l2_linear = nn.Linear(64, 32)\n",
    "    self.l3_linear = nn.Linear(32, output_size)\n",
    "    nn.init.kaiming_normal_(self.l1_linear.weight)\n",
    "    nn.init.kaiming_normal_(self.l2_linear.weight)\n",
    "    self.l3_linear.weight.data.zero_()\n",
    "    \n",
    "  def forward(self,x):\n",
    "    out = F.relu(self.l1_linear(x))\n",
    "    out = F.relu(self.l2_linear(out))\n",
    "    out = F.tanh(self.l3_linear(out))\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNet(nn.Module):\n",
    "  def __init__(self, input_size):\n",
    "    super(QNet,self).__init__()\n",
    "    self.l1_linear = nn.Linear(input_size, 64)\n",
    "    self.l2_linear = nn.Linear(64, 32)\n",
    "    self.l3_linear = nn.Linear(32, 1)\n",
    "    nn.init.kaiming_normal_(self.l1_linear.weight)\n",
    "    nn.init.kaiming_normal_(self.l2_linear.weight)\n",
    "    self.l3_linear.weight.data.zero_()\n",
    "\n",
    "  def forward(self, state, action):\n",
    "    out = F.relu(self.l1_linear(torch.cat([state,action],dim=1)))\n",
    "    out = F.relu(self.l2_linear(out))\n",
    "    out = self.l3_linear(out)\n",
    "    return out    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPG():\n",
    "  def __init__(self, env, steps_in_state = 1):\n",
    "    self.state_value_range = [{'max':None, 'min':None}] * env.observation_space.shape[0]\n",
    "    self.replay_memory = ReplayMemory(memory_size=100000)\n",
    "    self.steps_in_state = steps_in_state\n",
    "    self.policy_target = PolicyNet_continuous(env.observation_space.shape[0] * steps_in_state, env.action_space.shape[0])\n",
    "    self.policy_eval = PolicyNet_continuous(env.observation_space.shape[0] * steps_in_state, env.action_space.shape[0])\n",
    "    self.Q_target = QNet(env.observation_space.shape[0] * steps_in_state + env.action_space.shape[0])\n",
    "    self.Q_eval = QNet(env.observation_space.shape[0] * steps_in_state + env.action_space.shape[0])\n",
    "    if use_cuda:\n",
    "      self.policy_target.cuda()\n",
    "      self.policy_eval.cuda()\n",
    "      self.Q_target.cuda()\n",
    "      self.Q_eval.cuda()\n",
    "    # copy the weights from target to eval net\n",
    "    self.policy_eval.load_state_dict(self.policy_target.state_dict())\n",
    "    self.Q_eval.load_state_dict(self.Q_target.state_dict())\n",
    "    self.env = env\n",
    "    self.range_scale = (env.action_space.high[0] - env.action_space.low[0]) / 2.0\n",
    "    self._gamma = 0.96\n",
    "    self._epsilon = 0.2\n",
    "    \n",
    "  def pick_action(self, state):\n",
    "    if np.random.rand() < self._epsilon:\n",
    "      action = (np.random.rand() * 2 - 1.0)\n",
    "    else:\n",
    "      probs = self.policy_target(state)\n",
    "      action_dist = Normal(probs, 0.01)\n",
    "      action = action_dist.rsample()\n",
    "      action = action.item()\n",
    "    return np.clip(action, -1.0, 1.0)\n",
    "  \n",
    "  def norm_state(self, state):\n",
    "    eps = np.finfo.eps\n",
    "    for i in range(len(self.state_value_range)):\n",
    "      value_range = state_range[i]\n",
    "      if state[i] > value_range.max or value_range.max == None:\n",
    "        value_range.max = state[i]\n",
    "      if state[i] < value_range.min or value_range.min == None:\n",
    "        value_range.min = state[i]\n",
    "      r = value_range.max - value_range.min\n",
    "      scale = 1 / (r + eps)\n",
    "      state[i] = state[i] - value_range.min\n",
    "  \n",
    "  def update_ddpg(self, batch):\n",
    "    (states, actions, rewards, next_states, ended) = zip(*batch)\n",
    "    states_tensor = torch.stack(states)\n",
    "    actions_tensor = FloatTensor(actions).view(-1,1)\n",
    "    rewards_tensor = FloatTensor(rewards).view(-1,1)\n",
    "    next_states_tensor = torch.stack(next_states)\n",
    "    ended_tensor = FloatTensor(ended).view(-1,1)\n",
    "\n",
    "    # Q loss = reward + gamma * Q_Net(next_states + policy_net(next_states)) - Q_Net(state + action)\n",
    "    # Q loss = F.mse(Q loss) \n",
    "    Q_target = rewards_tensor + self._gamma * (1 - ended_tensor) * self.Q_target(next_states_tensor, self.policy_target(next_states_tensor) * 2.0)\n",
    "    Q_loss = F.mse_loss(self.Q_target(states_tensor, actions_tensor), Q_target)\n",
    "    \n",
    "    # policy loss = \n",
    "    policy_loss = -self.Q_target(states_tensor, self.policy_target(states_tensor) * 2.0)\n",
    "    policy_loss = policy_loss.mean()\n",
    "    \n",
    "    self.Q_optimizer.zero_grad()\n",
    "    zero_grad(self.Q_target)\n",
    "    Q_loss.backward()\n",
    "    copy_grad(self.Q_target, self.Q_eval)\n",
    "    self.Q_optimizer.step()\n",
    "    \n",
    "    self.policy_optimizer.zero_grad()\n",
    "    zero_grad(self.policy_target)\n",
    "    policy_loss.backward()\n",
    "    copy_grad(self.policy_target, self.policy_eval)\n",
    "    self.policy_optimizer.step()\n",
    "    \n",
    "  def train(self, env, update_limit=1000, batch_size=200, update_per_episode=10, lr=1e-3, lr_policy=None, lr_q=None, checkpoint=100):\n",
    "    lr_policy = lr if lr_policy == None else lr_policy\n",
    "    lr_q = lr if lr_q == None else lr_q\n",
    "    self.policy_optimizer = torch.optim.SGD(self.policy_eval.parameters(), lr=lr_policy, weight_decay=1e-3)\n",
    "    self.Q_optimizer = torch.optim.SGD(self.Q_eval.parameters(), lr=lr_q, weight_decay=1e-3)\n",
    "    best_score = -99999\n",
    "    running_score = None\n",
    "    update_count = 0\n",
    "    while update_count < update_limit:\n",
    "      s0 = env.reset()\n",
    "      s0[2] /= 8\n",
    "      seq = [s0] * self.steps_in_state\n",
    "      state = FloatTensor(seq).view(-1)\n",
    "      episode = []\n",
    "      episode_ended = False\n",
    "      score = 0\n",
    "      while not episode_ended:\n",
    "        action =  self.pick_action(state)\n",
    "        (s1, reward, episode_ended, info) = env.step([action * 2.0])\n",
    "        s1[2] /= 8\n",
    "        seq = seq[1:]\n",
    "        seq.append(s1)\n",
    "        next_state = FloatTensor(seq).view(-1)\n",
    "        if episode_ended:\n",
    "          ended = 1\n",
    "        else:\n",
    "          ended = 0\n",
    "        self.replay_memory.add((state, action, reward, next_state, ended))\n",
    "\n",
    "        s0 = s1\n",
    "        state = next_state\n",
    "        score += reward\n",
    "        \n",
    "      if running_score == None:\n",
    "        running_score = score\n",
    "      else:\n",
    "        running_score = running_score * 0.9 + score * 0.1\n",
    "        \n",
    "      if self.replay_memory.usage() > 0.3:\n",
    "        if (update_count + 1) % checkpoint == 0:\n",
    "          is_best = False\n",
    "          if running_score > best_score:\n",
    "            is_best = True\n",
    "            save_torch_model(self.policy_target, 'model/ddpg_policy_best.pth')\n",
    "            best_score = running_score\n",
    "          save_torch_model(self.policy_target,'model/ddpg_policy_iter_%d.pth' %(update_count+1))\n",
    "          print('%d: running_score:%.2f, is_best:%s' %(update_count+1, running_score, is_best))\n",
    "        update_count += 1\n",
    "        for i in range(update_per_episode):\n",
    "          self.update_ddpg(self.replay_memory.get_sample(batch_size))\n",
    "        \n",
    "        if (update_count + 1) % 20 == 0:\n",
    "          update_target(self.policy_target, self.policy_eval, 0.1)\n",
    "          update_target(self.Q_target, self.Q_eval, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('Pendulum-v0')\n",
    "agent = DDPG(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent.train(env, update_limit=1, batch_size=2, update_per_episode=1, lr=1e-5, checkpoint=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100: running_score:-1216.66, is_best:True\n",
      "200: running_score:-1396.58, is_best:False\n",
      "300: running_score:-1443.62, is_best:False\n",
      "400: running_score:-1442.63, is_best:False\n",
      "500: running_score:-1453.49, is_best:False\n",
      "600: running_score:-1395.50, is_best:False\n",
      "700: running_score:-1375.38, is_best:False\n",
      "800: running_score:-1437.00, is_best:False\n",
      "900: running_score:-1438.07, is_best:False\n",
      "1000: running_score:-1373.70, is_best:False\n",
      "1100: running_score:-1460.23, is_best:False\n",
      "1200: running_score:-1473.71, is_best:False\n",
      "1300: running_score:-1419.84, is_best:False\n",
      "1400: running_score:-1323.61, is_best:False\n",
      "1500: running_score:-1452.72, is_best:False\n",
      "1600: running_score:-1444.70, is_best:False\n",
      "1700: running_score:-1417.82, is_best:False\n",
      "1800: running_score:-1416.24, is_best:False\n",
      "1900: running_score:-1425.59, is_best:False\n",
      "2000: running_score:-1441.20, is_best:False\n",
      "2100: running_score:-1418.61, is_best:False\n",
      "2200: running_score:-1423.54, is_best:False\n",
      "2300: running_score:-1356.16, is_best:False\n",
      "2400: running_score:-1400.53, is_best:False\n",
      "2500: running_score:-1448.95, is_best:False\n",
      "2600: running_score:-1416.46, is_best:False\n",
      "2700: running_score:-1384.95, is_best:False\n",
      "2800: running_score:-1470.64, is_best:False\n",
      "2900: running_score:-1389.52, is_best:False\n",
      "3000: running_score:-1435.28, is_best:False\n",
      "3100: running_score:-1409.40, is_best:False\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-7e7e1bf6c9ad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate_limit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate_per_episode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr_policy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr_q\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-21-f7900072187c>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, env, update_limit, batch_size, update_per_episode, lr, lr_policy, lr_q, checkpoint)\u001b[0m\n\u001b[1;32m     89\u001b[0m       \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m       \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mepisode_ended\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpick_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0;34m(\u001b[0m\u001b[0ms1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisode_ended\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m2.0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0ms1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-21-f7900072187c>\u001b[0m in \u001b[0;36mpick_action\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m       \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m       \u001b[0maction_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m       \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maction_dist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrsample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m       \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/pytorch0.4/lib/python3.6/site-packages/torch/distributions/normal.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, loc, scale, validate_args)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_args\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbroadcast_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNumber\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNumber\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0mbatch_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/pytorch0.4/lib/python3.6/site-packages/torch/distributions/utils.py\u001b[0m in \u001b[0;36mbroadcast_all\u001b[0;34m(*values)\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0mbroadcast_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_broadcast_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtensor_idxs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtensor_idxs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m             \u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbroadcast_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m         \u001b[0mtemplate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtensor_idxs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mscalar_idxs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "agent.train(env, update_limit=50000, batch_size=64, update_per_episode=5, lr_policy=1e-4, lr_q=1e-3, checkpoint=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch0.4]",
   "language": "python",
   "name": "conda-env-pytorch0.4-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
