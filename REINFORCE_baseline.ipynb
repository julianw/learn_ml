{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REINFORCE - Monte Carlo Policy Gradient Method for Cartpole\n",
    "\n",
    "REINFORCE is a on-policy monte carlo method. It let the agent complete the rollout and use the accumulated reward to update the policy. The basic idea in the vanilla version of REINFORCE is to reduce the probability of actions that are involved in a bad rollout, and increase the probablity of actions in a good roll out. So over time, the good action will be separated from the bad action.\n",
    "\n",
    "Sum of reward of a roll out $G = r_1 + \\_gamma * r_2 + \\_gamma^2 * r_3 + ... + \\_gamma^{n-1} * r_n$\n",
    "Then calculate the gradient for the probability of the action. If G is good, increase the probablility of the action, otherwise decrease it. \n",
    "\n",
    "It may seem obvious that equally assign blame to all actions in a bad roll out is not a good idea. There is an improved version of REINFORCE that try to estimate a baseline and assign blame / praise to action by comparing the reward to the base line.\n",
    "\n",
    "Baseline $A = G - Baseline$  \n",
    "Now update the action base on A instead of G.\n",
    "\n",
    "The intuition with this is sometimes the rewards are bound to be bad after certain point and nothing the agent can do to improve that. For example in pong, when the ball pass the bar. There is nothing the agent can save that.\n",
    "\n",
    "The usual choise of a baseline is a value function approximation. And it can be learnt by using the monte carlo reward collected by the policy. Then train a neural network with supervised learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.distributions import Categorical, Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "FloatTensor = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor if use_cuda else torch.LongTensor\n",
    "ByteTensor = torch.cuda.ByteTensor if use_cuda else torch.ByteTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_torch_model(model, filename):\n",
    "  if not os.path.exists(os.path.dirname(filename)):\n",
    "    os.makedirs(os.path.dirname(filename))\n",
    "  torch.save(model.state_dict(), filename)\n",
    "\n",
    "def load_torch_model(model, filename):\n",
    "  model.load_state_dict(torch.load(filename))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNet_discret(nn.Module):\n",
    "  def __init__(self, input_size, output_size):\n",
    "    super(PolicyNet_discret,self).__init__()\n",
    "    self.l1_linear = nn.Linear(input_size, 256)\n",
    "    self.l2_linear = nn.Linear(256, 128)\n",
    "    self.l3_linear = nn.Linear(128, output_size)\n",
    "    nn.init.kaiming_normal_(self.l1_linear.weight)\n",
    "    nn.init.kaiming_normal_(self.l2_linear.weight)\n",
    "    self.l3_linear.weight.data.zero_()\n",
    "    \n",
    "  def forward(self,x):\n",
    "    out = F.relu(self.l1_linear(x))\n",
    "    out = F.relu(self.l2_linear(out))\n",
    "    out = F.softmax(self.l3_linear(out),dim=0)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueNet(nn.Module):\n",
    "  def __init__(self, input_size):\n",
    "    super(ValueNet,self).__init__()\n",
    "    self.l1_linear = nn.Linear(input_size, 256)\n",
    "    self.l2_linear = nn.Linear(256, 128)\n",
    "    self.l3_linear = nn.Linear(128, 1)\n",
    "    nn.init.kaiming_normal_(self.l1_linear.weight)\n",
    "    nn.init.kaiming_normal_(self.l2_linear.weight)\n",
    "    self.l3_linear.weight.data.zero_()\n",
    "\n",
    "  def forward(self, x):\n",
    "    out = F.relu(self.l1_linear(x))\n",
    "    out = F.relu(self.l2_linear(out))\n",
    "    out = self.l3_linear(out)\n",
    "    return out    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "class REINFORCE_wBaseline():\n",
    "  def __init__(self, env, steps_in_state = 2):\n",
    "    self.steps_in_state = steps_in_state\n",
    "    self.policy = PolicyNet_discret(env.observation_space.shape[0] * steps_in_state,env.action_space.n)\n",
    "    self.value = ValueNet(env.observation_space.shape[0] * steps_in_state)\n",
    "    self.env = env\n",
    "    self._gamma = 0.96\n",
    "    \n",
    "  def predict_value(self, state):\n",
    "    return self.value(state)\n",
    "  \n",
    "  def predict_action(self, state):\n",
    "    return self.policy(state)\n",
    "  \n",
    "  def pick_action(self, state):\n",
    "    probs = self.predict_action(state)\n",
    "    action_dist = Categorical(probs)\n",
    "    action = action_dist.sample()\n",
    "    action = action.item()\n",
    "    return (action, action_dist.log_prob(FloatTensor([action])))\n",
    "  \n",
    "  def update_policy_and_value(self, episode):\n",
    "    (states, actions, rewards, log_probs) = zip(*episode)\n",
    "    \n",
    "    MC_rewards = []\n",
    "    R = 0\n",
    "    for r in rewards[::-1]:\n",
    "      R = r + self._gamma * R\n",
    "      MC_rewards.insert(0, R)\n",
    "      \n",
    "    value_prediction = self.value(torch.stack(states))\n",
    "    value_loss = F.mse_loss(value_prediction, FloatTensor(MC_rewards).view(-1,1))\n",
    "    \n",
    "    policy_loss = []\n",
    "    for (log_prob, reward, baseline) in zip(log_probs, MC_rewards, value_prediction.view(-1).tolist()):\n",
    "      policy_loss.append(-log_prob*(reward - baseline))\n",
    "    policy_loss = torch.stack(policy_loss).sum()\n",
    "    \n",
    "    self.value_optimizer.zero_grad()\n",
    "    value_loss.backward()\n",
    "    self.value_optimizer.step()\n",
    "    \n",
    "    self.policy_optimizer.zero_grad()\n",
    "    policy_loss.backward()\n",
    "    self.policy_optimizer.step()\n",
    "\n",
    "  def train(self, env, update_limit=1000, samples_per_update=200, lr=1e-3, lr_policy=None, lr_value=None, checkpoint=100):\n",
    "    lr_policy = lr if lr_policy == None else lr_policy\n",
    "    lr_value = lr if lr_value == None else lr_value\n",
    "    self.policy_optimizer = torch.optim.Adam(self.policy.parameters(), lr=lr_policy, weight_decay=1e-3)\n",
    "    self.value_optimizer = torch.optim.Adam(self.value.parameters(), lr=lr_value, weight_decay=1e-3)\n",
    "    best_score = -99999\n",
    "    running_score = None\n",
    "    samples = []\n",
    "    samples_per_update = samples_per_update\n",
    "    update_count = 0\n",
    "    while update_count < update_limit:\n",
    "      s0 = env.reset()\n",
    "      seq = [s0] * self.steps_in_state\n",
    "      state = FloatTensor(seq).view(-1)\n",
    "      episode_ended = False\n",
    "      score = 0\n",
    "      while not episode_ended:\n",
    "        (action, log_prob) =  self.pick_action(state)\n",
    "        (s1, reward, episode_ended, info) = env.step(action)\n",
    "        samples.append((state, action, reward, log_prob))\n",
    "        seq = seq[1:]\n",
    "        seq.append(s1)\n",
    "        next_state = FloatTensor(seq).view(-1)\n",
    "        if episode_ended:\n",
    "          ended = 1\n",
    "        else:\n",
    "          ended = 0\n",
    "        s0 = s1\n",
    "        state = next_state\n",
    "        score += reward\n",
    "        \n",
    "      if running_score == None:\n",
    "        running_score = score\n",
    "      else:\n",
    "        running_score = running_score * 0.9 + score * 0.1\n",
    "        \n",
    "      if len(samples) > samples_per_update:\n",
    "        if (update_count + 1) % checkpoint == 0:\n",
    "          is_best = False\n",
    "          if running_score > best_score:\n",
    "            is_best = True\n",
    "            save_torch_model(self.policy, 'model/reinforce_cartpole_policy_best.pth')\n",
    "            best_score = running_score\n",
    "          save_torch_model(self.policy,'model/reinforce_cartpole_policy_iter_%d.pth' %(update_count+1))\n",
    "          print('%d: running_score:%.2f, is_best:%s' %(update_count+1, running_score, is_best))\n",
    "        update_count += 1\n",
    "        self.update_policy_and_value(samples)\n",
    "        samples = []\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "agent = REINFORCE_wBaseline(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200: running_score:24.56, is_best:True\n",
      "400: running_score:25.27, is_best:True\n",
      "600: running_score:25.59, is_best:True\n",
      "800: running_score:25.76, is_best:True\n",
      "1000: running_score:37.80, is_best:True\n",
      "1200: running_score:48.21, is_best:True\n",
      "1400: running_score:104.48, is_best:True\n",
      "1600: running_score:154.74, is_best:True\n",
      "1800: running_score:176.12, is_best:True\n",
      "2000: running_score:195.02, is_best:True\n"
     ]
    }
   ],
   "source": [
    "agent.train(env, update_limit=2000, samples_per_update=100, lr=1e-4, checkpoint=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class REINFORCE_wBaseline_MC():\n",
    "  def __init__(self, env, steps_in_state = 2):\n",
    "    self.steps_in_state = steps_in_state\n",
    "    self.policy = PolicyNet_discret(env.observation_space.shape[0] * steps_in_state,env.action_space.n)\n",
    "    self.value = ValueNet(env.observation_space.shape[0] * steps_in_state)\n",
    "    self.env = env\n",
    "    self._gamma = 0.96\n",
    "    \n",
    "  def predict_value(self, state):\n",
    "    return self.value(state)\n",
    "  \n",
    "  def predict_action(self, state):\n",
    "    return self.policy(state)\n",
    "  \n",
    "  def pick_action(self, state):\n",
    "    probs = self.predict_action(state)\n",
    "    action_dist = Categorical(probs)\n",
    "    action = action_dist.sample()\n",
    "    action = action.item()\n",
    "    return (action, action_dist.log_prob(FloatTensor([action])))\n",
    "  \n",
    "  def update_policy_and_value(self, episode):\n",
    "    (states, actions, rewards, log_probs) = zip(*episode)\n",
    "    \n",
    "    MC_rewards = []\n",
    "    R = 0\n",
    "    for r in rewards[::-1]:\n",
    "      R = r + self._gamma * R\n",
    "      MC_rewards.insert(0, R)\n",
    "      \n",
    "    value_prediction = self.value(torch.stack(states))\n",
    "    value_loss = F.mse_loss(value_prediction, FloatTensor(MC_rewards).view(-1,1))\n",
    "    \n",
    "    policy_loss = []\n",
    "    for (log_prob, reward, baseline) in zip(log_probs, MC_rewards, value_prediction.view(-1).tolist()):\n",
    "      policy_loss.append(-log_prob*(reward - baseline))\n",
    "    policy_loss = torch.stack(policy_loss).sum()\n",
    "    \n",
    "    self.value_optimizer.zero_grad()\n",
    "    value_loss.backward()\n",
    "    self.value_optimizer.step()\n",
    "    \n",
    "    self.policy_optimizer.zero_grad()\n",
    "    policy_loss.backward()\n",
    "    self.policy_optimizer.step()\n",
    "\n",
    "  def train(self, env, update_limit=1000, samples_per_update=200, lr=1e-3, lr_policy=None, lr_value=None, checkpoint=100):\n",
    "    lr_policy = lr if lr_policy == None else lr_policy\n",
    "    lr_value = lr if lr_value == None else lr_value\n",
    "    self.policy_optimizer = torch.optim.Adam(self.policy.parameters(), lr=lr_policy, weight_decay=1e-3)\n",
    "    self.value_optimizer = torch.optim.Adam(self.value.parameters(), lr=lr_value, weight_decay=1e-3)\n",
    "    best_score = -99999\n",
    "    running_score = None\n",
    "    samples = []\n",
    "    samples_per_update = samples_per_update\n",
    "    update_count = 0\n",
    "    while update_count < update_limit:\n",
    "      s0 = env.reset()\n",
    "      seq = [s0] * self.steps_in_state\n",
    "      state = FloatTensor(seq).view(-1)\n",
    "      episode_ended = False\n",
    "      score = 0\n",
    "      while not episode_ended:\n",
    "        (action, log_prob) =  self.pick_action(state)\n",
    "        (s1, reward, episode_ended, info) = env.step(action)\n",
    "        if reward > 0:\n",
    "          print('hit goal!!!')\n",
    "        reward = abs(s1[0] - 0.5)\n",
    "        samples.append((state, action, reward, log_prob))\n",
    "        seq = seq[1:]\n",
    "        seq.append(s1)\n",
    "        next_state = FloatTensor(seq).view(-1)\n",
    "        if episode_ended:\n",
    "          ended = 1\n",
    "        else:\n",
    "          ended = 0\n",
    "        s0 = s1\n",
    "        state = next_state\n",
    "        score += reward\n",
    "        \n",
    "      if running_score == None:\n",
    "        running_score = score\n",
    "      else:\n",
    "        running_score = running_score * 0.9 + score * 0.1\n",
    "        \n",
    "      if len(samples) > samples_per_update:\n",
    "        if (update_count + 1) % checkpoint == 0:\n",
    "          is_best = False\n",
    "          if running_score > best_score:\n",
    "            is_best = True\n",
    "            save_torch_model(self.policy, 'model/reinforce_cartpole_policy_best.pth')\n",
    "            best_score = running_score\n",
    "          save_torch_model(self.policy,'model/reinforce_cartpole_policy_iter_%d.pth' %(update_count+1))\n",
    "          print('%d: running_score:%.2f, is_best:%s' %(update_count+1, running_score, is_best))\n",
    "        update_count += 1\n",
    "        self.update_policy_and_value(samples)\n",
    "        samples = []\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('MountainCar-v0')\n",
    "agent = REINFORCE_wBaseline_MC(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200: running_score:203.67, is_best:True\n",
      "400: running_score:207.20, is_best:True\n",
      "600: running_score:227.50, is_best:True\n",
      "800: running_score:232.21, is_best:True\n",
      "1000: running_score:233.59, is_best:True\n",
      "1200: running_score:234.64, is_best:True\n",
      "1400: running_score:235.61, is_best:True\n",
      "1600: running_score:235.61, is_best:False\n",
      "1800: running_score:235.38, is_best:False\n",
      "2000: running_score:235.31, is_best:False\n"
     ]
    }
   ],
   "source": [
    "agent.train(env, update_limit=2000, samples_per_update=500, lr=1e-4, checkpoint=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('KungFuMaster-ram-v0')\n",
    "env = gym.make('Pendulum-v0')\n",
    "def avg_reward_random(env, iteration):\n",
    "  total_reward = 0.0\n",
    "  for i in range(iteration):\n",
    "    env.reset()\n",
    "    ended = False\n",
    "    while not ended:\n",
    "      (state, reward, ended, info) = env.step(env.action_space.sample())\n",
    "      reward = abs(state[0] - 0.5)\n",
    "      total_reward += reward\n",
    "  print (total_reward / iteration)\n",
    "avg_reward_random(env, 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch0.4]",
   "language": "python",
   "name": "conda-env-pytorch0.4-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
