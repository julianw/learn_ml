{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actor Critic Cartpole\n",
    "\n",
    "The base Actor Critic method is very similar to REINFORCE with baseline. The different in implementation here is instead of use  $G_t = \\sum_{k=0}^T\\gamma^k*r_{t+k}$ to calculate the policy gradient estimator $loss=\\sum_{t=0}^T[log(\\pi_\\theta(state_t))*G_t]$, it replace $G_t$ with $reward_t + \\gamma*V(state_{t+1})$. Such that $loss=\\sum_{t=0}^T[log(\\pi_\\theta(state))*(reward_t + \\gamma*V(state_{t+1}) - V(state_t))]$. This actually improve the algorithm as $G_t$ is a high variance estimation of future reward while the state-value function learn to return the expected reward of current policy given state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from IPython import display\n",
    "from IPython.display import HTML\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.distributions import Normal, Categorical\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "FloatTensor = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor if use_cuda else torch.LongTensor\n",
    "ByteTensor = torch.cuda.ByteTensor if use_cuda else torch.ByteTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_torch_model(model, filename):\n",
    "  if not os.path.exists(os.path.dirname(filename)):\n",
    "    os.makedirs(os.path.dirname(filename))\n",
    "  torch.save(model.state_dict(), filename)\n",
    "\n",
    "def load_torch_model(model, filename):\n",
    "  model.load_state_dict(torch.load(filename))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_grad(source, target):\n",
    "  grads = []\n",
    "  for param in source.parameters():\n",
    "    grads.append(param.grad.clone())\n",
    "  grads.reverse()\n",
    "  for param in target.parameters():\n",
    "    param.grad = grads.pop()\n",
    "\n",
    "def zero_grad(model):\n",
    "  for param in model.parameters():\n",
    "    if type(param.grad) != type(None):\n",
    "      param.grad.data.zero_()\n",
    "      \n",
    "def update_target(target_net, eval_net, tau):\n",
    "  fast = eval_net.state_dict()\n",
    "  slow = target_net.state_dict()\n",
    "  for t in slow:\n",
    "    slow[t] = slow[t] * (1. - tau) + fast[t] * tau\n",
    "\n",
    "  target_net.load_state_dict(slow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNet_discret(nn.Module):\n",
    "  def __init__(self, input_size, output_size):\n",
    "    super(PolicyNet_discret,self).__init__()\n",
    "    self.l1_linear = nn.Linear(input_size, 512)\n",
    "    self.l2_linear = nn.Linear(512, 256)\n",
    "    self.l3_linear = nn.Linear(256, output_size)\n",
    "    nn.init.kaiming_normal_(self.l1_linear.weight)\n",
    "    nn.init.kaiming_normal_(self.l2_linear.weight)\n",
    "    self.l3_linear.weight.data.zero_()\n",
    "    \n",
    "  def forward(self,x):\n",
    "    out = F.relu(self.l1_linear(x))\n",
    "    out = F.relu(self.l2_linear(out))\n",
    "    out = F.softmax(self.l3_linear(out),dim=0)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueNet(nn.Module):\n",
    "  def __init__(self, input_size):\n",
    "    super(ValueNet,self).__init__()\n",
    "    self.l1_linear = nn.Linear(input_size, 512)\n",
    "    self.l2_linear = nn.Linear(512,256)\n",
    "    self.l3_linear = nn.Linear(256, 1)\n",
    "    nn.init.kaiming_normal_(self.l1_linear.weight)\n",
    "    nn.init.kaiming_normal_(self.l2_linear.weight)\n",
    "    self.l3_linear.weight.data.zero_()\n",
    "\n",
    "  def forward(self, x):\n",
    "    out = F.relu(self.l1_linear(x))\n",
    "    out = F.relu(self.l2_linear(out))\n",
    "    out = self.l3_linear(out)\n",
    "    return out    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic():\n",
    "  def __init__(self, env, steps_in_state = 2):\n",
    "    self.steps_in_state = steps_in_state\n",
    "    self.policy = PolicyNet_discret(env.observation_space.shape[0] * steps_in_state,env.action_space.n)\n",
    "    self.value = ValueNet(env.observation_space.shape[0] * steps_in_state)\n",
    "    if use_cuda:\n",
    "      self.policy.cuda()\n",
    "      self.value.cuda()\n",
    "    self.env = env\n",
    "    self._gamma = 0.96\n",
    "    \n",
    "  def pick_action(self, state):\n",
    "    probs = self.policy(state)\n",
    "    action_dist = Categorical(probs)\n",
    "    action = action_dist.sample()\n",
    "    action = action.item()\n",
    "    return (action, action_dist.log_prob(FloatTensor([action])))\n",
    "  \n",
    "  def update_actor_critic(self, episode):\n",
    "    (states, actions, rewards, next_states, log_probs, ended) = zip(*episode)\n",
    "    \n",
    "    rewards = FloatTensor(rewards)\n",
    "    ended = FloatTensor(ended)\n",
    "    state_value = self.value(torch.stack(states))\n",
    "    next_state_value = self.value(torch.stack(next_states))\n",
    "    target_value = rewards + (1 - ended) * self._gamma * next_state_value\n",
    "    \n",
    "    delta = target_value - state_value\n",
    "\n",
    "    value_loss = F.mse_loss(state_value, target_value)\n",
    "    \n",
    "    policy_loss = []\n",
    "    for log_prob, d in zip(log_probs, delta):\n",
    "      policy_loss.append(-log_prob * d)\n",
    "    policy_loss = torch.stack(policy_loss).sum()\n",
    "    \n",
    "    self.value_optimizer.zero_grad()\n",
    "    value_loss.backward(retain_graph=True)\n",
    "    self.value_optimizer.step()\n",
    "    \n",
    "    self.policy_optimizer.zero_grad()\n",
    "    policy_loss.backward()\n",
    "    self.policy_optimizer.step()\n",
    "\n",
    "  def train(self, env, episode, lr=1e-3, target_copylr=1e-3, lr_policy=None, lr_value=None, checkpoint=100):\n",
    "    lr_policy = lr if lr_policy == None else lr_policy\n",
    "    lr_value = lr if lr_value == None else lr_value\n",
    "    self.policy_optimizer = torch.optim.Adam(self.policy.parameters(), lr=lr_policy, weight_decay=1e-3)\n",
    "    self.value_optimizer = torch.optim.Adam(self.value.parameters(), lr=lr_value, weight_decay=1e-3)\n",
    "    best_score = -99999\n",
    "    running_score = None\n",
    "    for i in range(episode):\n",
    "      s0 = env.reset()\n",
    "      seq = [s0] * self.steps_in_state\n",
    "      state = FloatTensor(seq).view(-1)\n",
    "      episode = []\n",
    "      episode_ended = False\n",
    "      score = 0\n",
    "      while not episode_ended:\n",
    "        (action, log_prob) =  self.pick_action(state)\n",
    "        (s1, reward, episode_ended, info) = env.step(action)\n",
    "        seq = seq[1:]\n",
    "        seq.append(s1)\n",
    "        next_state = FloatTensor(seq).view(-1)\n",
    "        if episode_ended:\n",
    "          ended = 1\n",
    "        else:\n",
    "          ended = 0\n",
    "        episode.append((state, action, reward, next_state, log_prob, ended))\n",
    "\n",
    "        s0 = s1\n",
    "        state = next_state\n",
    "        score += reward\n",
    "        \n",
    "      if running_score == None:\n",
    "        running_score = score\n",
    "      else:\n",
    "        running_score = running_score * 0.9 + score * 0.1\n",
    "        \n",
    "      if (i + 1) % checkpoint == 0:\n",
    "        is_best = False\n",
    "        if running_score > best_score:\n",
    "          save_torch_model(self.policy, 'model/actor_critic_cartpole_policy_best.pth')\n",
    "          best_score = running_score\n",
    "        save_torch_model(self.policy,'model/actor_critic_cartpole_policy_iter_%d.pth' %(i+1))\n",
    "        print('%d: running_score:%.2f' %(i+1, running_score))\n",
    "\n",
    "      self.update_actor_critic(episode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "agent = ActorCritic(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100: running_score:37.28\n",
      "200: running_score:66.70\n",
      "300: running_score:131.67\n",
      "400: running_score:172.97\n",
      "500: running_score:194.11\n",
      "600: running_score:199.86\n",
      "700: running_score:188.74\n",
      "800: running_score:199.47\n",
      "900: running_score:184.77\n",
      "1000: running_score:198.16\n"
     ]
    }
   ],
   "source": [
    "agent.train(env, 1000, lr_policy=1e-4, lr_value=1e-3,checkpoint=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch0.4]",
   "language": "python",
   "name": "conda-env-pytorch0.4-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
