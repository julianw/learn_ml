{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Deterministic Policy Gradient (DDPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This implement the pseudo code in [CONTINUOUS CONTROL WITH DEEP REINFORCEMENT\n",
    "LEARNING](https://arxiv.org/pdf/1509.02971.pdf) from Deepmind in pytorch.\n",
    "<img src=\"ddpg_pseudocode.png\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One change in the code here $\\mathcal{N}$ is just a uniform random number of -0.5 to 0.5 that decay over number of iteration, $\\mathcal{N} = (rand() - 0.5) / (1 + 1e^{-3} * iteration)$. In the paper they used Ornstein-Uhlenbeck process for $\\mathcal{N}$, but for simple task like pendulum the uniform random noise seems to work just fine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation Detail\n",
    "\n",
    "The way how DDPG update its Q value function and policy is a quite interesting. The pseudocode is actually pretty detail, though a simple diagram might help visualize the deligate part.\n",
    "\n",
    "In DDPG it start out with policy(P) and policy_prime(P') with same weight $\\theta^\\mu$. Same thing for Q, Q(Q) and Q_prime(Q') with $\\theta^Q$. Then it sample the environment using policy(P). After it collected enough transitions from the environment, it sample a mini batch from replay memory at each timestep to perform an update. Inside the update process, it calculate the Q loss and policy loss as follow:\n",
    "\n",
    "<img src=\"Q_loss.png\" width=\"500\"/>  \n",
    "It use policy_prime and Q_prime to caculate the predicted next-state-action value and use it compute the Q target\n",
    "$Q\\_target=reward + \\gamma * Q'(S\\_next,P'(S\\_next))$  \n",
    "$Q\\_loss = MSE(Q\\_target - Q(S,A))$ where $Q\\_target$ is using Q' and P' while $Q(S,A)$ just use Q.   \n",
    "Then do a backprop on Q with $Q\\_loss$\n",
    "\n",
    "For the policy update:  \n",
    "<img src=\"ddpg_policy_grad.png\" width=\"350\"/>  \n",
    "Actuall it just mean the gradient of $\\theta^\\mu$ with respect to the mean of $Q(s,P(s))$. Here in pytorch it performs gradient descent, so the policy loss is $-\\overline{Q(s,P(s))}$. It use the policy loss to update P.\n",
    "\n",
    "After both P and Q are update, do a soft copy of the parameters in the neural network from P to P'. e.g. parameters of P' \\* 0.9 + parameters of P \\* 0.1. So P will take a full step on the gradient while P' will trail behind it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from IPython import display\n",
    "from IPython.display import HTML\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.distributions import Normal, Categorical\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "FloatTensor = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor if use_cuda else torch.LongTensor\n",
    "ByteTensor = torch.cuda.ByteTensor if use_cuda else torch.ByteTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_torch_model(model, filename):\n",
    "  if not os.path.exists(os.path.dirname(filename)):\n",
    "    os.makedirs(os.path.dirname(filename))\n",
    "  torch.save(model.state_dict(), filename)\n",
    "\n",
    "def load_torch_model(model, filename):\n",
    "  model.load_state_dict(torch.load(filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_grad(source, target):\n",
    "  grads = []\n",
    "  for param in source.parameters():\n",
    "    grads.append(param.grad.clone())\n",
    "  grads.reverse()\n",
    "  for param in target.parameters():\n",
    "    param.grad = grads.pop()\n",
    "\n",
    "def zero_grad(model):\n",
    "  for param in model.parameters():\n",
    "    if type(param.grad) != type(None):\n",
    "      param.grad.data.zero_()\n",
    "      \n",
    "def update_target(target_net, eval_net, tau):\n",
    "  fast = eval_net.state_dict()\n",
    "  slow = target_net.state_dict()\n",
    "  for t in slow:\n",
    "    slow[t] = slow[t] * (1. - tau) + fast[t] * tau\n",
    "\n",
    "  target_net.load_state_dict(slow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory():\n",
    "  def __init__(self, memory_size = 100000):\n",
    "    self.transitions = []\n",
    "    self.memory_size = memory_size\n",
    "    self.loc_pointer = 0\n",
    "  \n",
    "  def clear(self):\n",
    "    self.transitions = []\n",
    "    self.loc_pointer = 0\n",
    "  \n",
    "  def add(self, step_tuple):\n",
    "    # expect a tuple of transition contain:\n",
    "    # state, action, reward, next_state, ended\n",
    "    if len(self.transitions) <= self.loc_pointer:\n",
    "      self.transitions.append(None)\n",
    "    self.transitions[self.loc_pointer] = step_tuple\n",
    "    self.loc_pointer += 1\n",
    "    if self.loc_pointer >= self.memory_size:\n",
    "      self.loc_pointer %= self.memory_size\n",
    "  \n",
    "  def get_sample(self, batch_size):\n",
    "    return random.sample(self.transitions, batch_size)\n",
    "  \n",
    "  def usage(self):\n",
    "    return len(self.transitions) / self.memory_size\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNet_continuous(nn.Module):\n",
    "  def __init__(self, input_size, output_size):\n",
    "    super(PolicyNet_continuous,self).__init__()\n",
    "    self.l1_linear = nn.Linear(input_size, 128)\n",
    "    self.l2_linear = nn.Linear(128, 64)\n",
    "    self.l3_linear = nn.Linear(64, output_size)\n",
    "    nn.init.kaiming_normal_(self.l1_linear.weight)\n",
    "    nn.init.kaiming_normal_(self.l2_linear.weight)\n",
    "    self.l3_linear.weight.data.zero_()\n",
    "    \n",
    "  def forward(self,x):\n",
    "    out = F.relu(self.l1_linear(x))\n",
    "    out = F.relu(self.l2_linear(out))\n",
    "    out = F.tanh(self.l3_linear(out))\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNet(nn.Module):\n",
    "  def __init__(self, input_size):\n",
    "    super(QNet,self).__init__()\n",
    "    self.l1_linear = nn.Linear(input_size, 128)\n",
    "    self.l2_linear = nn.Linear(128, 64)\n",
    "    self.l3_linear = nn.Linear(64, 1)\n",
    "    nn.init.kaiming_normal_(self.l1_linear.weight)\n",
    "    nn.init.kaiming_normal_(self.l2_linear.weight)\n",
    "    self.l3_linear.weight.data.zero_()\n",
    "\n",
    "  def forward(self, state, action):\n",
    "    out = F.relu(self.l1_linear(torch.cat([state,action],dim=1)))\n",
    "    out = F.relu(self.l2_linear(out))\n",
    "    out = self.l3_linear(out)\n",
    "    return out    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPG():\n",
    "  def __init__(self, env, steps_in_state = 1):\n",
    "    self.is_training = True\n",
    "    self.state_value_range = [{'max':None, 'min':None}] * env.observation_space.shape[0]\n",
    "    self.replay_memory = ReplayMemory(memory_size=100000)\n",
    "    self.steps_in_state = steps_in_state\n",
    "    self.actor = PolicyNet_continuous(env.observation_space.shape[0] * steps_in_state, env.action_space.shape[0])\n",
    "    self.actor_prime = PolicyNet_continuous(env.observation_space.shape[0] * steps_in_state, env.action_space.shape[0])\n",
    "    self.critic = QNet(env.observation_space.shape[0] * steps_in_state + env.action_space.shape[0])\n",
    "    self.critic_prime = QNet(env.observation_space.shape[0] * steps_in_state + env.action_space.shape[0])\n",
    "    if use_cuda:\n",
    "      self.actor.cuda()\n",
    "      self.actor_prime.cuda()\n",
    "      self.critic.cuda()\n",
    "      self.critic_prime.cuda()\n",
    "    # copy the weights from target to eval net\n",
    "    self.actor_prime.load_state_dict(self.actor.state_dict())\n",
    "    self.critic_prime.load_state_dict(self.critic.state_dict())\n",
    "    self.env = env\n",
    "    self.range_scale = (env.action_space.high[0] - env.action_space.low[0]) / 2.0\n",
    "    self._gamma = 0.96\n",
    "\n",
    "  def pick_action(self, state):\n",
    "    action = self.actor(state)\n",
    "    action = action.item()\n",
    "    # add noise\n",
    "    if self.is_training:\n",
    "      action += (np.random.rand() - 0.5) / (1 + 1e-3 * self.iteration)\n",
    "    return np.clip(action, -1.0, 1.0)\n",
    "  \n",
    "  def update_ddpg(self, batch):\n",
    "    (states, actions, rewards, next_states, ended) = zip(*batch)\n",
    "    states_tensor = torch.stack(states)\n",
    "    actions_tensor = FloatTensor(actions).view(-1,1)\n",
    "    rewards_tensor = FloatTensor(rewards).view(-1,1)\n",
    "    next_states_tensor = torch.stack(next_states)\n",
    "    ended_tensor = FloatTensor(ended).view(-1,1)\n",
    "\n",
    "    critic_target = rewards_tensor + self._gamma * (1 - ended_tensor) * \\\n",
    "      self.critic_prime(next_states_tensor, self.actor_prime(next_states_tensor))\n",
    "    critic_loss = F.mse_loss(self.critic(states_tensor, actions_tensor), critic_target)\n",
    "\n",
    "    self.critic_coptimizer.zero_grad()\n",
    "    critic_loss.backward()\n",
    "    self.critic_coptimizer.step()\n",
    "    \n",
    "    actor_loss = -self.critic(states_tensor, self.actor(states_tensor))\n",
    "    actor_loss = actor_loss.mean()\n",
    "    \n",
    "    self.actor_optimizer.zero_grad()\n",
    "    actor_loss.backward()\n",
    "    self.actor_optimizer.step()\n",
    "    \n",
    "    update_target(self.critic_prime, self.critic, 0.1)\n",
    "    update_target(self.actor_prime, self.actor, 0.1)\n",
    "    \n",
    "  def train(self, env, update_limit=1000, batch_size=200, lr=1e-3, lr_actor=None, lr_critic=None, checkpoint=100):\n",
    "    lr_actor = lr if lr_actor == None else lr_actor\n",
    "    lr_critic = lr if lr_critic == None else lr_critic\n",
    "    \n",
    "    self.actor_optimizer = torch.optim.SGD(self.actor.parameters(), lr=lr_actor, weight_decay=1e-3)\n",
    "    self.critic_coptimizer = torch.optim.SGD(self.critic.parameters(), lr=lr_critic, weight_decay=1e-3)\n",
    "    \n",
    "    best_score = -99999\n",
    "    running_score = None\n",
    "    update_count = 0\n",
    "    self.iteration = 0\n",
    "    while update_count < update_limit:\n",
    "      s0 = env.reset()\n",
    "      seq = [s0] * self.steps_in_state\n",
    "      state = FloatTensor(seq).view(-1)\n",
    "      episode_ended = False\n",
    "      score = 0\n",
    "      while not episode_ended:\n",
    "        action =  self.pick_action(state)\n",
    "        (s1, reward, episode_ended, info) = env.step([action * self.range_scale])\n",
    "        seq = seq[1:]\n",
    "        seq.append(s1)\n",
    "        next_state = FloatTensor(seq).view(-1)\n",
    "        if episode_ended:\n",
    "          ended = 1\n",
    "        else:\n",
    "          ended = 0\n",
    "        self.replay_memory.add((state, action, reward, next_state, ended))\n",
    "        if self.replay_memory.usage() > 0.8:\n",
    "          if (update_count + 1) % checkpoint == 0:\n",
    "            save_torch_model(self.actor,'model/ddpg_actor_iter_%d.pth' %(update_count+1))\n",
    "            print('%d: running_score:%.2f, ' %(update_count+1, running_score))\n",
    "          self.update_ddpg(self.replay_memory.get_sample(batch_size))\n",
    "          update_count += 1\n",
    "          self.iteration += 1\n",
    "\n",
    "        s0 = s1\n",
    "        state = next_state\n",
    "        score += reward\n",
    "      if running_score == None:\n",
    "        running_score = score\n",
    "      else:\n",
    "        running_score = running_score * 0.9 + score * 0.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('Pendulum-v0')\n",
    "agent = DDPG(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent.train(env, update_limit=1, batch_size=2, update_per_episode=1, lr=1e-5, checkpoint=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000: running_score:-1187.10, \n",
      "2000: running_score:-1147.86, \n",
      "3000: running_score:-1038.53, \n",
      "4000: running_score:-1065.64, \n",
      "5000: running_score:-956.23, \n",
      "6000: running_score:-893.80, \n",
      "7000: running_score:-906.27, \n",
      "8000: running_score:-829.95, \n",
      "9000: running_score:-879.97, \n",
      "10000: running_score:-724.52, \n",
      "11000: running_score:-529.79, \n",
      "12000: running_score:-505.88, \n",
      "13000: running_score:-373.68, \n",
      "14000: running_score:-312.37, \n",
      "15000: running_score:-259.21, \n",
      "16000: running_score:-215.04, \n",
      "17000: running_score:-199.45, \n",
      "18000: running_score:-209.75, \n",
      "19000: running_score:-203.33, \n",
      "20000: running_score:-241.67, \n"
     ]
    }
   ],
   "source": [
    "agent.is_training = True\n",
    "agent.train(env, update_limit=20000, batch_size=64, lr_actor=1e-4, lr_critic=1e-3, checkpoint=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-129.79144899953593\n"
     ]
    }
   ],
   "source": [
    "# use the final mean as solution and run a sample episode\n",
    "agent.is_training = False\n",
    "load_torch_model(agent.actor,'model/ddpg_actor_iter_16000.pth')\n",
    "state = env.reset()\n",
    "frames = []\n",
    "frames.append(env.render(mode='rgb_array'))\n",
    "ended = False\n",
    "score = 0\n",
    "while not ended:\n",
    "  action = agent.pick_action(FloatTensor([state]).view(-1))\n",
    "  (state, reward, ended, info) = env.step([action*2])\n",
    "  score += reward\n",
    "  frames.append(env.render(mode='rgb_array'))\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "def animate(frames):\n",
    "  fig, ax = plt.subplots()\n",
    "  ax.grid('off')\n",
    "  ax.axis('off')\n",
    "  ims = []\n",
    "  for i in range(len(frames)):\n",
    "      im = plt.imshow(frames[i], animated=True)\n",
    "      ims.append([im])\n",
    "  ani = animation.ArtistAnimation(fig, ims, interval=20, blit=True, repeat_delay=1000)\n",
    "  return ani\n",
    "\n",
    "ani = animate(frames)\n",
    "ani.save('pendulum_ddpg.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video width=\"400\" controls loop>\n",
       "  <source src=\"pendulum_ddpg.mp4\" type=\"video/mp4\">\n",
       "</video>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<video width=\"400\" controls loop>\n",
    "  <source src=\"pendulum_ddpg.mp4\" type=\"video/mp4\">\n",
    "</video>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch0.4]",
   "language": "python",
   "name": "conda-env-pytorch0.4-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
