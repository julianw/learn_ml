{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from IPython import display\n",
    "from IPython.display import HTML\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.distributions import Normal, Categorical\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "FloatTensor = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor if use_cuda else torch.LongTensor\n",
    "ByteTensor = torch.cuda.ByteTensor if use_cuda else torch.ByteTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_torch_model(model, filename):\n",
    "  if not os.path.exists(os.path.dirname(filename)):\n",
    "    os.makedirs(os.path.dirname(filename))\n",
    "  torch.save(model.state_dict(), filename)\n",
    "\n",
    "def load_torch_model(model, filename):\n",
    "  model.load_state_dict(torch.load(filename))\n",
    "\n",
    "def play(agent, env, img = None):\n",
    "  frames = []\n",
    "  state = env.reset()\n",
    "  agent.init_state(state)\n",
    "  ended = False\n",
    "  frame = env.render(mode='rgb_array')\n",
    "  img.set_data(frame)\n",
    "  frames.append(frame)\n",
    "  while not ended:\n",
    "    action = agent.get_action()\n",
    "    (state, reward, ended, info) = env.step(action)\n",
    "    agent.add_state(state)    \n",
    "    frame = env.render(mode='rgb_array')\n",
    "    img.set_data(frame)\n",
    "    display.display(plt.gcf())\n",
    "    display.clear_output(wait=True)\n",
    "    frames.append(frame)\n",
    "  return frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNet_discret(nn.Module):\n",
    "  def __init__(self, input_size, output_size):\n",
    "    super(PolicyNet_discret,self).__init__()\n",
    "    self.l1_linear = nn.Linear(input_size, 512)\n",
    "    self.l2_linear = nn.Linear(512, 256)\n",
    "    self.l3_linear = nn.Linear(256, output_size)\n",
    "    nn.init.kaiming_normal_(self.l1_linear.weight)\n",
    "    nn.init.kaiming_normal_(self.l2_linear.weight)\n",
    "    \n",
    "  def forward(self,x):\n",
    "    out = F.relu(self.l1_linear(x))\n",
    "    out = F.relu(self.l2_linear(out))\n",
    "    out = F.softmax(self.l3_linear(out),dim=0)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class REINFORCE():\n",
    "  def __init__(self, env, steps_in_state = 2):\n",
    "    self.steps_in_state = steps_in_state\n",
    "    self.policy = PolicyNet_discret(env.observation_space.shape[0] * steps_in_state,env.action_space.n)\n",
    "    self.env = env\n",
    "    self._gamma = 0.96\n",
    "\n",
    "  def init_state(self, env_state):\n",
    "    self.running_state_seq = [env_state] * self.steps_in_state\n",
    "    self.running_state = FloatTensor(self.running_state_seq).view(-1)\n",
    "\n",
    "  def add_state(self, env_state):\n",
    "    self.running_state_seq = self.running_state_seq[1:]\n",
    "    self.running_state_seq.append(env_state)\n",
    "    self.running_state = FloatTensor(self.running_state_seq).view(-1)\n",
    "\n",
    "  def get_action(self):\n",
    "    action = self.policy(self.running_state)\n",
    "    return action.argmax().item()\n",
    "\n",
    "  def pick_action(self, state):\n",
    "    probs = self.policy(state)\n",
    "    action_dist = Categorical(probs)\n",
    "    action = action_dist.sample()\n",
    "    action = action.item()\n",
    "    return (action, action_dist.log_prob(FloatTensor([action])))\n",
    "  \n",
    "  def update_policy(self, episode):\n",
    "    (states, actions, rewards, log_probs) = zip(*episode)\n",
    "    MC_rewards = []\n",
    "    R = 0\n",
    "    for r in rewards[::-1]:\n",
    "      R = r + self._gamma * R\n",
    "      MC_rewards.insert(0, R)\n",
    "    loss = []\n",
    "    for (log_prob, reward) in zip(log_probs, MC_rewards):\n",
    "      loss.append(-log_prob*reward)\n",
    "    loss = torch.stack(loss).sum()\n",
    "    self.optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    self.optimizer.step()\n",
    "\n",
    "  def train(self, env, episode, lr=1e-3, checkpoint=100):\n",
    "    self.optimizer = torch.optim.Adam(self.policy.parameters(), lr=lr, weight_decay=1e-3)\n",
    "    best_score = 0\n",
    "    for i in range(episode):\n",
    "      s0 = env.reset()\n",
    "      self.init_state(s0)\n",
    "      state = self.running_state\n",
    "      episode = []\n",
    "      episode_ended = False\n",
    "      score = 0\n",
    "      while not episode_ended:\n",
    "        (action, log_prob) =  self.pick_action(state)\n",
    "        (s1, reward, episode_ended, info) = env.step(action)\n",
    "        episode.append((state, action, reward, log_prob))\n",
    "        self.add_state(s1)\n",
    "        next_state = self.running_state\n",
    "        if episode_ended:\n",
    "          ended = 1\n",
    "        else:\n",
    "          ended = 0\n",
    "        s0 = s1\n",
    "        state = next_state\n",
    "        score += reward\n",
    "        \n",
    "      if score > best_score:\n",
    "        save_torch_model(self.policy, 'model/reinforce_cartpole_best.pth')\n",
    "        best_score = score\n",
    "        print('new best score:',best_score)\n",
    "        \n",
    "      if (i + 1) % checkpoint == 0:\n",
    "        save_torch_model(self.policy,'model/reinforce_cartpole_iter_%d.pth' %(i+1))\n",
    "        print(i+1,': score:', score)\n",
    "\n",
    "      self.update_policy(episode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "agent = REINFORCE(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new best score: 15.0\n",
      "new best score: 17.0\n",
      "new best score: 22.0\n",
      "new best score: 29.0\n",
      "new best score: 42.0\n",
      "50 : score: 13.0\n",
      "new best score: 54.0\n",
      "new best score: 169.0\n",
      "new best score: 200.0\n",
      "100 : score: 164.0\n",
      "150 : score: 133.0\n",
      "200 : score: 200.0\n"
     ]
    }
   ],
   "source": [
    "agent.train(env, 200, lr=1e-3, checkpoint=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x123338908>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "img = plt.imshow(env.render(mode='rgb_array'))\n",
    "load_torch_model(agent.policy,'model/reinforce_cartpole_iter_200.pth')\n",
    "frames = play(agent, env, img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class REINFORCE_MountainCar():\n",
    "  def __init__(self, env, steps_in_state = 2):\n",
    "    self.steps_in_state = steps_in_state\n",
    "    self.policy = PolicyNet_discret(env.observation_space.shape[0] * steps_in_state,env.action_space.n)\n",
    "    self.env = env\n",
    "    self._gamma = 0.96\n",
    "    self._epsilon = 0.2\n",
    "\n",
    "  def init_state(self, env_state):\n",
    "    self.running_state_seq = [env_state] * self.steps_in_state\n",
    "    self.running_state = FloatTensor(self.running_state_seq).view(-1)\n",
    "\n",
    "  def add_state(self, env_state):\n",
    "    self.running_state_seq = self.running_state_seq[1:]\n",
    "    self.running_state_seq.append(env_state)\n",
    "    self.running_state = FloatTensor(self.running_state_seq).view(-1)\n",
    "\n",
    "  def get_action(self):\n",
    "    # get_action does not sample or explore, just take the policy function output and use it\n",
    "    action = self.policy(self.running_state)\n",
    "    return action.argmax().item()\n",
    "\n",
    "  def epsilon(self):\n",
    "    (1. / (1. + self.decay * self.iterations))\n",
    "    return self._epsilon\n",
    "    \n",
    "  def pick_action(self, state):\n",
    "    probs = self.policy(state)\n",
    "    action_dist = Categorical(probs)\n",
    "    if np.random.rand() < self.epsilon():\n",
    "      action = self.env.action_space.sample()\n",
    "    else:\n",
    "      action = action_dist.sample()\n",
    "      action = action.item()\n",
    "    return (action, action_dist.log_prob(FloatTensor([action])))\n",
    "  \n",
    "  def update_policy(self, episode):\n",
    "    (states, actions, rewards, log_probs) = zip(*episode)\n",
    "    MC_rewards = []\n",
    "    R = 0\n",
    "    for r in rewards[::-1]:\n",
    "      R = r + self._gamma * R\n",
    "      MC_rewards.insert(0, R)\n",
    "    loss = []\n",
    "    for (log_prob, reward) in zip(log_probs, MC_rewards):\n",
    "      loss.append(-log_prob*reward)\n",
    "    loss = torch.stack(loss).sum()\n",
    "    self.optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    self.optimizer.step()\n",
    "\n",
    "  def train(self, env, episode, lr=1e-3, checkpoint=100):\n",
    "    self.optimizer = torch.optim.Adam(self.policy.parameters(), lr=lr, weight_decay=1e-3)\n",
    "    best_score = 0\n",
    "    for i in range(episode):\n",
    "      s0 = env.reset()\n",
    "      self.init_state(s0)\n",
    "      state = self.running_state\n",
    "      episode = []\n",
    "      episode_ended = False\n",
    "      score = 0\n",
    "      while not episode_ended:\n",
    "        (action, log_prob) =  self.pick_action(state)\n",
    "        (s1, reward, episode_ended, info) = env.step(action)\n",
    "        reward = abs(s1[0] - (-0.5))\n",
    "        episode.append((state, action, reward, log_prob))\n",
    "        self.add_state(s1)\n",
    "        next_state = self.running_state\n",
    "        if episode_ended:\n",
    "          ended = 1\n",
    "        else:\n",
    "          ended = 0\n",
    "        s0 = s1\n",
    "        state = next_state\n",
    "        score += reward\n",
    "        \n",
    "      if score > best_score:\n",
    "        save_torch_model(self.policy, 'model/reinforce_mountaincar_best.pth')\n",
    "        best_score = score\n",
    "        print('new best score:',best_score)\n",
    "        \n",
    "      if (i + 1) % checkpoint == 0:\n",
    "        save_torch_model(self.policy,'model/reinforce_mountaincar_iter_%d.pth' %(i+1))\n",
    "        print(i+1,': score:', score)\n",
    "\n",
    "      self.update_policy(episode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "env_MC = gym.make('MountainCar-v0')\n",
    "agent_MC = REINFORCE_MountainCar(env_MC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new best score: 13.973131466355717\n",
      "new best score: 30.332211930404373\n",
      "50 : score: 11.936303359432838\n",
      "100 : score: 16.329909875734476\n",
      "150 : score: 10.84273817350573\n",
      "new best score: 33.43971294670785\n",
      "new best score: 33.87204493711082\n",
      "200 : score: 17.923291866815568\n"
     ]
    }
   ],
   "source": [
    "agent_MC.train(env_MC, 200, lr=1e-3, checkpoint=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNet_continuous(nn.Module):\n",
    "  def __init__(self, input_size, output_size):\n",
    "    super(PolicyNet_continuous,self).__init__()\n",
    "    self.l1_linear = nn.Linear(input_size,512)\n",
    "    self.l2_linear = nn.Linear(512,256)\n",
    "    self.l3_linear = nn.Linear(256,output_size)\n",
    "    nn.init.kaiming_normal_(self.l1_linear.weight)\n",
    "    nn.init.kaiming_normal_(self.l2_linear.weight)\n",
    "    \n",
    "  def forward(self,x):\n",
    "    out = F.relu(self.l1_linear(x))\n",
    "    out = F.relu(self.l2_linear(out))\n",
    "    out = F.tanh(self.l3_linear(out))\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "class REINFORCE_continuous():\n",
    "  def __init__(self, env, steps_in_state = 2):\n",
    "    self.steps_in_state = steps_in_state\n",
    "    self.policy = PolicyNet_continuous(env.observation_space.shape[0] * steps_in_state,1)\n",
    "    self.env = env\n",
    "    self.range_scale = (env.action_space.high[0] - env.action_space.low[0]) / 2.0\n",
    "    self._gamma = 0.96\n",
    "    self._epsilon = 0.0\n",
    "  \n",
    "  def epsilon(self):\n",
    "    return self._epsilon\n",
    "  \n",
    "  def predict(self, state):\n",
    "    return self.policy(state)\n",
    "  \n",
    "  def pick_action(self, state):\n",
    "    probs = self.predict(state) * self.range_scale\n",
    "    action_dist = Normal(probs, 0.2)\n",
    "    if np.random.rand() < self.epsilon():\n",
    "      action = self.env.action_space.sample()[0]\n",
    "    else:\n",
    "      action = action_dist.rsample().item()\n",
    "    return (action, action_dist.log_prob(FloatTensor([action])))\n",
    "  \n",
    "  def update_policy(self, episode):\n",
    "    (states, actions, rewards, log_probs) = zip(*episode)\n",
    "    MC_rewards = []\n",
    "    R = 0\n",
    "    for r in rewards[::-1]:\n",
    "      R = r + self._gamma * R\n",
    "      MC_rewards.insert(0, R)\n",
    "    loss = []\n",
    "    for (log_prob, reward) in zip(log_probs, MC_rewards):\n",
    "      loss.append(-log_prob*reward)\n",
    "    loss = torch.stack(loss).sum()\n",
    "    self.optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    self.optimizer.step()\n",
    "\n",
    "  def train(self, env, episode, lr=1e-3, checkpoint=100):\n",
    "    self.optimizer = torch.optim.Adam(self.policy.parameters(), lr=lr, weight_decay=1e-3)\n",
    "    best_score = -99999\n",
    "    for i in range(episode):\n",
    "      s0 = env.reset()\n",
    "      seq = [s0] * self.steps_in_state\n",
    "      state = FloatTensor(seq).view(-1)\n",
    "      episode = []\n",
    "      episode_ended = False\n",
    "      step = 0\n",
    "      score = 0\n",
    "      while not episode_ended:\n",
    "        (action, log_prob) =  self.pick_action(state)\n",
    "        (s1, reward, episode_ended, info) = env.step([action])\n",
    "        episode.append((state, action, reward, log_prob))\n",
    "        seq = seq[1:]\n",
    "        seq.append(s1)\n",
    "        next_state = FloatTensor(seq).view(-1)\n",
    "        step += 1\n",
    "        if episode_ended:\n",
    "          ended = 1\n",
    "          if step < 200:\n",
    "            # pole tipped over\n",
    "            reward = 0.0\n",
    "          else:\n",
    "            # environment terminate as max step reached\n",
    "            reward = 1.0\n",
    "        else:\n",
    "          ended = 0\n",
    "        s0 = s1\n",
    "        state = next_state\n",
    "        score += reward\n",
    "        \n",
    "      if score > best_score:\n",
    "        save_torch_model(self.policy, 'model/reinforce_mountaincar_best.pth')\n",
    "        best_score = score\n",
    "        print('new best score:',best_score)\n",
    "\n",
    "      if (i + 1) % checkpoint == 0:\n",
    "        save_torch_model(self.policy,'model/reinforce_mountaincar_iter_%d.pth' %(i+1))\n",
    "        print(i+1,': score:', score)\n",
    "\n",
    "      self.update_policy(episode)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('Pendulum-v0')\n",
    "# env = gym.make('MountainCarContinuous-v0')\n",
    "agent = REINFORCE_continuous(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new best score: -1453.4346701422298\n",
      "new best score: -1344.229292963753\n",
      "new best score: -1021.2208387193907\n",
      "new best score: -833.7026407974064\n",
      "new best score: -829.011154336805\n",
      "100 : score: -1188.859591652436\n",
      "new best score: -823.0727270718667\n",
      "200 : score: -1635.3359380956833\n",
      "300 : score: -1578.5392590559215\n",
      "new best score: -752.1505662111351\n",
      "400 : score: -1063.1799564137377\n",
      "500 : score: -1640.118534303802\n"
     ]
    }
   ],
   "source": [
    "agent.train(env, 500, lr=1e-3, checkpoint=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAD8CAYAAAB9y7/cAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAAEhpJREFUeJzt3XGM33d93/Hna3ZIGLA6IZfIs505tN5KOg0nugWjTFOa0DZJqzmVypRoKhGK5E4KEqiobdJJa5EWqZVWsiFt0dwmxUyMkAZorCgr9UxQxR8kHGCMHZPmAAtf7cXHSAIMLZvDe3/8Pge/OT/f/Xx3P9v32fMh/fT7fj/fz/d770/8y+u+97nvR5eqQpLUn791vguQJE2GAS9JnTLgJalTBrwkdcqAl6ROGfCS1KmJBXySW5I8l2Q2yb2T+jqSpNEyiefgk6wD/hr4BWAO+CJwZ1U9u+pfTJI00qTu4K8HZqvqm1X1v4FHgJ0T+lqSpBHWT+i6m4BjQ/tzwNvP1Pnyyy+vrVu3TqgUSVp7jh49yne+852s5BqTCvhRRf0/c0FJdgG7AK666ipmZmYmVIokrT3T09MrvsakpmjmgC1D+5uB48Mdqmp3VU1X1fTU1NSEypCk/39NKuC/CGxLcnWS1wF3AHsn9LUkSSNMZIqmqk4leS/wGWAd8HBVHZ7E15IkjTapOXiq6kngyUldX5K0OFeySlKnDHhJ6pQBL0mdMuAlqVMGvCR1yoCXpE4Z8JLUKQNekjplwEtSpwx4SeqUAS9JnTLgJalTBrwkdcqAl6ROGfCS1CkDXpI6ZcBLUqcMeEnq1Ir+ZF+So8D3gVeBU1U1neQy4BPAVuAo8M+r6sWVlSlJOlurcQf/81W1vaqm2/69wP6q2gbsb/uSpHNsElM0O4E9bXsPcPsEvoYkaQkrDfgC/jLJl5Lsam1XVtUJgPZ+xQq/hiRpGVY0Bw/cUFXHk1wB7Evy9XFPbN8QdgFcddVVKyxDknS6Fd3BV9Xx9n4S+DRwPfBCko0A7f3kGc7dXVXTVTU9NTW1kjIkSSMsO+CTvCHJmxa2gV8EDgF7gbtat7uAx1dapCTp7K1kiuZK4NNJFq7zX6rqL5J8EXg0yd3At4F3rbxMSdLZWnbAV9U3gbeNaP8fwM0rKUqStHKuZJWkThnwktQpA16SOmXAS1KnDHhJ6pQBL0mdMuAlqVMGvCR1yoCXpE4Z8JLUKQNekjplwEtSpwx4SeqUAS9JnTLgJalTBrwkdcqAl6ROGfCS1CkDXpI6tWTAJ3k4yckkh4baLkuyL8nz7f3S1p4kH04ym+RgkusmWbwk6czGuYP/CHDLaW33Avurahuwv+0D3Apsa69dwIOrU6Yk6WwtGfBV9VfAd09r3gnsadt7gNuH2j9aA18ANiTZuFrFSpLGt9w5+Cur6gRAe7+itW8Cjg31m2ttr5FkV5KZJDPz8/PLLEOSdCar/UvWjGirUR2randVTVfV9NTU1CqXIUlabsC/sDD10t5PtvY5YMtQv83A8eWXJ0laruUG/F7grrZ9F/D4UPu729M0O4CXF6ZyJEnn1vqlOiT5OHAjcHmSOeD3gD8AHk1yN/Bt4F2t+5PAbcAs8EPgPROoWZI0hiUDvqruPMOhm0f0LeCelRYlSVo5V7JKUqcMeEnqlAEvSZ0y4CWpUwa8JHXKgJekThnwktQpA16SOmXAS1KnDHhJ6pQBL0mdMuAlqVMGvCR1yoCXpE4Z8JLUKQNekjplwEtSpwx4SerUkgGf5OEkJ5McGmr7/SR/k+RAe902dOy+JLNJnkvyS5MqXJK0uHHu4D8C3DKi/YGq2t5eTwIkuQa4A/i5ds5/TLJutYqVJI1vyYCvqr8Cvjvm9XYCj1TVK1X1LWAWuH4F9UmSlmklc/DvTXKwTeFc2to2AceG+sy1ttdIsivJTJKZ+fn5FZQhSRpluQH/IPDTwHbgBPBHrT0j+taoC1TV7qqarqrpqampZZYhSTqTZQV8Vb1QVa9W1Y+AP+Yn0zBzwJahrpuB4ysrUZK0HMsK+CQbh3Z/FVh4wmYvcEeSi5NcDWwDnllZiZKk5Vi/VIckHwduBC5PMgf8HnBjku0Mpl+OAr8BUFWHkzwKPAucAu6pqlcnU7okaTFLBnxV3Tmi+aFF+t8P3L+SoiRJK+dKVknqlAEvSZ0y4CWpUwa8JHXKgJekThnwktQpA16SOmXAS1KnDHhJ6pQBL0mdMuAlqVMGvCR1yoCXpE4Z8JLUKQNekjplwEtSpwx4SeqUAS9JnVoy4JNsSfJUkiNJDid5X2u/LMm+JM+390tbe5J8OMlskoNJrpv0ICRJrzXOHfwp4ANV9VZgB3BPkmuAe4H9VbUN2N/2AW4FtrXXLuDBVa9akrSkJQO+qk5U1Zfb9veBI8AmYCewp3XbA9zetncCH62BLwAbkmxc9colSYs6qzn4JFuBa4GngSur6gQMvgkAV7Rum4BjQ6fNtbbTr7UryUySmfn5+bOvXJK0qLEDPskbgU8C76+q7y3WdURbvaahandVTVfV9NTU1LhlSJLGNFbAJ7mIQbh/rKo+1ZpfWJh6ae8nW/scsGXo9M3A8dUpV5I0rnGeognwEHCkqj40dGgvcFfbvgt4fKj93e1pmh3AywtTOZKkc2f9GH1uAH4d+FqSA63td4E/AB5NcjfwbeBd7diTwG3ALPBD4D2rWrEkaSxLBnxVfZ7R8+oAN4/oX8A9K6xLkrRCrmSVpE4Z8JLUKQNekjplwEtSpwx4SeqUAS9JnTLgJalTBrwkdcqAl6ROGfCS1CkDXpI6ZcBLUqcMeEnqlAEvSZ0y4CWpUwa8JHXKgJekThnwktSpcf7o9pYkTyU5kuRwkve19t9P8jdJDrTXbUPn3JdkNslzSX5pkgOQJI02zh/dPgV8oKq+nORNwJeS7GvHHqiqfzvcOck1wB3AzwF/F/hvSf5+Vb26moVLkha35B18VZ2oqi+37e8DR4BNi5yyE3ikql6pqm8Bs8D1q1GsJGl8ZzUHn2QrcC3wdGt6b5KDSR5Ocmlr2wQcGzptjsW/IUiSJmDsgE/yRuCTwPur6nvAg8BPA9uBE8AfLXQdcXqNuN6uJDNJZubn58+6cEnS4sYK+CQXMQj3j1XVpwCq6oWqerWqfgT8MT+ZhpkDtgydvhk4fvo1q2p3VU1X1fTU1NRKxiBJGmGcp2gCPAQcqaoPDbVvHOr2q8Chtr0XuCPJxUmuBrYBz6xeyZKkcYzzFM0NwK8DX0tyoLX9LnBnku0Mpl+OAr8BUFWHkzwKPMvgCZx7fIJGks69JQO+qj7P6Hn1Jxc5537g/hXUJUlaIVeySlKnDHhJ6pQBL0mdMuAlqVMGvCR1yoCXpE4Z8JLUKQNekjplwEtSpwx4SeqUAS9JnTLgJalTBrwkdcqAl6ROGfCS1CkDXpI6ZcBLUqcMeEnq1Dh/dPuSJM8k+WqSw0k+2NqvTvJ0kueTfCLJ61r7xW1/th3fOtkhSJJGGecO/hXgpqp6G7AduCXJDuAPgQeqahvwInB363838GJV/QzwQOsnSTrHlgz4GvhB272ovQq4CXiste8Bbm/bO9s+7fjNSUb90W5J0gSNNQefZF2SA8BJYB/wDeClqjrVuswBm9r2JuAYQDv+MvDm1SxakrS0sQK+ql6tqu3AZuB64K2jurX3UXfrdXpDkl1JZpLMzM/Pj1uvJGlMZ/UUTVW9BHwO2AFsSLK+HdoMHG/bc8AWgHb8p4DvjrjW7qqarqrpqamp5VUvSTqjcZ6imUqyoW2/HngncAR4Cvi11u0u4PG2vbft045/tqpecwcvSZqs9Ut3YSOwJ8k6Bt8QHq2qJ5I8CzyS5N8AXwEeav0fAv5zklkGd+53TKBuSdISlgz4qjoIXDui/ZsM5uNPb/9fwLtWpTpJ0rK5klWSOmXAS1KnDHhJ6pQBL0mdMuAlqVMGvCR1yoCXpE4Z8JLUKQNekjplwEtSpwx4SeqUAS9JnTLgJalTBrwkdcqAl6ROGfCS1CkDXpI6ZcBLUqfG+aPblyR5JslXkxxO8sHW/pEk30pyoL22t/Yk+XCS2SQHk1w36UFIkl5rnD+6/QpwU1X9IMlFwOeT/Nd27Leq6rHT+t8KbGuvtwMPtndJ0jm05B18Dfyg7V7UXrXIKTuBj7bzvgBsSLJx5aVKks7GWHPwSdYlOQCcBPZV1dPt0P1tGuaBJBe3tk3AsaHT51qbJOkcGivgq+rVqtoObAauT/IPgfuAnwX+MXAZ8Dute0Zd4vSGJLuSzCSZmZ+fX1bxkqQzO6unaKrqJeBzwC1VdaJNw7wC/Clwfes2B2wZOm0zcHzEtXZX1XRVTU9NTS2reEnSmY3zFM1Ukg1t+/XAO4GvL8yrJwlwO3ConbIXeHd7mmYH8HJVnZhI9ZKkMxrnKZqNwJ4k6xh8Q3i0qp5I8tkkUwymZA4A/7L1fxK4DZgFfgi8Z/XLliQtZcmAr6qDwLUj2m86Q/8C7ll5aZKklXAlqyR1yoCXpE4Z8JLUKQNekjplwEtSpwx4SeqUAS9JnTLgJalTBrwkdcqAl6ROGfCS1CkDXpI6ZcBLUqcMeEnqlAEvSZ0y4CWpUwa8JHXKgJekThnwktSpsQM+ybokX0nyRNu/OsnTSZ5P8okkr2vtF7f92XZ862RKlyQt5mzu4N8HHBna/0PggaraBrwI3N3a7wZerKqfAR5o/SRJ59hYAZ9kM/DLwJ+0/QA3AY+1LnuA29v2zrZPO35z6y9JOofWj9nv3wG/Dbyp7b8ZeKmqTrX9OWBT294EHAOoqlNJXm79vzN8wSS7gF1t95Ukh5Y1ggvf5Zw29k70Oi7od2yOa235e0l2VdXu5V5gyYBP8ivAyar6UpIbF5pHdK0xjv2kYVD07vY1ZqpqeqyK15hex9bruKDfsTmutSfJDC0nl2OcO/gbgH+W5DbgEuDvMLij35BkfbuL3wwcb/3ngC3AXJL1wE8B311ugZKk5VlyDr6q7quqzVW1FbgD+GxV/QvgKeDXWre7gMfb9t62Tzv+2ap6zR28JGmyVvIc/O8Av5lklsEc+0Ot/SHgza39N4F7x7jWsn8EWQN6HVuv44J+x+a41p4VjS3eXEtSn1zJKkmdOu8Bn+SWJM+1la/jTOdcUJI8nOTk8GOeSS5Lsq+t8t2X5NLWniQfbmM9mOS681f54pJsSfJUkiNJDid5X2tf02NLckmSZ5J8tY3rg629i5XZva44T3I0ydeSHGhPlqz5zyJAkg1JHkvy9fb/2jtWc1znNeCTrAP+A3ArcA1wZ5JrzmdNy/AR4JbT2u4F9rdVvvv5ye8hbgW2tdcu4MFzVONynAI+UFVvBXYA97R/m7U+tleAm6rqbcB24JYkO+hnZXbPK85/vqq2Dz0SudY/iwD/HviLqvpZ4G0M/u1Wb1xVdd5ewDuAzwzt3wfcdz5rWuY4tgKHhvafAza27Y3Ac237PwF3jup3ob8YPCX1Cz2NDfjbwJeBtzNYKLO+tf/4cwl8BnhH217f+uV8136G8WxugXAT8ASDNSlrflytxqPA5ae1renPIoNHzr91+n/31RzX+Z6i+fGq12Z4RexadmVVnQBo71e09jU53vbj+7XA03QwtjaNcQA4CewDvsGYK7OBhZXZF6KFFec/avtjrzjnwh4XDBZL/mWSL7VV8LD2P4tvAeaBP23Tan+S5A2s4rjOd8CPteq1I2tuvEneCHwSeH9VfW+xriPaLsixVdWrVbWdwR3v9cBbR3Vr72tiXBlacT7cPKLrmhrXkBuq6joG0xT3JPmni/RdK2NbD1wHPFhV1wL/k8UfKz/rcZ3vgF9Y9bpgeEXsWvZCko0A7f1ka19T401yEYNw/1hVfao1dzE2gKp6Cfgcg98xbGgrr2H0ymwu8JXZCyvOjwKPMJim+fGK89ZnLY4LgKo63t5PAp9m8I15rX8W54C5qnq67T/GIPBXbVznO+C/CGxrv+l/HYOVsnvPc02rYXg17+mrfN/dfhu+A3h54UexC02SMFi0dqSqPjR0aE2PLclUkg1t+/XAOxn8YmtNr8yujlecJ3lDkjctbAO/CBxijX8Wq+q/A8eS/IPWdDPwLKs5rgvgFw23AX/NYB70X53vepZR/8eBE8D/YfAd9m4Gc5n7gefb+2Wtbxg8NfQN4GvA9Pmuf5Fx/RMGP/4dBA60121rfWzAPwK+0sZ1CPjXrf0twDPALPBnwMWt/ZK2P9uOv+V8j2GMMd4IPNHLuNoYvtpehxdyYq1/Flut24GZ9nn8c+DS1RyXK1klqVPne4pGkjQhBrwkdcqAl6ROGfCS1CkDXpI6ZcBLUqcMeEnqlAEvSZ36v+EtaRE4R3zaAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11767efd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "env.reset()\n",
    "img = plt.imshow(env.render(mode='rgb_array')) # only call this once\n",
    "for _ in range(100):\n",
    "    img.set_data(env.render(mode='rgb_array')) # just update the data\n",
    "    display.display(plt.gcf())\n",
    "    display.clear_output(wait=True)\n",
    "    action = env.action_space.sample()\n",
    "    env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch0.4]",
   "language": "python",
   "name": "conda-env-pytorch0.4-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
