{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from IPython import display\n",
    "from IPython.display import HTML\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.distributions import Normal, Categorical\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "FloatTensor = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor if use_cuda else torch.LongTensor\n",
    "ByteTensor = torch.cuda.ByteTensor if use_cuda else torch.ByteTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_torch_model(model, filename):\n",
    "  if not os.path.exists(os.path.dirname(filename)):\n",
    "    os.makedirs(os.path.dirname(filename))\n",
    "  torch.save(model.state_dict(), filename)\n",
    "\n",
    "def load_torch_model(model, filename):\n",
    "  model.load_state_dict(torch.load(filename))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_grad(source, target):\n",
    "  grads = []\n",
    "  for param in source.parameters():\n",
    "    grads.append(param.grad.clone())\n",
    "  grads.reverse()\n",
    "  for param in target.parameters():\n",
    "    param.grad = grads.pop()\n",
    "\n",
    "def zero_grad(model):\n",
    "  for param in model.parameters():\n",
    "    if type(param.grad) != type(None):\n",
    "      param.grad.data.zero_()\n",
    "      \n",
    "def update_target(target_net, eval_net, tau):\n",
    "  fast = eval_net.state_dict()\n",
    "  slow = target_net.state_dict()\n",
    "  for t in slow:\n",
    "    slow[t] = slow[t] * (1. - tau) + fast[t] * tau\n",
    "\n",
    "  target_net.load_state_dict(slow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNet_discret(nn.Module):\n",
    "  def __init__(self, input_size, output_size):\n",
    "    super(PolicyNet_discret,self).__init__()\n",
    "    self.l1_linear = nn.Linear(input_size, 512)\n",
    "    self.l2_linear = nn.Linear(512, 256)\n",
    "    self.l3_linear = nn.Linear(256, output_size)\n",
    "    nn.init.kaiming_normal_(self.l1_linear.weight)\n",
    "    nn.init.kaiming_normal_(self.l2_linear.weight)\n",
    "    self.l3_linear.weight.data.zero_()\n",
    "    \n",
    "  def forward(self,x):\n",
    "    out = F.relu(self.l1_linear(x))\n",
    "    out = F.relu(self.l2_linear(out))\n",
    "    out = F.softmax(self.l3_linear(out),dim=0)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNet_continuous(nn.Module):\n",
    "  def __init__(self, input_size, output_size):\n",
    "    super(PolicyNet_continuous,self).__init__()\n",
    "    self.l1_linear = nn.Linear(input_size,2048)\n",
    "    self.l2_linear = nn.Linear(2048,512)\n",
    "    self.l3_linear = nn.Linear(512,output_size)\n",
    "    nn.init.kaiming_normal_(self.l1_linear.weight)\n",
    "    nn.init.kaiming_normal_(self.l2_linear.weight)\n",
    "    self.l3_linear.weight.data.zero_()\n",
    "    \n",
    "  def forward(self,x):\n",
    "    out = F.relu(self.l1_linear(x))\n",
    "    out = F.relu(self.l2_linear(out))\n",
    "    out = F.tanh(self.l3_linear(out))\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueNet(nn.Module):\n",
    "  def __init__(self, input_size):\n",
    "    super(ValueNet,self).__init__()\n",
    "    self.l1_linear = nn.Linear(input_size, 512)\n",
    "    self.l2_linear = nn.Linear(512,256)\n",
    "    self.l3_linear = nn.Linear(256, 1)\n",
    "    nn.init.kaiming_normal_(self.l1_linear.weight)\n",
    "    nn.init.kaiming_normal_(self.l2_linear.weight)\n",
    "    self.l3_linear.weight.data.zero_()\n",
    "\n",
    "  def forward(self, x):\n",
    "    out = F.relu(self.l1_linear(x))\n",
    "    out = F.relu(self.l2_linear(out))\n",
    "    out = self.l3_linear(out)\n",
    "    return out    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic():\n",
    "  def __init__(self, env, steps_in_state = 2):\n",
    "    self.steps_in_state = steps_in_state\n",
    "    self.policy = PolicyNet_discret(env.observation_space.shape[0] * steps_in_state,env.action_space.n)\n",
    "    self.value = ValueNet(env.observation_space.shape[0] * steps_in_state)\n",
    "    if use_cuda:\n",
    "      self.policy.cuda()\n",
    "      self.value.cuda()\n",
    "    self.env = env\n",
    "    self._gamma = 0.96\n",
    "    \n",
    "  def pick_action(self, state):\n",
    "    probs = self.policy(state)\n",
    "    action_dist = Categorical(probs)\n",
    "    action = action_dist.sample()\n",
    "    action = action.item()\n",
    "    return (action, action_dist.log_prob(FloatTensor([action])))\n",
    "  \n",
    "  def update_actor_critic(self, episode):\n",
    "    (states, actions, rewards, next_states, log_probs, ended) = zip(*episode)\n",
    "    \n",
    "    rewards = FloatTensor(rewards)\n",
    "    ended = FloatTensor(ended)\n",
    "    state_value = self.value(torch.stack(states))\n",
    "    next_state_value = self.value(torch.stack(next_states))\n",
    "    target_value = rewards + (1 - ended) * self._gamma * next_state_value\n",
    "    \n",
    "    delta = target_value - state_value\n",
    "\n",
    "    value_loss = F.mse_loss(state_value, target_value)\n",
    "    \n",
    "    policy_loss = []\n",
    "    for log_prob, d in zip(log_probs, delta):\n",
    "      policy_loss.append(-log_prob * d)\n",
    "    policy_loss = torch.stack(policy_loss).sum()\n",
    "    \n",
    "    self.value_optimizer.zero_grad()\n",
    "    value_loss.backward(retain_graph=True)\n",
    "    self.value_optimizer.step()\n",
    "    \n",
    "    self.policy_optimizer.zero_grad()\n",
    "    policy_loss.backward()\n",
    "    self.policy_optimizer.step()\n",
    "\n",
    "  def train(self, env, episode, lr=1e-3, target_copylr=1e-3, lr_policy=None, lr_value=None, checkpoint=100):\n",
    "    lr_policy = lr if lr_policy == None else lr_policy\n",
    "    lr_value = lr if lr_value == None else lr_value\n",
    "    self.policy_optimizer = torch.optim.Adam(self.policy.parameters(), lr=lr_policy, weight_decay=1e-3)\n",
    "    self.value_optimizer = torch.optim.Adam(self.value.parameters(), lr=lr_value, weight_decay=1e-3)\n",
    "    best_score = -99999\n",
    "    running_score = None\n",
    "    for i in range(episode):\n",
    "      s0 = env.reset()\n",
    "      seq = [s0] * self.steps_in_state\n",
    "      state = FloatTensor(seq).view(-1)\n",
    "      episode = []\n",
    "      episode_ended = False\n",
    "      score = 0\n",
    "      while not episode_ended:\n",
    "        (action, log_prob) =  self.pick_action(state)\n",
    "        (s1, reward, episode_ended, info) = env.step(action)\n",
    "        seq = seq[1:]\n",
    "        seq.append(s1)\n",
    "        next_state = FloatTensor(seq).view(-1)\n",
    "        if episode_ended:\n",
    "          ended = 1\n",
    "        else:\n",
    "          ended = 0\n",
    "        episode.append((state, action, reward, next_state, log_prob, ended))\n",
    "\n",
    "        s0 = s1\n",
    "        state = next_state\n",
    "        score += reward\n",
    "        \n",
    "      if running_score == None:\n",
    "        running_score = score\n",
    "      else:\n",
    "        running_score = running_score * 0.9 + score * 0.1\n",
    "        \n",
    "      if (i + 1) % checkpoint == 0:\n",
    "        is_best = False\n",
    "        if running_score > best_score:\n",
    "          is_best = True\n",
    "          save_torch_model(self.policy, 'model/actor_critic_cartpole_policy_best.pth')\n",
    "          best_score = running_score\n",
    "        save_torch_model(self.policy,'model/actor_critic_cartpole_policy_iter_%d.pth' %(i+1))\n",
    "        print('%d: running_score:%.2f, is_best:%s' %(i+1, running_score, is_best))\n",
    "\n",
    "      self.update_actor_critic(episode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "agent = ActorCritic(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100: running_score:-200.00, is_best:True\n",
      "200: running_score:-200.00, is_best:False\n",
      "300: running_score:-200.00, is_best:False\n",
      "400: running_score:-200.00, is_best:False\n",
      "500: running_score:-200.00, is_best:False\n",
      "600: running_score:-200.00, is_best:False\n",
      "700: running_score:-200.00, is_best:False\n",
      "800: running_score:-200.00, is_best:False\n",
      "900: running_score:-200.00, is_best:False\n",
      "1000: running_score:-200.00, is_best:False\n"
     ]
    }
   ],
   "source": [
    "agent.train(env, 1000, lr=1e-4, checkpoint=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic_MountainCar():\n",
    "  def __init__(self, env, steps_in_state = 2):\n",
    "    self.steps_in_state = steps_in_state\n",
    "    self.policy = PolicyNet_discret(env.observation_space.shape[0] * steps_in_state,env.action_space.n)\n",
    "    self.value = ValueNet(env.observation_space.shape[0] * steps_in_state)\n",
    "    if use_cuda:\n",
    "      self.policy.cuda()\n",
    "      self.value.cuda()\n",
    "    self.env = env\n",
    "    self._gamma = 0.96\n",
    "    \n",
    "  def pick_action(self, state):\n",
    "    probs = self.policy(state)\n",
    "    action_dist = Categorical(probs)\n",
    "    action = action_dist.sample()\n",
    "    action = action.item()\n",
    "    return (action, action_dist.log_prob(FloatTensor([action])))    \n",
    "  \n",
    "  def update_actor_critic(self, episode):\n",
    "    (states, actions, rewards, next_states, log_probs, ended) = zip(*episode)\n",
    "    \n",
    "    rewards = FloatTensor(rewards)\n",
    "    ended = FloatTensor(ended)\n",
    "    state_value = self.value(torch.stack(states))\n",
    "    next_state_value = self.value(torch.stack(next_states))\n",
    "    target_value = rewards + (1 - ended) * self._gamma * next_state_value\n",
    "    \n",
    "    delta = target_value - state_value\n",
    "\n",
    "    value_loss = F.mse_loss(state_value, target_value)\n",
    "    \n",
    "    policy_loss = []\n",
    "    for log_prob, d in zip(log_probs, delta):\n",
    "      policy_loss.append(-log_prob * d)\n",
    "    policy_loss = torch.stack(policy_loss).sum()\n",
    "    \n",
    "    self.value_optimizer.zero_grad()\n",
    "    value_loss.backward(retain_graph=True)\n",
    "    self.value_optimizer.step()\n",
    "    \n",
    "    self.policy_optimizer.zero_grad()\n",
    "    policy_loss.backward()\n",
    "    self.policy_optimizer.step()\n",
    "\n",
    "  def train(self, env, episode, lr=1e-3, target_copylr=1e-3, lr_policy=None, lr_value=None, checkpoint=100):\n",
    "    lr_policy = lr if lr_policy == None else lr_policy\n",
    "    lr_value = lr if lr_value == None else lr_value\n",
    "    self.policy_optimizer = torch.optim.Adam(self.policy.parameters(), lr=lr_policy, weight_decay=1e-3)\n",
    "    self.value_optimizer = torch.optim.Adam(self.value.parameters(), lr=lr_value, weight_decay=1e-3)\n",
    "    best_score = -99999\n",
    "    running_score = None\n",
    "    for i in range(episode):\n",
    "      s0 = env.reset()\n",
    "      seq = [s0] * self.steps_in_state\n",
    "      state = FloatTensor(seq).view(-1)\n",
    "      episode = []\n",
    "      episode_ended = False\n",
    "      score = 0\n",
    "      while not episode_ended:\n",
    "        (action, log_prob) =  self.pick_action(state)\n",
    "        (s1, reward, episode_ended, info) = env.step(action)\n",
    "        seq = seq[1:]\n",
    "        seq.append(s1)\n",
    "        next_state = FloatTensor(seq).view(-1)\n",
    "        if episode_ended:\n",
    "          ended = 1\n",
    "        else:\n",
    "          ended = 0\n",
    "        episode.append((state, action, reward, next_state, log_prob, ended))\n",
    "\n",
    "        s0 = s1\n",
    "        state = next_state\n",
    "        score += reward\n",
    "        \n",
    "      if running_score == None:\n",
    "        running_score = score\n",
    "      else:\n",
    "        running_score = running_score * 0.9 + score * 0.1\n",
    "        \n",
    "      if (i + 1) % checkpoint == 0:\n",
    "        is_best = False\n",
    "        if running_score > best_score:\n",
    "          is_best = True\n",
    "          save_torch_model(self.policy, 'model/actor_critic_cartpole_policy_best.pth')\n",
    "          best_score = running_score\n",
    "        save_torch_model(self.policy,'model/actor_critic_cartpole_policy_iter_%d.pth' %(i+1))\n",
    "        print('%d: running_score:%.2f, is_best:%s' %(i+1, running_score, is_best))\n",
    "\n",
    "      self.update_actor_critic(episode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('MountainCar-v0')\n",
    "agent_mc = ActorCritic_MountainCar(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.2 , -0.07], dtype=float32)"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.6 , 0.07], dtype=float32)"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "[-0.44360457  0.        ]\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('MountainCar-v0')\n",
    "o = env.reset()\n",
    "env.render()\n",
    "print(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.9999999999999999"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(-1.2 + 0.3)/0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "-0.07 = -1.0 , 0.07 = 1.0. in * 1.0/0.07\n",
    "-1.2 = -1.0, 0.6 = 1.0\n",
    "\n",
    "range = 1.8\n",
    "in + 0.3 / 0.9 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100: running_score:-200.00, is_best:True\n",
      "200: running_score:-200.00, is_best:False\n",
      "300: running_score:-200.00, is_best:False\n",
      "400: running_score:-200.00, is_best:False\n",
      "500: running_score:-200.00, is_best:False\n",
      "600: running_score:-200.00, is_best:False\n",
      "700: running_score:-200.00, is_best:False\n",
      "800: running_score:-200.00, is_best:False\n",
      "900: running_score:-200.00, is_best:False\n",
      "1000: running_score:-200.00, is_best:False\n"
     ]
    }
   ],
   "source": [
    "agent_mc.train(env, 1000, lr=1e-6, checkpoint=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic_target_eval():\n",
    "  def __init__(self, env, steps_in_state = 2):\n",
    "    self.steps_in_state = steps_in_state\n",
    "    self.policy_target = PolicyNet_discret(env.observation_space.shape[0] * steps_in_state,env.action_space.n)\n",
    "    self.policy_eval = PolicyNet_discret(env.observation_space.shape[0] * steps_in_state,env.action_space.n)\n",
    "    self.value_target = ValueNet(env.observation_space.shape[0] * steps_in_state)\n",
    "    self.value_eval = ValueNet(env.observation_space.shape[0] * steps_in_state)\n",
    "    if use_cuda:\n",
    "      self.policy_target.cuda()\n",
    "      self.policy_eval.cuda()\n",
    "      self.value_target.cuda()\n",
    "      self.value_eval.cuda()\n",
    "    self.policy_eval.load_state_dict(self.policy_target.state_dict())\n",
    "    self.value_eval.load_state_dict(self.value_target.state_dict())\n",
    "    self.env = env\n",
    "    self._gamma = 0.96\n",
    "    \n",
    "  def pick_action(self, state):\n",
    "    probs = self.policy_target(state)\n",
    "    action_dist = Categorical(probs)\n",
    "    action = action_dist.sample()\n",
    "    action = action.item()\n",
    "    return (action, action_dist.log_prob(FloatTensor([action])))\n",
    "  \n",
    "  def update_actor_critic(self, episode):\n",
    "    (states, actions, rewards, next_states, log_probs, ended) = zip(*episode)\n",
    "    \n",
    "    rewards = FloatTensor(rewards)\n",
    "    ended = FloatTensor(ended)\n",
    "    state_value = self.value_target(torch.stack(states))\n",
    "    next_state_value = self.value_target(torch.stack(next_states))\n",
    "    target_value = rewards + (1 - ended) * self._gamma * next_state_value\n",
    "    \n",
    "    delta = target_value - state_value\n",
    "\n",
    "    value_loss = F.mse_loss(state_value, target_value)\n",
    "    \n",
    "    policy_loss = []\n",
    "    for log_prob, d in zip(log_probs, delta):\n",
    "      policy_loss.append(-log_prob * d)\n",
    "    policy_loss = torch.stack(policy_loss).sum()\n",
    "    \n",
    "    self.value_optimizer.zero_grad()\n",
    "    zero_grad(self.value_target)\n",
    "    value_loss.backward(retain_graph=True)\n",
    "    copy_grad(self.value_target, self.value_eval)\n",
    "    self.value_optimizer.step()\n",
    "    \n",
    "    self.policy_optimizer.zero_grad()\n",
    "    zero_grad(self.policy_target)\n",
    "    policy_loss.backward(retain_graph=True)\n",
    "    copy_grad(self.policy_target, self.policy_eval)\n",
    "    self.policy_optimizer.step()\n",
    "\n",
    "  def train(self, env, episode, lr=1e-3, lr_policy=None, lr_value=None, checkpoint=100):\n",
    "    lr_policy = lr if lr_policy == None else lr_policy\n",
    "    lr_value = lr if lr_value == None else lr_value\n",
    "    self.policy_optimizer = torch.optim.Adam(self.policy_eval.parameters(), lr=lr_policy, weight_decay=1e-3)\n",
    "    self.value_optimizer = torch.optim.Adam(self.value_eval.parameters(), lr=lr_value, weight_decay=1e-3)\n",
    "    best_score = -99999\n",
    "    running_score = None\n",
    "    sample_per_update = 500\n",
    "    sample = []\n",
    "    nn_update_count = 0\n",
    "    for i in range(episode):\n",
    "      s0 = env.reset()\n",
    "      seq = [s0] * self.steps_in_state\n",
    "      state = FloatTensor(seq).view(-1)\n",
    "      episode_ended = False\n",
    "      score = 0\n",
    "      while not episode_ended:\n",
    "        (action, log_prob) =  self.pick_action(state)\n",
    "        (s1, reward, episode_ended, info) = env.step(action)\n",
    "        seq = seq[1:]\n",
    "        seq.append(s1)\n",
    "        next_state = FloatTensor(seq).view(-1)\n",
    "        if episode_ended:\n",
    "          ended = 1\n",
    "        else:\n",
    "          ended = 0\n",
    "        sample.append((state, action, reward, next_state, log_prob, ended))\n",
    "\n",
    "        s0 = s1\n",
    "        state = next_state\n",
    "        score += reward\n",
    "        \n",
    "      if running_score == None:\n",
    "        running_score = score\n",
    "      else:\n",
    "        running_score = running_score * 0.9 + score * 0.1\n",
    "\n",
    "      if (i + 1) % checkpoint == 0:\n",
    "        is_best = False\n",
    "        if running_score > best_score:\n",
    "          is_best = True\n",
    "          save_torch_model(self.policy_target, 'model/actor_critic_cartpole_policy_best.pth')\n",
    "          best_score = running_score\n",
    "        save_torch_model(self.policy_target,'model/actor_critic_cartpole_policy_iter_%d.pth' %(i+1))\n",
    "        print('%d: running_score:%.2f, is_best:%s' %(i+1, running_score, is_best))\n",
    "      \n",
    "      if len(sample) > sample_per_update:\n",
    "        self.update_actor_critic(sample)\n",
    "        nn_update_count += 1\n",
    "\n",
    "      if nn_update_count % 5 == 0:\n",
    "        update_target(self.policy_target, self.policy_eval, 0.1)\n",
    "        update_target(self.value_target, self.value_eval, 0.1)\n",
    "        nn_update_count = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "agent_target_eval = ActorCritic_target_eval(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-038199e4da51>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0magent_target_eval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5e-5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-20-7ef399c18dd8>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, env, episode, lr, lr_policy, lr_value, checkpoint)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0msample_per_update\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_actor_critic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m         \u001b[0mnn_update_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-20-7ef399c18dd8>\u001b[0m in \u001b[0;36mupdate_actor_critic\u001b[0;34m(self, episode)\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy_target\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m     \u001b[0mpolicy_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m     \u001b[0mcopy_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy_eval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/pytorch0.4/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/pytorch0.4/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     87\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     88\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "agent_target_eval.train(env, 3000, lr=5e-5, checkpoint=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic_target_eval_continuous():\n",
    "  def __init__(self, env, steps_in_state = 2):\n",
    "    self.steps_in_state = steps_in_state\n",
    "    self.policy_target = PolicyNet_discret(env.observation_space.shape[0] * steps_in_state, env.action_space.shape[0])\n",
    "    self.policy_eval = PolicyNet_discret(env.observation_space.shape[0] * steps_in_state, env.action_space.shape[0])\n",
    "    self.value_target = ValueNet(env.observation_space.shape[0] * steps_in_state)\n",
    "    self.value_eval = ValueNet(env.observation_space.shape[0] * steps_in_state)\n",
    "    if use_cuda:\n",
    "      self.policy_target.cuda()\n",
    "      self.policy_eval.cuda()\n",
    "      self.value_target.cuda()\n",
    "      self.value_eval.cuda()\n",
    "    self.policy_eval.load_state_dict(self.policy_target.state_dict())\n",
    "    self.value_eval.load_state_dict(self.value_target.state_dict())\n",
    "    self.env = env\n",
    "    self.range_scale = (env.action_space.high[0] - env.action_space.low[0]) / 2.0\n",
    "\n",
    "    self._gamma = 0.96\n",
    "    \n",
    "  def pick_action(self, state):\n",
    "    probs = self.policy_target(state) * self.range_scale\n",
    "    action_dist = Normal(probs, 0.01)\n",
    "    action = action_dist.sample()\n",
    "    action = action.item()\n",
    "    return (action, action_dist.log_prob(FloatTensor([action])))\n",
    "  \n",
    "  def update_actor_critic(self, episode):\n",
    "    (states, actions, rewards, next_states, log_probs, ended) = zip(*episode)\n",
    "    \n",
    "    rewards = FloatTensor(rewards)\n",
    "    ended = FloatTensor(ended)\n",
    "    state_value = self.value_target(torch.stack(states))\n",
    "    next_state_value = self.value_target(torch.stack(next_states))\n",
    "    target_value = rewards + (1 - ended) * self._gamma * next_state_value\n",
    "    \n",
    "    delta = target_value - state_value\n",
    "\n",
    "    value_loss = F.mse_loss(state_value, target_value)\n",
    "    \n",
    "    policy_loss = []\n",
    "    for log_prob, d in zip(log_probs, delta):\n",
    "      policy_loss.append(-log_prob * d)\n",
    "    policy_loss = torch.stack(policy_loss).sum()\n",
    "    \n",
    "    self.value_optimizer.zero_grad()\n",
    "    zero_grad(self.value_target)\n",
    "    value_loss.backward(retain_graph=True)\n",
    "    copy_grad(self.value_target, self.value_eval)\n",
    "    self.value_optimizer.step()\n",
    "    \n",
    "    self.policy_optimizer.zero_grad()\n",
    "    zero_grad(self.policy_target)\n",
    "    policy_loss.backward()\n",
    "    copy_grad(self.policy_target, self.policy_eval)\n",
    "    self.policy_optimizer.step()\n",
    "    \n",
    "  def train(self, env, episode, lr=1e-3, lr_policy=None, lr_value=None, checkpoint=100):\n",
    "    train_data = {\n",
    "      'model':{\n",
    "        'policy_eval':self.policy_eval.state_dict(),\n",
    "        'policy_target':self.policy_target.state_dict(),\n",
    "        'value_eval':self.value_eval.state_dict(),\n",
    "        'value_target':self.value_target.state_dict()\n",
    "      },\n",
    "      'episode':0,\n",
    "      'statistic':{\n",
    "      }\n",
    "    }\n",
    "    lr_policy = lr if lr_policy == None else lr_policy\n",
    "    lr_value = lr if lr_value == None else lr_value\n",
    "    self.policy_optimizer = torch.optim.SGD(self.policy_eval.parameters(), lr=lr_policy, weight_decay=1e-3)\n",
    "    self.value_optimizer = torch.optim.SGD(self.value_eval.parameters(), lr=lr_value, weight_decay=1e-3)\n",
    "    best_score = -99999\n",
    "    running_score = None\n",
    "    \n",
    "    for i in range(episode):\n",
    "      s0 = env.reset()\n",
    "      seq = [s0] * self.steps_in_state\n",
    "      state = FloatTensor(seq).view(-1)\n",
    "      episode = []\n",
    "      episode_ended = False\n",
    "      score = 0\n",
    "      while not episode_ended:\n",
    "        (action, log_prob) =  self.pick_action(state)\n",
    "        (s1, reward, episode_ended, info) = env.step([action])\n",
    "        seq = seq[1:]\n",
    "        seq.append(s1)\n",
    "        next_state = FloatTensor(seq).view(-1)\n",
    "        if episode_ended:\n",
    "          ended = 1\n",
    "        else:\n",
    "          ended = 0\n",
    "        episode.append((state, action, reward, next_state, log_prob, ended))\n",
    "\n",
    "        s0 = s1\n",
    "        state = next_state\n",
    "        score += reward\n",
    "        \n",
    "      if running_score == None:\n",
    "        running_score = score\n",
    "      else:\n",
    "        running_score = running_score * 0.9 + score * 0.1\n",
    "        \n",
    "      if (i + 1) % checkpoint == 0:\n",
    "        is_best = False\n",
    "        if running_score > best_score:\n",
    "          is_best = True\n",
    "          save_torch_model(self.policy_target, 'model/actor_critic_pendulum_policy_best.pth')\n",
    "          best_score = running_score\n",
    "        save_torch_model(self.policy_target,'model/actor_critic_pendulum_policy_iter_%d.pth' %(i+1))\n",
    "        print('%d: running_score:%.2f, is_best:%s' %(i+1, running_score, is_best))\n",
    "        \n",
    "      self.update_actor_critic(episode)\n",
    "\n",
    "      if i % 20 == 0:\n",
    "        update_target(self.policy_target, self.policy_eval, 0.1)\n",
    "        update_target(self.value_target, self.value_eval, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('Pendulum-v0')\n",
    "agent_target_eval_continuous = ActorCritic_target_eval_continuous(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50: running_score:-1377.57, is_best:True\n",
      "100: running_score:-1523.91, is_best:False\n",
      "150: running_score:-1418.31, is_best:False\n",
      "200: running_score:-1345.32, is_best:True\n",
      "250: running_score:-1478.96, is_best:False\n",
      "300: running_score:-1399.82, is_best:False\n",
      "350: running_score:-1368.12, is_best:False\n",
      "400: running_score:-1390.04, is_best:False\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-140-7356279dbf56>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0magent_target_eval_continuous\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-138-058fd5e418f3>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, env, episode, lr, lr_policy, lr_value, checkpoint)\u001b[0m\n\u001b[1;32m     82\u001b[0m       \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m       \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mepisode_ended\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         \u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_prob\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpick_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m         \u001b[0;34m(\u001b[0m\u001b[0ms1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisode_ended\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0mseq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-138-058fd5e418f3>\u001b[0m in \u001b[0;36mpick_action\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maction_dist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_dist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mupdate_actor_critic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/pytorch0.4/lib/python3.6/site-packages/torch/distributions/normal.py\u001b[0m in \u001b[0;36mlog_prob\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0mvar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0mlog_scale\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNumber\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mvar\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlog_scale\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcdf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "agent_target_eval_continuous.train(env, 8000, lr=1e-6, checkpoint=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic_continuous():\n",
    "  def __init__(self, env, steps_in_state = 2):\n",
    "    self.steps_in_state = steps_in_state\n",
    "    self.policy = PolicyNet_continuous(env.observation_space.shape[0] * steps_in_state, env.action_space.shape[0])\n",
    "    self.value = ValueNet(env.observation_space.shape[0] * steps_in_state)\n",
    "    self.range_scale = (env.action_space.high[0] - env.action_space.low[0]) / 2.0\n",
    "    self.env = env\n",
    "    self._gamma = 0.96\n",
    "    \n",
    "  def predict_value(self, state):\n",
    "    return self.value(state)\n",
    "  \n",
    "  def predict_action(self, state):\n",
    "    return self.policy(state)\n",
    "  \n",
    "  def pick_action(self, state):\n",
    "    probs = self.predict_action(state) * self.range_scale\n",
    "    action_dist = Normal(probs, 0.2)\n",
    "    action = action_dist.sample()\n",
    "    action = action.item()\n",
    "    return (action, action_dist.log_prob(FloatTensor([action])))\n",
    "  \n",
    "  def update_actor_critic(self, episode):\n",
    "    (states, actions, rewards, next_states, log_probs, ended) = zip(*episode)\n",
    "    \n",
    "    rewards = FloatTensor(rewards)\n",
    "    ended = FloatTensor(ended)\n",
    "    state_value = self.value(torch.stack(states))\n",
    "    next_state_value = self.value(torch.stack(next_states))\n",
    "    target_value = rewards + (1 - ended) * self._gamma * next_state_value\n",
    "    \n",
    "    delta = target_value - state_value\n",
    "\n",
    "    value_loss = F.mse_loss(state_value, target_value)\n",
    "    \n",
    "    policy_loss = []\n",
    "    for log_prob, d in zip(log_probs, delta):\n",
    "      policy_loss.append(-log_prob * d)\n",
    "    policy_loss = torch.stack(policy_loss).sum()\n",
    "    \n",
    "    self.value_optimizer.zero_grad()\n",
    "    value_loss.backward(retain_graph=True)\n",
    "    self.value_optimizer.step()\n",
    "    \n",
    "    self.policy_optimizer.zero_grad()\n",
    "    policy_loss.backward()\n",
    "    self.policy_optimizer.step()\n",
    "\n",
    "  def train(self, env, episode, lr=1e-3, lr_policy=None, lr_value=None, checkpoint=100):\n",
    "    lr_policy = lr if lr_policy == None else lr_policy\n",
    "    lr_value = lr if lr_value == None else lr_value\n",
    "    self.policy_optimizer = torch.optim.Adam(self.policy.parameters(), lr=lr_policy, weight_decay=1e-3)\n",
    "    self.value_optimizer = torch.optim.Adam(self.value.parameters(), lr=lr_value, weight_decay=1e-3)\n",
    "    best_score = -99999\n",
    "    for i in range(episode):\n",
    "      s0 = env.reset()\n",
    "      seq = [s0] * self.steps_in_state\n",
    "      state = FloatTensor(seq).view(-1)\n",
    "      episode = []\n",
    "      episode_ended = False\n",
    "      score = 0\n",
    "      while not episode_ended:\n",
    "        (action, log_prob) =  self.pick_action(state)\n",
    "        (s1, reward, episode_ended, info) = env.step([action])\n",
    "        seq = seq[1:]\n",
    "        seq.append(s1)\n",
    "        next_state = FloatTensor(seq).view(-1)\n",
    "        if episode_ended:\n",
    "          ended = 1\n",
    "        else:\n",
    "          ended = 0\n",
    "        episode.append((state, action, reward, next_state, log_prob, ended))\n",
    "\n",
    "        s0 = s1\n",
    "        state = next_state\n",
    "        score += reward\n",
    "        \n",
    "      if score > best_score:\n",
    "        save_torch_model(self.policy, 'model/actor_critic_Pendulum_policy_best.pth')\n",
    "        best_score = score\n",
    "        print('new best score:',best_score)\n",
    "        \n",
    "      self.update_actor_critic(episode)\n",
    "\n",
    "      if (i + 1) % checkpoint == 0:\n",
    "        save_torch_model(self.policy,'model/actor_critic_Pendulum_policy_iter_%d.pth' %(i+1))\n",
    "        print(i+1,': score:', score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('Pendulum-v0')\n",
    "agent = ActorCritic_continuous(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new best score: -1420.6990168430254\n",
      "new best score: -1240.4158551602857\n",
      "new best score: -1181.922163316851\n",
      "new best score: -1116.51925322667\n",
      "new best score: -1080.0822897288838\n",
      "new best score: -715.8367356613004\n",
      "50 : score: -1513.117540566958\n",
      "100 : score: -1511.7525513447758\n",
      "150 : score: -1558.7815375703308\n",
      "200 : score: -1228.3368534381889\n",
      "250 : score: -1386.1700872323856\n",
      "300 : score: -1455.4761546471093\n",
      "350 : score: -1368.2759435930686\n",
      "400 : score: -1509.7915599460023\n",
      "450 : score: -1517.2225833786401\n",
      "500 : score: -1417.091764092609\n",
      "550 : score: -1522.2141861350397\n",
      "600 : score: -1366.1865629114368\n",
      "650 : score: -1451.0626714816392\n",
      "new best score: -528.1708752457364\n",
      "700 : score: -1510.3500868767105\n",
      "750 : score: -1532.600699011077\n",
      "800 : score: -1483.236599622256\n",
      "850 : score: -1574.0829310164986\n",
      "900 : score: -1634.894965989276\n",
      "950 : score: -1477.6349356056828\n",
      "1000 : score: -859.2797486401734\n"
     ]
    }
   ],
   "source": [
    "agent.train(env, 1000, lr=1e-4, checkpoint=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "-1219.606081432608\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('KungFuMaster-ram-v0')\n",
    "env = gym.make('Pendulum-v0')\n",
    "def avg_reward_random(env, iteration):\n",
    "  total_reward = 0.0\n",
    "  for i in range(iteration):\n",
    "    env.reset()\n",
    "    ended = False\n",
    "    while not ended:\n",
    "      (state, reward, ended, info) = env.step(env.action_space.sample())\n",
    "      total_reward += reward\n",
    "  print (total_reward / iteration)\n",
    "avg_reward_random(env, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = nn.Linear(50,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.]])"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l.weight.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.]])"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch0.4]",
   "language": "python",
   "name": "conda-env-pytorch0.4-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
