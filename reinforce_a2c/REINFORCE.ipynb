{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REINFORCE - Monte Carlo Policy Gradient Method for Cartpole\n",
    "\n",
    "The REINFORCE policy gradient is defined by $\\nabla J(\\theta)=E_\\pi[G_t*\\nabla log(\\pi_\\theta(stat))]$. Where $G_t$ is the discounted Monte Carlo reward ie. $G_t = \\sum_{k=0}^T\\gamma^k*r_{t+k}$ \n",
    "\n",
    "To do the policy gradient in pytorch, here it calculate the policy gradient esitmator as $loss_{policy}=\\sum_{t=0}^T[log(\\pi_\\theta(state_t))*G_t]$ and just call the backward function on the loss tensor / variable.\n",
    "\n",
    "The base algorithm of REINFORCE suffer from high variance as it estimate the advantage using $G_t = \\sum_{k=0}^T\\gamma^k*r_{t+k}$. An \n",
    "improved version of REINFORCE add a state-value function estimator as baseline to reduce the variance. The REINFORCE_wBaseline class after the vallina REINFORCE class added a neural network as a state-value function approximator. The new policy gradient esitmator with baseline is: $loss_{policy}=\\sum_{t=0}^T[log(\\pi_\\theta(state_t))*(G_t - V(state_t))]$. \n",
    "\n",
    "For the gradient estimator of state-value function, it can simply reuse the same monte carlo reward to caculate the mean square error(MSE): $loss_v=MSE(G_t - V(state_t))$ or use TD error: $loss_v=MSE(r_{t+1} + \\gamma * V(state_{t+1}) - V(state_t))$. Here in the implementation it reuse the monte carlo reward $G_t$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.distributions import Categorical, Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "FloatTensor = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor if use_cuda else torch.LongTensor\n",
    "ByteTensor = torch.cuda.ByteTensor if use_cuda else torch.ByteTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_torch_model(model, filename):\n",
    "  if not os.path.exists(os.path.dirname(filename)):\n",
    "    os.makedirs(os.path.dirname(filename))\n",
    "  torch.save(model.state_dict(), filename)\n",
    "\n",
    "def load_torch_model(model, filename):\n",
    "  model.load_state_dict(torch.load(filename))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNet_discret(nn.Module):\n",
    "  def __init__(self, input_size, output_size):\n",
    "    super(PolicyNet_discret,self).__init__()\n",
    "    self.l1_linear = nn.Linear(input_size, 128)\n",
    "    self.l2_linear = nn.Linear(128, 64)\n",
    "    self.l3_linear = nn.Linear(64, output_size)\n",
    "    nn.init.kaiming_normal_(self.l1_linear.weight)\n",
    "    nn.init.kaiming_normal_(self.l2_linear.weight)\n",
    "    self.l3_linear.weight.data.zero_()\n",
    "    \n",
    "  def forward(self,x):\n",
    "    out = F.relu(self.l1_linear(x))\n",
    "    out = F.relu(self.l2_linear(out))\n",
    "    out = F.softmax(self.l3_linear(out),dim=0)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueNet(nn.Module):\n",
    "  def __init__(self, input_size):\n",
    "    super(ValueNet,self).__init__()\n",
    "    self.l1_linear = nn.Linear(input_size, 128)\n",
    "    self.l2_linear = nn.Linear(128, 64)\n",
    "    self.l3_linear = nn.Linear(64, 1)\n",
    "    nn.init.kaiming_normal_(self.l1_linear.weight)\n",
    "    nn.init.kaiming_normal_(self.l2_linear.weight)\n",
    "    self.l3_linear.weight.data.zero_()\n",
    "\n",
    "  def forward(self, x):\n",
    "    out = F.relu(self.l1_linear(x))\n",
    "    out = F.relu(self.l2_linear(out))\n",
    "    out = self.l3_linear(out)\n",
    "    return out    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class REINFORCE():\n",
    "  def __init__(self, env, steps_in_state = 2):\n",
    "    self.steps_in_state = steps_in_state\n",
    "    self.policy = PolicyNet_discret(env.observation_space.shape[0] * steps_in_state,env.action_space.n)\n",
    "    self.env = env\n",
    "    self._gamma = 0.96\n",
    "\n",
    "  def init_state(self, env_state):\n",
    "    self.running_state_seq = [env_state] * self.steps_in_state\n",
    "    self.running_state = FloatTensor(self.running_state_seq).view(-1)\n",
    "\n",
    "  def add_state(self, env_state):\n",
    "    self.running_state_seq = self.running_state_seq[1:]\n",
    "    self.running_state_seq.append(env_state)\n",
    "    self.running_state = FloatTensor(self.running_state_seq).view(-1)\n",
    "\n",
    "  def get_action(self):\n",
    "    action = self.policy(self.running_state)\n",
    "    return action.argmax().item()\n",
    "\n",
    "  def pick_action(self, state):\n",
    "    probs = self.policy(state)\n",
    "    action_dist = Categorical(probs)\n",
    "    action = action_dist.sample()\n",
    "    action = action.item()\n",
    "    return (action, action_dist.log_prob(FloatTensor([action])))\n",
    "  \n",
    "  def update_policy(self, rollout):\n",
    "    (states, actions, rewards, log_probs) = zip(*rollout)\n",
    "    MC_rewards = []\n",
    "    R = 0\n",
    "    for r in rewards[::-1]:\n",
    "      R = r + self._gamma * R\n",
    "      MC_rewards.insert(0, R)\n",
    "    loss = []\n",
    "    for (log_prob, reward) in zip(log_probs, MC_rewards):\n",
    "      loss.append(-log_prob*reward)\n",
    "    loss = torch.stack(loss).sum()\n",
    "    self.optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    self.optimizer.step()\n",
    "\n",
    "  def train(self, env, episode, lr=1e-3, checkpoint=100):\n",
    "    self.optimizer = torch.optim.Adam(self.policy.parameters(), lr=lr, weight_decay=1e-3)\n",
    "    best_score = 0\n",
    "    running_score = None\n",
    "    for i in range(episode):\n",
    "      s0 = env.reset()\n",
    "      self.init_state(s0)\n",
    "      state = self.running_state\n",
    "      rollout = []\n",
    "      episode_ended = False\n",
    "      score = 0\n",
    "      while not episode_ended:\n",
    "        (action, log_prob) =  self.pick_action(state)\n",
    "        (s1, reward, episode_ended, info) = env.step(action)\n",
    "        rollout.append((state, action, reward, log_prob))\n",
    "        self.add_state(s1)\n",
    "        next_state = self.running_state\n",
    "        if episode_ended:\n",
    "          ended = 1\n",
    "        else:\n",
    "          ended = 0\n",
    "        s0 = s1\n",
    "        state = next_state\n",
    "        score += reward\n",
    "    \n",
    "      if running_score == None:\n",
    "        running_score = score\n",
    "      else:\n",
    "        running_score = running_score * 0.9 + 0.1 * score\n",
    "        \n",
    "      if (i + 1) % checkpoint == 0:\n",
    "        if running_score > best_score:\n",
    "          save_torch_model(self.policy, 'model/reinforce_cartpole_best.pth')\n",
    "          best_score = running_score\n",
    "        save_torch_model(self.policy,'model/reinforce_cartpole_iter_%d.pth' %(i+1))\n",
    "        print(i+1,': running_score:', running_score)\n",
    "\n",
    "      self.update_policy(rollout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "agent = REINFORCE(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200 : running_score: 21.269940462098422\n",
      "400 : running_score: 29.375980104308145\n",
      "600 : running_score: 50.05871229111651\n",
      "800 : running_score: 69.30404227531287\n",
      "1000 : running_score: 97.64375629710904\n",
      "1200 : running_score: 141.81128305453186\n",
      "1400 : running_score: 164.22592458012318\n",
      "1600 : running_score: 172.4267281043214\n",
      "1800 : running_score: 191.29587196835206\n",
      "2000 : running_score: 161.36453303726924\n"
     ]
    }
   ],
   "source": [
    "agent.train(env, episode=2000, lr=7e-5, checkpoint=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class REINFORCE_wBaseline():\n",
    "  def __init__(self, env, steps_in_state = 2):\n",
    "    self.steps_in_state = steps_in_state\n",
    "    self.policy = PolicyNet_discret(env.observation_space.shape[0] * steps_in_state,env.action_space.n)\n",
    "    self.value = ValueNet(env.observation_space.shape[0] * steps_in_state)\n",
    "    self.env = env\n",
    "    self._gamma = 0.96\n",
    "    \n",
    "  def predict_value(self, state):\n",
    "    return self.value(state)\n",
    "  \n",
    "  def predict_action(self, state):\n",
    "    return self.policy(state)\n",
    "  \n",
    "  def pick_action(self, state):\n",
    "    probs = self.predict_action(state)\n",
    "    action_dist = Categorical(probs)\n",
    "    action = action_dist.sample()\n",
    "    action = action.item()\n",
    "    return (action, action_dist.log_prob(FloatTensor([action])))\n",
    "  \n",
    "  def update_policy_and_value(self, rollout):\n",
    "    (states, actions, rewards, log_probs) = zip(*rollout)\n",
    "    \n",
    "    MC_rewards = []\n",
    "    R = 0\n",
    "    for r in rewards[::-1]:\n",
    "      R = r + self._gamma * R\n",
    "      MC_rewards.insert(0, R)\n",
    "      \n",
    "    value_prediction = self.value(torch.stack(states))\n",
    "    value_loss = F.mse_loss(value_prediction, FloatTensor(MC_rewards).view(-1,1))\n",
    "    \n",
    "    policy_loss = []\n",
    "    for (log_prob, reward, baseline) in zip(log_probs, MC_rewards, value_prediction.view(-1).tolist()):\n",
    "      policy_loss.append(-log_prob*(reward - baseline))\n",
    "    policy_loss = torch.stack(policy_loss).sum()\n",
    "    \n",
    "    self.value_optimizer.zero_grad()\n",
    "    value_loss.backward()\n",
    "    self.value_optimizer.step()\n",
    "    \n",
    "    self.policy_optimizer.zero_grad()\n",
    "    policy_loss.backward()\n",
    "    self.policy_optimizer.step()\n",
    "\n",
    "  def train(self, env, episode=1000, lr=1e-3, lr_policy=None, lr_value=None, checkpoint=100):\n",
    "    lr_policy = lr if lr_policy == None else lr_policy\n",
    "    lr_value = lr if lr_value == None else lr_value\n",
    "    self.policy_optimizer = torch.optim.Adam(self.policy.parameters(), lr=lr_policy, weight_decay=1e-3)\n",
    "    self.value_optimizer = torch.optim.Adam(self.value.parameters(), lr=lr_value, weight_decay=1e-3)\n",
    "    best_score = -99999\n",
    "    running_score = None\n",
    "    for i in range(episode):\n",
    "      s0 = env.reset()\n",
    "      seq = [s0] * self.steps_in_state\n",
    "      rollout = []\n",
    "      state = FloatTensor(seq).view(-1)\n",
    "      episode_ended = False\n",
    "      score = 0\n",
    "      while not episode_ended:\n",
    "        (action, log_prob) =  self.pick_action(state)\n",
    "        (s1, reward, episode_ended, info) = env.step(action)\n",
    "        rollout.append((state, action, reward, log_prob))\n",
    "        seq = seq[1:]\n",
    "        seq.append(s1)\n",
    "        next_state = FloatTensor(seq).view(-1)\n",
    "        if episode_ended:\n",
    "          ended = 1\n",
    "        else:\n",
    "          ended = 0\n",
    "        s0 = s1\n",
    "        state = next_state\n",
    "        score += reward\n",
    "        \n",
    "      if running_score == None:\n",
    "        running_score = score\n",
    "      else:\n",
    "        running_score = running_score * 0.9 + score * 0.1\n",
    "        \n",
    "      if (i + 1) % checkpoint == 0:\n",
    "        if running_score > best_score:\n",
    "          save_torch_model(self.policy, 'model/reinforce_cartpole_best.pth')\n",
    "          best_score = running_score\n",
    "        save_torch_model(self.policy,'model/reinforce_cartpole_iter_%d.pth' %(i+1))\n",
    "        print(i+1,': running_score:', running_score)\n",
    "        \n",
    "      self.update_policy_and_value(rollout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "agent = REINFORCE_wBaseline(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200 : running_score: 22.324116660275386\n",
      "400 : running_score: 43.74702665869132\n",
      "600 : running_score: 95.82207029866576\n",
      "800 : running_score: 163.71738047252506\n",
      "1000 : running_score: 184.3465488749348\n",
      "1200 : running_score: 171.7118348831992\n",
      "1400 : running_score: 192.37269649593756\n",
      "1600 : running_score: 179.388897217873\n",
      "1800 : running_score: 194.5123934497959\n",
      "2000 : running_score: 193.0464476556347\n"
     ]
    }
   ],
   "source": [
    "agent.train(env, episode=2000, lr=1e-4, checkpoint=200)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch0.4]",
   "language": "python",
   "name": "conda-env-pytorch0.4-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
