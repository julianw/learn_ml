{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Deterministic Policy Gradient (DDPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This implement the pseudo code in [CONTINUOUS CONTROL WITH DEEP REINFORCEMENT\n",
    "LEARNING](https://arxiv.org/pdf/1509.02971.pdf) from Deepmind in pytorch.  \n",
    "<img src=\"ddpg_pseudocode.png\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One change in the code here $\\mathcal{N}$ is just a uniform random number of -0.5 to 0.5 that decay over number of iteration, $\\mathcal{N} = (rand() - 0.5) / (1 + 1e^{-3} * iteration)$. In the paper they used Ornstein-Uhlenbeck process for $\\mathcal{N}$, but for simple task like pendulum the uniform random noise seems to work just fine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Couple thing worth mentioning when implementing DDPG.\n",
    "- When updating the Q network, it use Q' and P' to calculate the Q target. i.e. $Q\\_target=reward + \\gamma * Q'(state\\_next, P'(state\\_next))$\n",
    "- When caculate the Q loss, it use $Q\\_target - Q(state, action)$ where action is from the replay memory instead of P(state)\n",
    "- The policy gradient  \n",
    "<img src=\"ddpg_policy_grad.png\" width=\"350\"/>  \n",
    "Actuall mean the gradient of $\\theta^\\mu$ with respect to the mean of $Q(s,P(s))$. Here in pytorch it performs gradient descent, so the policy loss is $-\\bar{Q}(s,P(s))$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from IPython import display\n",
    "from IPython.display import HTML\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.distributions import Normal, Categorical\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "FloatTensor = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor if use_cuda else torch.LongTensor\n",
    "ByteTensor = torch.cuda.ByteTensor if use_cuda else torch.ByteTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_torch_model(model, filename):\n",
    "  if not os.path.exists(os.path.dirname(filename)):\n",
    "    os.makedirs(os.path.dirname(filename))\n",
    "  torch.save(model.state_dict(), filename)\n",
    "\n",
    "def load_torch_model(model, filename):\n",
    "  model.load_state_dict(torch.load(filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_grad(source, target):\n",
    "  grads = []\n",
    "  for param in source.parameters():\n",
    "    grads.append(param.grad.clone())\n",
    "  grads.reverse()\n",
    "  for param in target.parameters():\n",
    "    param.grad = grads.pop()\n",
    "\n",
    "def zero_grad(model):\n",
    "  for param in model.parameters():\n",
    "    if type(param.grad) != type(None):\n",
    "      param.grad.data.zero_()\n",
    "      \n",
    "def update_target(target_net, eval_net, tau):\n",
    "  fast = eval_net.state_dict()\n",
    "  slow = target_net.state_dict()\n",
    "  for t in slow:\n",
    "    slow[t] = slow[t] * (1. - tau) + fast[t] * tau\n",
    "\n",
    "  target_net.load_state_dict(slow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory():\n",
    "  def __init__(self, memory_size = 100000):\n",
    "    self.transitions = []\n",
    "    self.memory_size = memory_size\n",
    "    self.loc_pointer = 0\n",
    "  \n",
    "  def clear(self):\n",
    "    self.transitions = []\n",
    "    self.loc_pointer = 0\n",
    "  \n",
    "  def add(self, step_tuple):\n",
    "    # expect a tuple of transition contain:\n",
    "    # state, action, reward, next_state, ended\n",
    "    if len(self.transitions) <= self.loc_pointer:\n",
    "      self.transitions.append(None)\n",
    "    self.transitions[self.loc_pointer] = step_tuple\n",
    "    self.loc_pointer += 1\n",
    "    if self.loc_pointer >= self.memory_size:\n",
    "      self.loc_pointer %= self.memory_size\n",
    "  \n",
    "  def get_sample(self, batch_size):\n",
    "    return random.sample(self.transitions, batch_size)\n",
    "  \n",
    "  def usage(self):\n",
    "    return len(self.transitions) / self.memory_size\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNet_continuous(nn.Module):\n",
    "  def __init__(self, input_size, output_size):\n",
    "    super(PolicyNet_continuous,self).__init__()\n",
    "    self.l1_linear = nn.Linear(input_size, 128)\n",
    "    self.l2_linear = nn.Linear(128, 64)\n",
    "    self.l3_linear = nn.Linear(64, output_size)\n",
    "    nn.init.kaiming_normal_(self.l1_linear.weight)\n",
    "    nn.init.kaiming_normal_(self.l2_linear.weight)\n",
    "    self.l3_linear.weight.data.zero_()\n",
    "    \n",
    "  def forward(self,x):\n",
    "    out = F.relu(self.l1_linear(x))\n",
    "    out = F.relu(self.l2_linear(out))\n",
    "    out = F.tanh(self.l3_linear(out))\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNet(nn.Module):\n",
    "  def __init__(self, input_size):\n",
    "    super(QNet,self).__init__()\n",
    "    self.l1_linear = nn.Linear(input_size, 128)\n",
    "    self.l2_linear = nn.Linear(128, 64)\n",
    "    self.l3_linear = nn.Linear(64, 1)\n",
    "    nn.init.kaiming_normal_(self.l1_linear.weight)\n",
    "    nn.init.kaiming_normal_(self.l2_linear.weight)\n",
    "    self.l3_linear.weight.data.zero_()\n",
    "\n",
    "  def forward(self, state, action):\n",
    "    out = F.relu(self.l1_linear(torch.cat([state,action],dim=1)))\n",
    "    out = F.relu(self.l2_linear(out))\n",
    "    out = self.l3_linear(out)\n",
    "    return out    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPG():\n",
    "  def __init__(self, env, steps_in_state = 1):\n",
    "    self.is_training = True\n",
    "    self.state_value_range = [{'max':None, 'min':None}] * env.observation_space.shape[0]\n",
    "    self.replay_memory = ReplayMemory(memory_size=100000)\n",
    "    self.steps_in_state = steps_in_state\n",
    "    self.actor = PolicyNet_continuous(env.observation_space.shape[0] * steps_in_state, env.action_space.shape[0])\n",
    "    self.actor_prime = PolicyNet_continuous(env.observation_space.shape[0] * steps_in_state, env.action_space.shape[0])\n",
    "    self.critic = QNet(env.observation_space.shape[0] * steps_in_state + env.action_space.shape[0])\n",
    "    self.critic_prime = QNet(env.observation_space.shape[0] * steps_in_state + env.action_space.shape[0])\n",
    "    if use_cuda:\n",
    "      self.actor.cuda()\n",
    "      self.actor_prime.cuda()\n",
    "      self.critic.cuda()\n",
    "      self.critic_prime.cuda()\n",
    "    # copy the weights from target to eval net\n",
    "    self.actor_prime.load_state_dict(self.actor.state_dict())\n",
    "    self.critic_prime.load_state_dict(self.critic.state_dict())\n",
    "    self.env = env\n",
    "    self.range_scale = (env.action_space.high[0] - env.action_space.low[0]) / 2.0\n",
    "    self._gamma = 0.96\n",
    "\n",
    "  def pick_action(self, state):\n",
    "    action = self.actor(state)\n",
    "    action = action.item()\n",
    "    # add noise\n",
    "    if self.is_training:\n",
    "      action += (np.random.rand() - 0.5) / (1 + 1e-3 * self.iteration)\n",
    "    return np.clip(action, -1.0, 1.0)\n",
    "  \n",
    "  def update_ddpg(self, batch):\n",
    "    (states, actions, rewards, next_states, ended) = zip(*batch)\n",
    "    states_tensor = torch.stack(states)\n",
    "    actions_tensor = FloatTensor(actions).view(-1,1)\n",
    "    rewards_tensor = FloatTensor(rewards).view(-1,1)\n",
    "    next_states_tensor = torch.stack(next_states)\n",
    "    ended_tensor = FloatTensor(ended).view(-1,1)\n",
    "\n",
    "    critic_target = rewards_tensor + self._gamma * (1 - ended_tensor) * \\\n",
    "      self.critic_prime(next_states_tensor, self.actor_prime(next_states_tensor))\n",
    "    critic_loss = F.mse_loss(self.critic(states_tensor, actions_tensor), critic_target)\n",
    "\n",
    "    self.critic_coptimizer.zero_grad()\n",
    "    critic_loss.backward()\n",
    "    self.critic_coptimizer.step()\n",
    "    \n",
    "    actor_loss = -self.critic(states_tensor, self.actor(states_tensor))\n",
    "    actor_loss = actor_loss.mean()\n",
    "    \n",
    "    self.actor_optimizer.zero_grad()\n",
    "    actor_loss.backward()\n",
    "    self.actor_optimizer.step()\n",
    "    \n",
    "    update_target(self.critic_prime, self.critic, 0.1)\n",
    "    update_target(self.actor_prime, self.actor, 0.1)\n",
    "    \n",
    "  def train(self, env, update_limit=1000, batch_size=200, lr=1e-3, lr_actor=None, lr_critic=None, checkpoint=100):\n",
    "    lr_actor = lr if lr_actor == None else lr_actor\n",
    "    lr_critic = lr if lr_critic == None else lr_critic\n",
    "    \n",
    "    self.actor_optimizer = torch.optim.SGD(self.actor.parameters(), lr=lr_actor, weight_decay=1e-3)\n",
    "    self.critic_coptimizer = torch.optim.SGD(self.critic.parameters(), lr=lr_critic, weight_decay=1e-3)\n",
    "    \n",
    "    best_score = -99999\n",
    "    running_score = None\n",
    "    update_count = 0\n",
    "    self.iteration = 0\n",
    "    while update_count < update_limit:\n",
    "      s0 = env.reset()\n",
    "      seq = [s0] * self.steps_in_state\n",
    "      state = FloatTensor(seq).view(-1)\n",
    "      episode_ended = False\n",
    "      score = 0\n",
    "      while not episode_ended:\n",
    "        action =  self.pick_action(state)\n",
    "        (s1, reward, episode_ended, info) = env.step([action * self.range_scale])\n",
    "        seq = seq[1:]\n",
    "        seq.append(s1)\n",
    "        next_state = FloatTensor(seq).view(-1)\n",
    "        if episode_ended:\n",
    "          ended = 1\n",
    "        else:\n",
    "          ended = 0\n",
    "        self.replay_memory.add((state, action, reward, next_state, ended))\n",
    "        if self.replay_memory.usage() > 0.8:\n",
    "          if (update_count + 1) % checkpoint == 0:\n",
    "            save_torch_model(self.actor,'model/ddpg_actor_iter_%d.pth' %(update_count+1))\n",
    "            print('%d: running_score:%.2f, ' %(update_count+1, running_score))\n",
    "          self.update_ddpg(self.replay_memory.get_sample(batch_size))\n",
    "          update_count += 1\n",
    "          self.iteration += 1\n",
    "\n",
    "        s0 = s1\n",
    "        state = next_state\n",
    "        score += reward\n",
    "      if running_score == None:\n",
    "        running_score = score\n",
    "      else:\n",
    "        running_score = running_score * 0.9 + score * 0.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('Pendulum-v0')\n",
    "agent = DDPG(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent.train(env, update_limit=1, batch_size=2, update_per_episode=1, lr=1e-5, checkpoint=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000: running_score:-1179.21, \n",
      "2000: running_score:-1095.16, \n",
      "3000: running_score:-1036.01, \n",
      "4000: running_score:-1022.48, \n",
      "5000: running_score:-986.71, \n",
      "6000: running_score:-955.99, \n",
      "7000: running_score:-923.19, \n",
      "8000: running_score:-886.69, \n",
      "9000: running_score:-825.40, \n",
      "10000: running_score:-701.01, \n",
      "11000: running_score:-547.35, \n",
      "12000: running_score:-421.68, \n",
      "13000: running_score:-328.89, \n",
      "14000: running_score:-292.08, \n",
      "15000: running_score:-268.45, \n",
      "16000: running_score:-212.12, \n",
      "17000: running_score:-215.08, \n",
      "18000: running_score:-157.03, \n",
      "19000: running_score:-174.16, \n",
      "20000: running_score:-208.39, \n"
     ]
    }
   ],
   "source": [
    "agent.is_training = True\n",
    "agent.train(env, update_limit=20000, batch_size=64, lr_actor=1e-4, lr_critic=1e-3, checkpoint=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-124.88584956807608\n"
     ]
    }
   ],
   "source": [
    "# run a sample episode with a trained agent\n",
    "agent.is_training = False\n",
    "load_torch_model(agent.actor,'model/ddpg_actor_iter_18000.pth')\n",
    "state = env.reset()\n",
    "frames = []\n",
    "frames.append(env.render(mode='rgb_array'))\n",
    "ended = False\n",
    "score = 0\n",
    "while not ended:\n",
    "  action = agent.pick_action(FloatTensor([state]).view(-1))\n",
    "  (state, reward, ended, info) = env.step([action*2])\n",
    "  score += reward\n",
    "  frames.append(env.render(mode='rgb_array'))\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "def animate(frames):\n",
    "  fig, ax = plt.subplots()\n",
    "  ax.grid('off')\n",
    "  ax.axis('off')\n",
    "  ims = []\n",
    "  for i in range(len(frames)):\n",
    "      im = plt.imshow(frames[i], animated=True)\n",
    "      ims.append([im])\n",
    "  ani = animation.ArtistAnimation(fig, ims, interval=20, blit=True, repeat_delay=1000)\n",
    "  return ani\n",
    "\n",
    "ani = animate(frames)\n",
    "ani.save('pendulum_ddpg.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video width=\"400\" controls loop>\n",
       "  <source src=\"pendulum_ddpg.mp4\" type=\"video/mp4\">\n",
       "</video>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<video width=\"400\" controls loop>\n",
    "  <source src=\"pendulum_ddpg.mp4\" type=\"video/mp4\">\n",
    "</video>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch0.4]",
   "language": "python",
   "name": "conda-env-pytorch0.4-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
